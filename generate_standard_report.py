#!/usr/bin/env python3
"""
æ ‡å‡†åŒ–æŠ¥å‘Šç”Ÿæˆå™¨ - å†…ç½®å®Œæ•´è§„èŒƒ
====================================

ğŸ“Š æŠ¥å‘Šç”Ÿæˆè§„èŒƒï¼ˆä¸¥æ ¼éµå¾ªï¼Œä¸å¾—ä¿®æ”¹ï¼‰ï¼š

æ•°æ®å­—æ®µè§„èŒƒï¼š
- newuser: æ–°ç”¨æˆ·æ•°ï¼ˆåŸºç¡€è®¡æ•°å•ä½ï¼‰
- status = "good": Goodç”¨æˆ·ï¼ˆé€šè¿‡è´¨é‡ç­›é€‰ï¼‰
- verification_status = "verified": è®¤è¯ç”¨æˆ·ï¼ˆå®Œæˆèº«ä»½è®¤è¯ï¼‰
- is_returned_1_day: ç•™å­˜ç”¨æˆ·æ•°ï¼ˆæ•°å€¼ï¼Œä¸æ˜¯å¸ƒå°”å€¼ï¼‰
- gender = "female": å¥³æ€§ç”¨æˆ·ï¼ˆä¸æ˜¯"å¥³"ï¼‰
- age_group IN ("20-", "20~23"): å¹´è½»ç”¨æˆ·ï¼ˆä¸æ˜¯"18-24"ï¼‰
- dengji IN ("ä¸€çº¿", "è¶…ä¸€çº¿", "äºŒçº¿"): é«˜çº¿åŸå¸‚ç”¨æˆ·

æ ¸å¿ƒæŒ‡æ ‡è®¡ç®—å…¬å¼ï¼š
1. Goodç‡ = Goodç”¨æˆ·æ•° Ã· æ€»ç”¨æˆ·æ•° * 100
2. è®¤è¯ç‡ = Goodä¸”è®¤è¯ç”¨æˆ·æ•° Ã· Goodç”¨æˆ·æ•° * 100 (é‡è¦ï¼šä¸æ˜¯æ€»ç”¨æˆ·æ•°!)
3. Goodä¸”è®¤è¯ç‡ = Goodä¸”è®¤è¯ç”¨æˆ·æ•° Ã· æ€»ç”¨æˆ·æ•° * 100
4. æ¬¡ç•™ç‡ = Goodä¸”è®¤è¯ç”¨æˆ·ç•™å­˜æ•° Ã· Goodä¸”è®¤è¯ç”¨æˆ·æ•° * 100
5. ARPU = æ”¶å…¥ Ã· Goodä¸”è®¤è¯ç”¨æˆ·æ•°
6. CPA = æ€»æˆæœ¬ Ã· Goodä¸”è®¤è¯ç”¨æˆ·æ•°

æŠ¥å‘Šç»“æ„ï¼ˆå›ºå®šä¸¤éƒ¨åˆ†ï¼‰ï¼š
ç¬¬ä¸€éƒ¨åˆ†ï¼šå¤§ç›˜æŒ‡æ ‡ (MAIN KPI + ç”¨æˆ·è´¨é‡ + æ³¨å†Œè½¬åŒ–)
ç¬¬äºŒéƒ¨åˆ†ï¼šå¼‚å¸¸åˆ†æ (å¼‚å¸¸æ£€æµ‹ + æ¸ é“åˆ†æ)

æ–‡ä»¶å‘½åï¼šdaily_report_YYYYMMDD_YYYYMMDD_HHMMSS.html
ä¿å­˜è·¯å¾„ï¼š./output/reports/
"""

import sqlite3
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os

class StandardReportGenerator:
    """æ ‡å‡†åŒ–æŠ¥å‘Šç”Ÿæˆå™¨ - ä¸¥æ ¼æŒ‰ç…§è§„èŒƒæ‰§è¡Œ"""
    
    def __init__(self, db_path: str = "./data/data.db"):
        self.db_path = db_path
        self.data_validation_enabled = True  # å¯ç”¨æ•°æ®éªŒè¯
        
        # è§„èŒƒåŒ–çš„SQLæŸ¥è¯¢æ¨¡æ¿
        self.QUERIES = {
            'core_metrics': """
                SELECT 
                    SUM(newuser) as total_users,
                    SUM(CASE WHEN status = 'good' THEN newuser ELSE 0 END) as good_users,
                    SUM(CASE WHEN verification_status = 'verified' THEN newuser ELSE 0 END) as verified_users,
                    SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END) as quality_users,
                    SUM(zizhu_revenue_1) as revenue,
                    SUM(CASE WHEN status = 'good' AND verification_status = 'verified' AND gender = 'female' THEN newuser ELSE 0 END) as female_users,
                    SUM(CASE WHEN status = 'good' AND verification_status = 'verified' AND age_group IN ('20-', '20~23') THEN newuser ELSE 0 END) as young_users,
                    SUM(CASE WHEN status = 'good' AND verification_status = 'verified' AND dengji IN ('ä¸€çº¿', 'è¶…ä¸€çº¿', 'äºŒçº¿') THEN newuser ELSE 0 END) as high_tier_users
                FROM cpz_qs_newuser_channel_i_d
                WHERE dt = '{date}'
            """,
            
            'retention_rate': """
                SELECT 
                    ROUND(SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN is_returned_1_day ELSE 0 END) * 100.0 / 
                          SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END), 2) as retention_rate
                FROM cpz_qs_newuser_channel_i_d
                WHERE dt = '{date}'
            """,
            
            'cost_data': """
                SELECT COALESCE(SUM(cash_cost), 0) as total_cost
                FROM dwd_ttx_market_cash_cost_i_d
                WHERE dt = '{date}'
            """,
            
            'channel_analysis': """
                SELECT 
                    ad_channel,
                    SUM(newuser) as total_users,
                    SUM(CASE WHEN status = 'good' THEN newuser ELSE 0 END) as good_users,
                    SUM(CASE WHEN verification_status = 'verified' THEN newuser ELSE 0 END) as verified_users,
                    SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END) as quality_users,
                    SUM(zizhu_revenue_1) as revenue,
                    SUM(zizhu_revenue_1) / NULLIF(SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END), 0) as arpu,
                    SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN is_returned_1_day ELSE 0 END) as retained_users,
                    SUM(CASE WHEN status = 'good' AND verification_status = 'verified' AND gender = 'female' THEN newuser ELSE 0 END) as female_users,
                    SUM(CASE WHEN status = 'good' AND verification_status = 'verified' AND age_group IN ('20-', '20~23') THEN newuser ELSE 0 END) as young_users,
                    SUM(CASE WHEN status = 'good' AND verification_status = 'verified' AND dengji IN ('ä¸€çº¿', 'è¶…ä¸€çº¿', 'äºŒçº¿') THEN newuser ELSE 0 END) as high_tier_users
                FROM cpz_qs_newuser_channel_i_d
                WHERE dt = '{date}'
                GROUP BY ad_channel
                HAVING SUM(newuser) >= 50
                ORDER BY SUM(newuser) DESC
                LIMIT 20
            """,
            
            'historical_data': """
                SELECT 
                    dt,
                    SUM(newuser) as total_users,
                    SUM(CASE WHEN status = 'good' THEN newuser ELSE 0 END) as good_users,
                    SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END) as quality_users,
                    SUM(zizhu_revenue_1) as revenue,
                    SUM(zizhu_revenue_1) / NULLIF(SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END), 0) as arpu,
                    ROUND(SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN is_returned_1_day ELSE 0 END) * 100.0 / 
                          NULLIF(SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END), 0), 2) as retention_rate,
                    SUM(CASE WHEN status = 'good' AND verification_status = 'verified' AND gender = 'female' THEN newuser ELSE 0 END) * 100.0 / 
                        NULLIF(SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END), 0) as female_ratio,
                    SUM(CASE WHEN status = 'good' AND verification_status = 'verified' AND age_group IN ('20-', '20~23') THEN newuser ELSE 0 END) * 100.0 / 
                        NULLIF(SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END), 0) as young_ratio,
                    SUM(CASE WHEN status = 'good' AND verification_status = 'verified' AND dengji IN ('ä¸€çº¿', 'è¶…ä¸€çº¿', 'äºŒçº¿') THEN newuser ELSE 0 END) * 100.0 / 
                        NULLIF(SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END), 0) as high_tier_ratio
                FROM cpz_qs_newuser_channel_i_d
                WHERE dt BETWEEN '{start_date}' AND '{end_date}'
                GROUP BY dt
                ORDER BY dt
            """,
            
            'historical_cost': """
                SELECT 
                    dt,
                    COALESCE(SUM(cash_cost), 0) as total_cost
                FROM dwd_ttx_market_cash_cost_i_d
                WHERE dt BETWEEN '{start_date}' AND '{end_date}'
                GROUP BY dt
                ORDER BY dt
            """,
            
            # ç´ æåˆ†ææŸ¥è¯¢æ¨¡æ¿ï¼ˆä¸åŒºåˆ†æ¸ é“ï¼ŒæŒ‰ç´ æIDæ±‡æ€»ï¼‰
            'creative_ranking_today': """
                SELECT 
                    media_id_str,
                    GROUP_CONCAT(DISTINCT channel) as channels,
                    COUNT(DISTINCT channel) as channel_count,
                    SUM(total_good_verified) as good_verified,
                    SUM(cash_cost) as cost,
                    ROUND(SUM(cash_cost) / NULLIF(SUM(total_good_verified), 0), 2) as cpa,
                    ROUND(SUM(total_payed_amount_good_verified) / NULLIF(SUM(total_good_verified), 0), 2) as arpu,
                    ROUND(SUM(total_good_verified_return_1d) * 100.0 / NULLIF(SUM(total_good_verified), 0), 1) as retention_rate,
                    ROUND(SUM(total_good_verified_female) * 100.0 / NULLIF(SUM(total_good_verified), 0), 1) as female_rate,
                    ROUND(SUM(total_good_verified_white) * 100.0 / NULLIF(SUM(total_good_verified), 0), 1) as white_collar_rate,
                    ROUND(SUM(total_good_verified_ios) * 100.0 / NULLIF(SUM(total_good_verified), 0), 1) as ios_rate
                FROM dws_ttx_market_media_reports_i_d
                WHERE dt = '{date}' AND media_id_str IS NOT NULL AND media_id_str != ''
                GROUP BY media_id_str
                HAVING SUM(total_good_verified) >= 10
                ORDER BY SUM(total_good_verified) DESC
                LIMIT 20
            """,
            
            'creative_ranking_yesterday': """
                SELECT 
                    media_id_str,
                    GROUP_CONCAT(DISTINCT channel) as channels,
                    COUNT(DISTINCT channel) as channel_count,
                    SUM(total_good_verified) as good_verified,
                    SUM(cash_cost) as cost,
                    ROUND(SUM(cash_cost) / NULLIF(SUM(total_good_verified), 0), 2) as cpa,
                    ROW_NUMBER() OVER (ORDER BY SUM(total_good_verified) DESC) as rank_yesterday
                FROM dws_ttx_market_media_reports_i_d
                WHERE dt = '{yesterday_date}' AND media_id_str IS NOT NULL AND media_id_str != ''
                GROUP BY media_id_str
                HAVING SUM(total_good_verified) >= 10
                ORDER BY SUM(total_good_verified) DESC
                LIMIT 50
            """,
            
            # è´¦æˆ·åˆ†ææŸ¥è¯¢
            'account_metrics_today': """
                SELECT 
                    account,
                    account_name,
                    CASE WHEN account_name LIKE '%å®‰å“%' THEN 'Android'
                         WHEN account_name LIKE '%iOS%' OR account_name LIKE '%ios%' OR account_name LIKE '%IOS%' THEN 'iOS'
                         ELSE 'other' END AS os,
                    CASE WHEN account_name LIKE '%ç”·%' AND account_name NOT LIKE '%ç”·å¥³%' THEN 'ç”·'
                         WHEN account_name LIKE '%å¥³%' AND account_name NOT LIKE '%ç”·å¥³%' THEN 'å¥³'
                         ELSE 'other' END AS gender,
                    SUM(total_good_verified) as good_verified,
                    ROUND(SUM(total_payed_amount_good_verified) / NULLIF(SUM(total_good_verified), 0), 2) as arpu,
                    ROUND(SUM(total_good_verified_twenty) * 100.0 / NULLIF(SUM(total_good_verified), 0), 1) as under_20_rate,
                    ROUND(SUM(total_good_verified_young) * 100.0 / NULLIF(SUM(total_good_verified), 0), 1) as under_23_rate,
                    ROUND((SUM(total_good_verified) - SUM(total_good_verified_22_40)) * 100.0 / NULLIF(SUM(total_good_verified), 0), 1) as under_24_rate,
                    ROUND(SUM(total_good_verified_white) * 100.0 / NULLIF(SUM(total_good_verified), 0), 1) as white_collar_rate,
                    ROUND(SUM(third_line_city_user) * 100.0 / NULLIF(SUM(total_good_verified), 0), 1) as third_tier_city_rate,
                    SUM(cash_cost) as cost
                FROM dwd_ttx_market_cash_cost_i_d_test
                WHERE dt = '{date}' AND channel = '{channel}'
                GROUP BY account, account_name
                HAVING SUM(total_good_verified) >= 100
                ORDER BY SUM(total_good_verified) DESC
            """,
            
            'account_metrics_yesterday': """
                SELECT 
                    account,
                    account_name,
                    CASE WHEN account_name LIKE '%å®‰å“%' THEN 'Android'
                         WHEN account_name LIKE '%iOS%' OR account_name LIKE '%ios%' OR account_name LIKE '%IOS%' THEN 'iOS'
                         ELSE 'other' END AS os,
                    CASE WHEN account_name LIKE '%ç”·%' AND account_name NOT LIKE '%ç”·å¥³%' THEN 'ç”·'
                         WHEN account_name LIKE '%å¥³%' AND account_name NOT LIKE '%ç”·å¥³%' THEN 'å¥³'
                         ELSE 'other' END AS gender,
                    SUM(total_good_verified) as good_verified,
                    ROUND(SUM(total_payed_amount_good_verified) / NULLIF(SUM(total_good_verified), 0), 2) as arpu,
                    ROUND(SUM(total_good_verified_twenty) * 100.0 / NULLIF(SUM(total_good_verified), 0), 1) as under_20_rate,
                    ROUND(SUM(total_good_verified_young) * 100.0 / NULLIF(SUM(total_good_verified), 0), 1) as under_23_rate,
                    ROUND((SUM(total_good_verified) - SUM(total_good_verified_22_40)) * 100.0 / NULLIF(SUM(total_good_verified), 0), 1) as under_24_rate,
                    ROUND(SUM(total_good_verified_white) * 100.0 / NULLIF(SUM(total_good_verified), 0), 1) as white_collar_rate,
                    ROUND(SUM(third_line_city_user) * 100.0 / NULLIF(SUM(total_good_verified), 0), 1) as third_tier_city_rate,
                    SUM(cash_cost) as cost
                FROM dwd_ttx_market_cash_cost_i_d_test
                WHERE dt = '{yesterday_date}' AND channel = '{channel}'
                GROUP BY account, account_name
                HAVING SUM(total_good_verified) >= 100
            """,
            
            'account_metrics_7d_avg': """
                WITH daily_account_metrics AS (
                    SELECT 
                        dt,
                        account,
                        account_name,
                        CASE WHEN account_name LIKE '%å®‰å“%' THEN 'Android'
                             WHEN account_name LIKE '%iOS%' OR account_name LIKE '%ios%' OR account_name LIKE '%IOS%' THEN 'iOS'
                             ELSE 'other' END AS os,
                        CASE WHEN account_name LIKE '%ç”·%' AND account_name NOT LIKE '%ç”·å¥³%' THEN 'ç”·'
                             WHEN account_name LIKE '%å¥³%' AND account_name NOT LIKE '%ç”·å¥³%' THEN 'å¥³'
                             ELSE 'other' END AS gender,
                        SUM(total_good_verified) as daily_good_verified,
                        CASE WHEN SUM(total_good_verified) > 0 
                             THEN SUM(total_payed_amount_after_tax_good_verified) / SUM(total_good_verified) 
                             ELSE 0 END as daily_arpu,
                        CASE WHEN SUM(total_good_verified) > 0 
                             THEN SUM(total_good_verified_twenty) * 100.0 / SUM(total_good_verified) 
                             ELSE 0 END as daily_under_20_rate,
                        CASE WHEN SUM(total_good_verified) > 0 
                             THEN SUM(total_good_verified_young) * 100.0 / SUM(total_good_verified) 
                             ELSE 0 END as daily_under_23_rate,
                        CASE WHEN SUM(total_good_verified) > 0 
                             THEN SUM(total_good_verified_white) * 100.0 / SUM(total_good_verified) 
                             ELSE 0 END as daily_white_collar_rate,
                        CASE WHEN SUM(total_good_verified) > 0 
                             THEN SUM(third_line_city_user) * 100.0 / SUM(total_good_verified) 
                             ELSE 0 END as daily_third_tier_city_rate,
                        SUM(cash_cost) as daily_cost
                    FROM dwd_ttx_market_cash_cost_i_d_test
                    WHERE dt BETWEEN '{start_date}' AND '{end_date}' AND channel = '{channel}'
                    GROUP BY dt, account, account_name
                    HAVING SUM(total_good_verified) >= 100
                )
                SELECT 
                    account,
                    account_name,
                    os,
                    gender,
                    ROUND(AVG(daily_good_verified), 0) as good_verified_avg,
                    ROUND(AVG(daily_arpu), 2) as arpu_avg,
                    ROUND(AVG(daily_under_20_rate), 1) as under_20_rate_avg,
                    ROUND(AVG(daily_under_23_rate), 1) as under_23_rate_avg,
                    ROUND(AVG(daily_white_collar_rate), 1) as white_collar_rate_avg,
                    ROUND(AVG(daily_third_tier_city_rate), 1) as third_tier_city_rate_avg,
                    ROUND(AVG(daily_cost), 2) as cost_avg
                FROM daily_account_metrics
                GROUP BY account, account_name, os, gender
                HAVING COUNT(*) >= 3
            """
        }
    
    def generate_report(self, date_str: str) -> str:
        """ç”Ÿæˆæ ‡å‡†åŒ–æŠ¥å‘Š"""
        print(f"ğŸ“Š å¼€å§‹ç”Ÿæˆ {date_str} çš„æ ‡å‡†åŒ–æŠ¥å‘Š...")
        print("âœ… å·²åŠ è½½å†…ç½®è§„èŒƒï¼Œä¸¥æ ¼æŒ‰ç…§æ ‡å‡†æ‰§è¡Œ")
        
        conn = sqlite3.connect(self.db_path)
        
        try:
            # 1. æ”¶é›†æ ¸å¿ƒæŒ‡æ ‡æ•°æ®
            core_data = self._collect_core_metrics(conn, date_str)
            print(f"âœ… æ ¸å¿ƒæŒ‡æ ‡è®¡ç®—å®Œæˆ - Quality Users: {core_data['quality_users']:,}, è®¤è¯ç‡: {core_data['verified_rate']:.2f}%")
            
            # 1.5 æ•°æ®ä¸€è‡´æ€§éªŒè¯
            if not self._validate_data_consistency(conn, date_str, core_data):
                print("âš ï¸  æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å‘ç°é—®é¢˜ï¼Œä½†ç»§ç»­ç”ŸæˆæŠ¥å‘Š")
            
            # 2. æ”¶é›†å¼‚å¸¸åˆ†ææ•°æ®
            anomaly_data = self._collect_anomaly_data(conn, date_str)
            
            # 3. æ”¶é›†ç´ æåˆ†ææ•°æ®
            creative_data = self._collect_creative_analysis(conn, date_str)
            
            # 4. æ”¶é›†è´¦æˆ·åˆ†ææ•°æ®
            account_data = self._collect_account_analysis(conn, date_str)
            
            # 5. ç”ŸæˆHTMLæŠ¥å‘Š
            html_content = self._generate_html_report(date_str, core_data, anomaly_data, creative_data, account_data, conn)
            
            # 4. ä¿å­˜æ–‡ä»¶
            filename = self._save_report(html_content, date_str)
            
            return filename
            
        finally:
            conn.close()
    
    def _collect_core_metrics(self, conn: sqlite3.Connection, date_str: str) -> dict:
        """æ”¶é›†æ ¸å¿ƒæŒ‡æ ‡æ•°æ® - ä¸¥æ ¼æŒ‰ç…§è§„èŒƒè®¡ç®—"""
        
        # 1. æ ¸å¿ƒæ•°æ®æŸ¥è¯¢
        core_query = self.QUERIES['core_metrics'].format(date=date_str)
        core_df = pd.read_sql_query(core_query, conn)
        
        if core_df.empty:
            raise Exception(f"æœªæ‰¾åˆ° {date_str} çš„æ•°æ®")
        
        row = core_df.iloc[0]
        
        # 2. ç•™å­˜ç‡æŸ¥è¯¢
        retention_query = self.QUERIES['retention_rate'].format(date=date_str)
        retention_df = pd.read_sql_query(retention_query, conn)
        retention_rate = retention_df.iloc[0]['retention_rate'] if not retention_df.empty else 0
        
        # 2.1 å¦‚æœå½“å¤©æ¬¡ç•™ç‡ä¸º0ï¼ŒæŸ¥è¯¢å‰ä¸€å¤©çš„æ¬¡ç•™ç‡
        prev_date_str = None
        prev_retention_rate = None
        if pd.isna(retention_rate) or retention_rate == 0:
            prev_date = datetime.strptime(date_str, '%Y-%m-%d') - timedelta(days=1)
            prev_date_str = prev_date.strftime('%Y-%m-%d')
            prev_retention_query = self.QUERIES['retention_rate'].format(date=prev_date_str)
            prev_retention_df = pd.read_sql_query(prev_retention_query, conn)
            if not prev_retention_df.empty and prev_retention_df.iloc[0]['retention_rate'] is not None:
                prev_retention_rate = prev_retention_df.iloc[0]['retention_rate']
        
        # 3. æˆæœ¬æ•°æ®æŸ¥è¯¢
        cost_query = self.QUERIES['cost_data'].format(date=date_str)
        cost_df = pd.read_sql_query(cost_query, conn)
        total_cost = cost_df.iloc[0]['total_cost'] if not cost_df.empty else 0
        
        # 4. æŒ‰è§„èŒƒè®¡ç®—æ‰€æœ‰æŒ‡æ ‡
        total_users = row['total_users'] or 0
        good_users = row['good_users'] or 0
        quality_users = row['quality_users'] or 0
        
        metrics = {
            # MAIN KPI
            'quality_users': quality_users,
            'cpa': total_cost / quality_users if quality_users > 0 else 0,
            'arpu': (row['revenue'] or 0) / quality_users if quality_users > 0 else 0,
            'retention_rate': retention_rate,
            'prev_retention_rate': prev_retention_rate,
            'prev_retention_date': prev_date_str,
            
            # ç”¨æˆ·è´¨é‡ (åŸºäºGoodä¸”è®¤è¯ç”¨æˆ·)
            'female_ratio': (row['female_users'] or 0) / quality_users * 100 if quality_users > 0 else 0,
            'young_ratio': (row['young_users'] or 0) / quality_users * 100 if quality_users > 0 else 0,
            'high_tier_ratio': (row['high_tier_users'] or 0) / quality_users * 100 if quality_users > 0 else 0,
            
            # æ³¨å†Œè½¬åŒ–
            'good_rate': good_users / total_users * 100 if total_users > 0 else 0,
            'verified_rate': quality_users / good_users * 100 if good_users > 0 else 0,  # å…³é”®ï¼šGoodç”¨æˆ·ä¸­çš„è®¤è¯ç‡
            'quality_rate': quality_users / total_users * 100 if total_users > 0 else 0,
            
            # åŸå§‹æ•°æ®
            'raw_data': row.to_dict()
        }
        
        print(f"âœ… æ ¸å¿ƒæŒ‡æ ‡è®¡ç®—å®Œæˆ - Quality Users: {quality_users:,}, è®¤è¯ç‡: {metrics['verified_rate']:.2f}%")
        return metrics
    
    def _collect_anomaly_data(self, conn: sqlite3.Connection, date_str: str) -> dict:
        """æ”¶é›†å¼‚å¸¸åˆ†ææ•°æ®"""
        
        # 1. å†å²æ•°æ® (14å¤©)
        start_date = (datetime.strptime(date_str, '%Y-%m-%d') - timedelta(days=14)).strftime('%Y-%m-%d')
        hist_query = self.QUERIES['historical_data'].format(start_date=start_date, end_date=date_str)
        hist_df = pd.read_sql_query(hist_query, conn)
        
        # 1.1 å†å²æˆæœ¬æ•°æ®
        cost_query = self.QUERIES['historical_cost'].format(start_date=start_date, end_date=date_str)
        cost_df = pd.read_sql_query(cost_query, conn)
        
        # åˆå¹¶æ•°æ®
        if not hist_df.empty and not cost_df.empty:
            hist_df = pd.merge(hist_df, cost_df, on='dt', how='left')
            hist_df['cpa'] = hist_df['total_cost'] / hist_df['quality_users'].replace(0, np.nan)
        
        # 2. æ¸ é“åˆ†ææ•°æ®
        channel_query = self.QUERIES['channel_analysis'].format(date=date_str)
        channel_df = pd.read_sql_query(channel_query, conn)
        
        # 2.1 è·å–å†å²æ¸ é“æ•°æ®ç”¨äºå¯¹æ¯”
        prev_date = (datetime.strptime(date_str, '%Y-%m-%d') - timedelta(days=1)).strftime('%Y-%m-%d')
        prev_channel_query = self.QUERIES['channel_analysis'].format(date=prev_date)
        prev_channel_df = pd.read_sql_query(prev_channel_query, conn)
        
        # 3. å…¨é¢å¼‚å¸¸æ£€æµ‹
        anomalies = []
        if len(hist_df) >= 7:
            current_data = hist_df[hist_df['dt'] == date_str]
            if not current_data.empty:
                hist_data = hist_df[hist_df['dt'] != date_str]
                
                # å®šä¹‰éœ€è¦æ£€æµ‹çš„æŒ‡æ ‡
                metrics_to_check = [
                    # MAIN KPI
                    {'name': 'Goodä¸”è®¤è¯ç”¨æˆ·æ•°', 'field': 'quality_users', 'format': '{:.0f}', 'category': 'MAIN KPI'},
                    {'name': 'CPA', 'field': 'cpa', 'format': 'Â¥{:.2f}', 'category': 'MAIN KPI'},
                    {'name': 'ARPU', 'field': 'arpu', 'format': 'Â¥{:.2f}', 'category': 'MAIN KPI'},
                    {'name': 'æ¬¡ç•™ç‡', 'field': 'retention_rate', 'format': '{:.2f}%', 'category': 'MAIN KPI'},
                    
                    # ç”¨æˆ·è´¨é‡
                    {'name': 'å¥³æ€§å æ¯”', 'field': 'female_ratio', 'format': '{:.2f}%', 'category': 'ç”¨æˆ·è´¨é‡'},
                    {'name': 'å¹´è½»å æ¯”', 'field': 'young_ratio', 'format': '{:.2f}%', 'category': 'ç”¨æˆ·è´¨é‡'},
                    {'name': 'é«˜çº¿åŸå¸‚å æ¯”', 'field': 'high_tier_ratio', 'format': '{:.2f}%', 'category': 'ç”¨æˆ·è´¨é‡'},
                    
                    # æ³¨å†Œè½¬åŒ– - éœ€è¦è®¡ç®—
                    {'name': 'Goodç‡', 'field': 'good_rate', 'format': '{:.2f}%', 'category': 'æ³¨å†Œè½¬åŒ–'},
                    {'name': 'è®¤è¯ç‡', 'field': 'verified_rate', 'format': '{:.2f}%', 'category': 'æ³¨å†Œè½¬åŒ–'},
                    {'name': 'Goodä¸”è®¤è¯ç‡', 'field': 'quality_rate', 'format': '{:.2f}%', 'category': 'æ³¨å†Œè½¬åŒ–'}
                ]
                
                # è®¡ç®—è½¬åŒ–ç‡æŒ‡æ ‡
                current_row = current_data.iloc[0]
                hist_df.loc[hist_df['dt'] == date_str, 'good_rate'] = current_row['good_users'] / current_row['total_users'] * 100 if current_row['total_users'] > 0 else 0
                hist_df.loc[hist_df['dt'] == date_str, 'verified_rate'] = current_row['quality_users'] / current_row['good_users'] * 100 if current_row['good_users'] > 0 else 0
                hist_df.loc[hist_df['dt'] == date_str, 'quality_rate'] = current_row['quality_users'] / current_row['total_users'] * 100 if current_row['total_users'] > 0 else 0
                
                # å®šä¹‰æŒ‡æ ‡æ–¹å‘ï¼šæ­£å‘æŒ‡æ ‡(è¶Šé«˜è¶Šå¥½) vs è´Ÿå‘æŒ‡æ ‡(è¶Šä½è¶Šå¥½)
                positive_indicators = [
                    'quality_users', 'arpu', 'retention_rate', 
                    'female_ratio', 'good_rate', 'verified_rate', 'quality_rate'
                ]
                negative_indicators = ['cpa', 'young_ratio', 'high_tier_ratio']  # CPAã€å¹´è½»å æ¯”ã€é«˜çº¿åŸå¸‚å æ¯”è¶Šä½è¶Šå¥½
                
                # å¯¹æ¯ä¸ªæŒ‡æ ‡è¿›è¡Œå¼‚å¸¸æ£€æµ‹
                for metric in metrics_to_check:
                    field = metric['field']
                    
                    # è®¡ç®—å†å²æ•°æ®çš„è½¬åŒ–ç‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
                    if field in ['good_rate', 'verified_rate', 'quality_rate']:
                        if field == 'good_rate':
                            hist_data[field] = hist_data['good_users'] / hist_data['total_users'].replace(0, np.nan) * 100
                        elif field == 'verified_rate':
                            hist_data[field] = hist_data['quality_users'] / hist_data['good_users'].replace(0, np.nan) * 100
                        elif field == 'quality_rate':
                            hist_data[field] = hist_data['quality_users'] / hist_data['total_users'].replace(0, np.nan) * 100
                    
                    # ç‰¹æ®Šå¤„ç†æ¬¡ç•™ç‡
                    if field == 'retention_rate' and (pd.isna(current_data.iloc[0][field]) or current_data.iloc[0][field] == 0):
                        # å¦‚æœå½“å¤©æ¬¡ç•™ç‡ä¸º0ï¼Œä½¿ç”¨å‰ä¸€å¤©çš„æ•°æ®
                        prev_date = datetime.strptime(date_str, '%Y-%m-%d') - timedelta(days=1)
                        prev_date_str = prev_date.strftime('%Y-%m-%d')
                        prev_data = hist_df[hist_df['dt'] == prev_date_str]
                        if not prev_data.empty and not pd.isna(prev_data.iloc[0][field]):
                            current_value = prev_data.iloc[0][field]
                            # å°†ç”¨äºæ¯”è¾ƒçš„å†å²æ•°æ®ä¹Ÿå‘å‰æ¨ä¸€å¤©
                            hist_values = hist_data[hist_data['dt'] < prev_date_str][field].dropna().values
                            metric['name'] = f"{metric['name']}ï¼ˆ{prev_date_str}ï¼‰"
                        else:
                            continue
                    else:
                        current_value = current_data.iloc[0][field] if field in current_data.columns else hist_df[hist_df['dt'] == date_str][field].iloc[0]
                        hist_values = hist_data[field].dropna().values
                    
                    if len(hist_values) > 0 and not pd.isna(current_value):
                        q1, q3 = np.percentile(hist_values, [25, 75])
                        iqr = q3 - q1
                        
                        # å¯¹äºæå°çš„IQRï¼Œä½¿ç”¨æœ€å°é˜ˆå€¼
                        if iqr < 0.1:
                            iqr = max(0.1, np.std(hist_values) * 0.5)
                        
                        lower_bound = q1 - 1.5 * iqr
                        upper_bound = q3 + 1.5 * iqr
                        
                        # åŒå±‚å¼‚å¸¸æ£€æµ‹ï¼šä¸¥é‡å¼‚å¸¸ + æ•æ„Ÿå¼‚å¸¸
                        # æ ¹æ®æŒ‡æ ‡ç±»å‹åˆ¤æ–­æ˜¯å¦ä¸ºä¸¥é‡å¼‚å¸¸
                        is_positive_indicator = field in positive_indicators
                        is_negative_indicator = field in negative_indicators
                        
                        if is_positive_indicator:
                            # æ­£å‘æŒ‡æ ‡ï¼šåªæœ‰åä½æ‰æ˜¯å¼‚å¸¸
                            is_serious_anomaly = current_value < lower_bound
                        elif is_negative_indicator:
                            # è´Ÿå‘æŒ‡æ ‡ï¼šåªæœ‰åé«˜æ‰æ˜¯å¼‚å¸¸
                            is_serious_anomaly = current_value > upper_bound
                        else:
                            # å…¶ä»–æŒ‡æ ‡ï¼šåé«˜æˆ–åä½éƒ½æ˜¯å¼‚å¸¸
                            is_serious_anomaly = current_value < lower_bound or current_value > upper_bound
                        
                        is_sensitive_anomaly = False
                        sensitive_reasons = []
                        
                        # æ•æ„Ÿå¼‚å¸¸æ£€æµ‹ - é’ˆå¯¹æ‰€æœ‰å¤§ç›˜æŒ‡æ ‡ï¼ŒåŒºåˆ†æ­£å‘/è´Ÿå‘æŒ‡æ ‡
                        if len(hist_values) >= 3:
                            # è·å–è¿‘æœŸæ•°æ®è¿›è¡Œæ›´æ•æ„Ÿçš„åˆ†æ
                            recent_7_days = hist_values[-7:] if len(hist_values) >= 7 else hist_values[-3:]
                            recent_mean = np.mean(recent_7_days)
                            
                            
                            # æ ¹æ®ä¸åŒæŒ‡æ ‡è®¾ç½®ä¸åŒçš„æ•æ„Ÿåº¦é˜ˆå€¼
                            if field == 'arpu':
                                # ARPU: æœ€æ•æ„Ÿï¼Œå…³é”®æ”¶å…¥æŒ‡æ ‡
                                change_threshold = 5.0  # æ—¥å˜åŒ–>5%
                                deviation_threshold = 3.0  # åç¦»å‡å€¼>3%
                            elif field in ['quality_users', 'cpa']:
                                # æ ¸å¿ƒKPI: ä¸­ç­‰æ•æ„Ÿåº¦
                                change_threshold = 8.0  # æ—¥å˜åŒ–>8%
                                deviation_threshold = 5.0  # åç¦»å‡å€¼>5%
                            elif field == 'retention_rate':
                                # ç•™å­˜ç‡: ä¸­ç­‰æ•æ„Ÿåº¦
                                change_threshold = 6.0  # æ—¥å˜åŒ–>6%
                                deviation_threshold = 4.0  # åç¦»å‡å€¼>4%
                            elif field in ['female_ratio', 'young_ratio', 'high_tier_ratio']:
                                # ç”¨æˆ·è´¨é‡æŒ‡æ ‡: è¾ƒä½æ•æ„Ÿåº¦
                                change_threshold = 10.0  # æ—¥å˜åŒ–>10%
                                deviation_threshold = 6.0  # åç¦»å‡å€¼>6%
                            elif field in ['good_rate', 'verified_rate', 'quality_rate']:
                                # è½¬åŒ–ç‡æŒ‡æ ‡: ä¸­ç­‰æ•æ„Ÿåº¦
                                change_threshold = 7.0  # æ—¥å˜åŒ–>7%
                                deviation_threshold = 5.0  # åç¦»å‡å€¼>5%
                            else:
                                # é»˜è®¤æ•æ„Ÿåº¦
                                change_threshold = 8.0
                                deviation_threshold = 5.0
                            
                            # 1. æ—¥å˜åŒ–ç‡æ£€æµ‹ - åªæ£€æµ‹è´Ÿå‘å˜åŒ–
                            if len(hist_values) > 0:
                                prev_value = hist_values[-1]
                                if prev_value != 0:  # é¿å…é™¤é›¶
                                    change_pct = abs(current_value - prev_value) / abs(prev_value) * 100
                                    
                                    # æ­£å‘æŒ‡æ ‡ï¼šåªå…³æ³¨ä¸‹é™
                                    if is_positive_indicator and current_value < prev_value and change_pct > change_threshold:
                                        is_sensitive_anomaly = True
                                        sensitive_reasons.append(f'æ—¥å˜åŒ–{change_pct:.1f}%(ä¸‹é™)')
                                    # è´Ÿå‘æŒ‡æ ‡ï¼šåªå…³æ³¨ä¸Šå‡
                                    elif is_negative_indicator and current_value > prev_value and change_pct > change_threshold:
                                        is_sensitive_anomaly = True
                                        sensitive_reasons.append(f'æ—¥å˜åŒ–{change_pct:.1f}%(ä¸Šå‡)')
                            
                            # 2. ç›¸å¯¹æå€¼æ£€æµ‹ - åªæ£€æµ‹ä¸å¥½çš„æå€¼
                            if is_positive_indicator and current_value < recent_7_days.min():
                                # æ­£å‘æŒ‡æ ‡ï¼šä½äºæœ€ä½å€¼æ˜¯é—®é¢˜
                                is_sensitive_anomaly = True
                                sensitive_reasons.append('ä½äºè¿‘7å¤©æœ€ä½å€¼')
                            elif is_negative_indicator and current_value > recent_7_days.max():
                                # è´Ÿå‘æŒ‡æ ‡ï¼šé«˜äºæœ€é«˜å€¼æ˜¯é—®é¢˜
                                is_sensitive_anomaly = True
                                sensitive_reasons.append('é«˜äºè¿‘7å¤©æœ€é«˜å€¼')
                                
                            # 3. åç¦»è¿‘æœŸå‡å€¼æ£€æµ‹ - åªæ£€æµ‹è´Ÿå‘åç¦»
                            if recent_mean != 0:  # é¿å…é™¤é›¶
                                deviation_pct = abs(current_value - recent_mean) / abs(recent_mean) * 100
                                
                                if deviation_pct > deviation_threshold:
                                    # æ­£å‘æŒ‡æ ‡ï¼šä½äºå‡å€¼æ˜¯é—®é¢˜
                                    if is_positive_indicator and current_value < recent_mean:
                                        is_sensitive_anomaly = True
                                        sensitive_reasons.append(f'åç¦»è¿‘æœŸå‡å€¼{deviation_pct:.1f}%')
                                    # è´Ÿå‘æŒ‡æ ‡ï¼šé«˜äºå‡å€¼æ˜¯é—®é¢˜
                                    elif is_negative_indicator and current_value > recent_mean:
                                        is_sensitive_anomaly = True
                                        sensitive_reasons.append(f'åç¦»è¿‘æœŸå‡å€¼{deviation_pct:.1f}%')
                        
                        # è®°å½•å¼‚å¸¸
                        if is_serious_anomaly:
                            direction = 'åä½' if current_value < lower_bound else 'åé«˜'
                            severity = 'high' if abs(current_value - np.median(hist_values)) / np.median(hist_values) > 0.3 else 'medium'
                            
                            anomalies.append({
                                'metric': metric['name'],
                                'category': metric['category'],
                                'current_value': metric['format'].format(current_value),
                                'normal_range': f"[{metric['format'].format(lower_bound)}, {metric['format'].format(upper_bound)}]",
                                'direction': direction,
                                'severity': severity,
                                'type': 'serious',
                                'reason': f'è¶…å‡ºIQRèŒƒå›´ï¼ˆ{direction}ï¼‰',
                                'raw_current': current_value,
                                'raw_lower': lower_bound,
                                'raw_upper': upper_bound
                            })
                        elif is_sensitive_anomaly:
                            # è®¡ç®—è¯¦ç»†çš„æ•æ„Ÿå¼‚å¸¸ä¸Šä¸‹æ–‡æ•°æ®
                            prev_value = hist_values[-1] if len(hist_values) > 0 else current_value
                            change_pct = abs(current_value - prev_value) / abs(prev_value) * 100 if prev_value != 0 else 0
                            recent_7_days = hist_values[-7:] if len(hist_values) >= 7 else hist_values[-3:]
                            recent_mean = np.mean(recent_7_days) if len(recent_7_days) > 0 else current_value
                            recent_min = recent_7_days.min() if len(recent_7_days) > 0 else current_value
                            recent_max = recent_7_days.max() if len(recent_7_days) > 0 else current_value
                            
                            # åˆ†ææ•æ„Ÿå¼‚å¸¸çš„å…·ä½“åŸå› ç±»å‹
                            anomaly_details = {
                                'has_daily_change': False,
                                'has_extreme_value': False,
                                'has_mean_deviation': False,
                                'daily_change_pct': change_pct,
                                'prev_value': prev_value,
                                'recent_mean': recent_mean,
                                'recent_min': recent_min,
                                'recent_max': recent_max,
                                'is_positive_indicator': field in positive_indicators
                            }
                            
                            # æ ‡è®°å…·ä½“çš„å¼‚å¸¸ç±»å‹
                            for reason in sensitive_reasons:
                                if 'æ—¥å˜åŒ–' in reason:
                                    anomaly_details['has_daily_change'] = True
                                elif 'æœ€ä½å€¼' in reason or 'æœ€é«˜å€¼' in reason:
                                    anomaly_details['has_extreme_value'] = True
                                elif 'åç¦»è¿‘æœŸå‡å€¼' in reason:
                                    anomaly_details['has_mean_deviation'] = True
                            
                            anomalies.append({
                                'metric': metric['name'],
                                'category': metric['category'],
                                'current_value': metric['format'].format(current_value),
                                'normal_range': f"[{metric['format'].format(lower_bound)}, {metric['format'].format(upper_bound)}]",
                                'direction': 'æ•æ„Ÿå¼‚å¸¸',
                                'severity': 'sensitive',
                                'type': 'sensitive',
                                'reason': ' | '.join(sensitive_reasons),
                                'raw_current': current_value,
                                'raw_lower': lower_bound,
                                'raw_upper': upper_bound,
                                'sensitive_details': anomaly_details  # æ–°å¢è¯¦ç»†ä¿¡æ¯
                            })
        
        # 4. åŸºäºå¼‚å¸¸æ£€æµ‹ç»“æœçš„æ™ºèƒ½æ¸ é“åˆ†æ
        smart_channel_analysis = self._analyze_channels_by_anomalies(
            anomalies, channel_df, prev_channel_df, date_str, conn
        )
        
        return {
            'anomalies': anomalies,
            'channel_analysis': channel_df.to_dict('records') if not channel_df.empty else [],
            'smart_channel_analysis': smart_channel_analysis,
            'historical_data': hist_df.to_dict('records')
        }
    
    def _analyze_channels_by_anomalies(self, anomalies: list, channel_df: pd.DataFrame, 
                                     prev_channel_df: pd.DataFrame, date_str: str, 
                                     conn: sqlite3.Connection) -> dict:
        """åŸºäºå¼‚å¸¸æ£€æµ‹ç»“æœè¿›è¡Œæ™ºèƒ½æ¸ é“åˆ†æ"""
        
        if channel_df.empty:
            return {'has_analysis': False, 'message': 'æ¸ é“æ•°æ®ä¸ºç©º'}
            
        analysis_results = {
            'has_analysis': False,
            'anomaly_metrics': [],
            'channel_impact_analysis': [],
            'summary': ''
        }
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸éœ€è¦åˆ†æ - æ‰©å±•åˆ°æ‰€æœ‰ç±»åˆ«
        key_anomaly_metrics = []
        for anomaly in anomalies:
            # åˆ†ææ‰€æœ‰ç±»åˆ«çš„å¼‚å¸¸æŒ‡æ ‡
            if anomaly.get('category') in ['MAIN KPI', 'ç”¨æˆ·è´¨é‡', 'æ³¨å†Œè½¬åŒ–']:
                key_anomaly_metrics.append(anomaly)
        
        if not key_anomaly_metrics:
            return {
                'has_analysis': False, 
                'message': 'æœªæ£€æµ‹åˆ°éœ€è¦æ¸ é“æ·±åº¦åˆ†æçš„å¼‚å¸¸æŒ‡æ ‡'
            }
        
        analysis_results['has_analysis'] = True
        analysis_results['anomaly_metrics'] = key_anomaly_metrics
        
        # æ™ºèƒ½å¤šç»´åº¦å¼‚å¸¸æ ¹å› åˆ†æ
        for anomaly in key_anomaly_metrics:
            root_cause_analysis = self._intelligent_root_cause_analysis(
                anomaly, date_str, conn, channel_df, prev_channel_df
            )
            
            if root_cause_analysis:
                analysis_results['channel_impact_analysis'].append(root_cause_analysis)
        
        # ç”Ÿæˆæ€»ç»“
        if analysis_results['channel_impact_analysis']:
            total_channels = len(analysis_results['channel_impact_analysis'])
            analysis_results['summary'] = f"æ£€æµ‹åˆ°{len(key_anomaly_metrics)}ä¸ªå¼‚å¸¸æŒ‡æ ‡ï¼Œå·²å®Œæˆ{total_channels}é¡¹æ¸ é“å½±å“åˆ†æ"
        
        return analysis_results
    
    def _intelligent_root_cause_analysis(self, anomaly: dict, date_str: str, conn: sqlite3.Connection, 
                                        channel_df: pd.DataFrame, prev_channel_df: pd.DataFrame) -> dict:
        """æ™ºèƒ½æ ¹å› åˆ†æ - å¤šç»´åº¦å¼‚å¸¸æ£€æµ‹"""
        try:
            metric_name = anomaly.get('metric', '')
            analysis_results = {
                'metric': metric_name,
                'root_causes': [],
                'confidence_score': 0.0,
                'analysis_summary': '',
                'analysis_type': 'intelligent_root_cause'
            }
            
            # 1. æ—¶é—´è¶‹åŠ¿åˆ†æ
            trend_analysis = self._analyze_time_trend(metric_name, date_str, conn)
            if trend_analysis and trend_analysis.get('significance', 0) > 0.15:
                analysis_results['root_causes'].append({
                    'dimension': 'æ—¶é—´è¶‹åŠ¿',
                    'finding': trend_analysis.get('description', 'è¶‹åŠ¿å¼‚å¸¸'),
                    'impact_score': trend_analysis.get('significance', 0),
                    'details': [trend_analysis.get('description', '')]
                })
                analysis_results['confidence_score'] += trend_analysis.get('significance', 0) * 0.3
            
            # 2. ç»“æ„æ€§å˜åŒ–åˆ†æ
            structural_analysis = self._analyze_structural_changes(metric_name, date_str, conn)
            if structural_analysis and structural_analysis.get('significance', 0) > 0.1:
                analysis_results['root_causes'].append({
                    'dimension': 'ç»“æ„å˜åŒ–',
                    'finding': structural_analysis.get('description', 'ç»“æ„å˜åŒ–å¼‚å¸¸'),
                    'impact_score': structural_analysis.get('significance', 0),
                    'details': [structural_analysis.get('description', '')]
                })
                analysis_results['confidence_score'] += structural_analysis.get('significance', 0) * 0.2
            
            # 3. å¤–éƒ¨å› ç´ åˆ†æ
            external_analysis = self._analyze_external_factors(metric_name, date_str, conn)
            if external_analysis and external_analysis.get('significance', 0) > 0.1:
                analysis_results['root_causes'].append({
                    'dimension': 'å¤–éƒ¨å› ç´ ',
                    'finding': external_analysis['description'],
                    'impact_score': external_analysis['significance'],
                    'details': external_analysis['details']
                })
                analysis_results['confidence_score'] += external_analysis['significance'] * 0.2
            
            # 4. CPAå¼‚å¸¸ä¸“é¡¹åˆ†æï¼ˆæ–°å¢ï¼‰
            if 'CPA' in metric_name:
                print(f"DEBUG: æ£€æµ‹åˆ°CPAå¼‚å¸¸ï¼Œå¯åŠ¨ä¸“é¡¹æ¸ é“åˆ†æ")
                cpa_channel_analysis = self._analyze_cpa_by_channel(
                    channel_df if channel_df is not None else pd.DataFrame(), 
                    prev_channel_df if prev_channel_df is not None else pd.DataFrame(), 
                    anomaly, 
                    date_str, 
                    conn
                )
                if cpa_channel_analysis and isinstance(cpa_channel_analysis, dict) and cpa_channel_analysis.get('channel_impacts'):
                    analysis_results['root_causes'].append({
                        'dimension': 'CPAæ¸ é“åˆ†æ',
                        'finding': cpa_channel_analysis.get('analysis_summary', 'CPAå¼‚å¸¸æ¸ é“å®šä½å®Œæˆ'),
                        'impact_score': 0.8,
                        'details': [f"è¯†åˆ«{len(cpa_channel_analysis.get('channel_impacts', []))}ä¸ªé«˜å½±å“æ¸ é“"],
                        'cpa_channel_detail': cpa_channel_analysis
                    })
                    analysis_results['confidence_score'] += 0.3
            
            # 5. ARPUå¼‚å¸¸ä¸“é¡¹åˆ†æ
            elif 'ARPU' in metric_name:
                print(f"DEBUG: æ£€æµ‹åˆ°ARPUå¼‚å¸¸ï¼Œå¯åŠ¨ä¸“é¡¹æ¸ é“åˆ†æ")
                arpu_channel_analysis = self._analyze_arpu_by_channel(
                    channel_df if channel_df is not None else pd.DataFrame(), 
                    prev_channel_df if prev_channel_df is not None else pd.DataFrame(), 
                    anomaly, 
                    date_str, 
                    conn
                )
                if arpu_channel_analysis and isinstance(arpu_channel_analysis, dict) and arpu_channel_analysis.get('channel_impacts'):
                    analysis_results['root_causes'].append({
                        'dimension': 'ARPUæ¸ é“åˆ†æ',
                        'finding': arpu_channel_analysis.get('analysis_summary', 'ARPUå¼‚å¸¸æ¸ é“å®šä½å®Œæˆ'),
                        'impact_score': 0.8,
                        'details': [f"è¯†åˆ«{len(arpu_channel_analysis.get('channel_impacts', []))}ä¸ªé«˜å½±å“æ¸ é“"],
                        'arpu_channel_detail': arpu_channel_analysis
                    })
                    analysis_results['confidence_score'] += 0.3
            
            # 6. Goodç‡ä¸“é¡¹åˆ†æ
            elif metric_name == 'Goodç‡':
                print(f"DEBUG: æ£€æµ‹åˆ°{metric_name}å¼‚å¸¸ï¼Œå¯åŠ¨ä¸“é¡¹æ¸ é“åˆ†æ")
                good_rate_channel_analysis = self._analyze_good_rate_by_channel(
                    None, None, anomaly, date_str, conn
                )
                if good_rate_channel_analysis and isinstance(good_rate_channel_analysis, dict) and good_rate_channel_analysis.get('channel_impacts'):
                    analysis_results['root_causes'].append({
                        'dimension': 'Goodç‡æ¸ é“åˆ†æ',
                        'finding': good_rate_channel_analysis.get('analysis_summary', 'Goodç‡å¼‚å¸¸æ¸ é“å®šä½å®Œæˆ'),
                        'impact_score': 0.8,
                        'details': [f"è¯†åˆ«{len(good_rate_channel_analysis.get('channel_impacts', []))}ä¸ªé«˜å½±å“æ¸ é“"],
                        'good_rate_channel_detail': good_rate_channel_analysis
                    })
                    analysis_results['confidence_score'] += 0.3
            
            # 7. è®¤è¯ç‡ä¸“é¡¹åˆ†æ
            elif metric_name == 'è®¤è¯ç‡':
                print(f"DEBUG: æ£€æµ‹åˆ°{metric_name}å¼‚å¸¸ï¼Œå¯åŠ¨ä¸“é¡¹æ¸ é“åˆ†æ")
                verified_rate_channel_analysis = self._analyze_verified_rate_by_channel(
                    None, None, anomaly, date_str, conn
                )
                if verified_rate_channel_analysis and isinstance(verified_rate_channel_analysis, dict) and verified_rate_channel_analysis.get('channel_impacts'):
                    analysis_results['root_causes'].append({
                        'dimension': 'è®¤è¯ç‡æ¸ é“åˆ†æ',
                        'finding': verified_rate_channel_analysis.get('analysis_summary', 'è®¤è¯ç‡å¼‚å¸¸æ¸ é“å®šä½å®Œæˆ'),
                        'impact_score': 0.8,
                        'details': [f"è¯†åˆ«{len(verified_rate_channel_analysis.get('channel_impacts', []))}ä¸ªé«˜å½±å“æ¸ é“"],
                        'verified_rate_channel_detail': verified_rate_channel_analysis
                    })
                    analysis_results['confidence_score'] += 0.3
            
            # 8. Goodä¸”è®¤è¯ç”¨æˆ·æ•°ä¸“é¡¹åˆ†æ
            elif metric_name == 'Goodä¸”è®¤è¯ç”¨æˆ·æ•°':
                print(f"DEBUG: æ£€æµ‹åˆ°{metric_name}å¼‚å¸¸ï¼Œå¯åŠ¨ä¸“é¡¹æ¸ é“åˆ†æ")
                quality_users_channel_analysis = self._analyze_quality_users_by_channel_enhanced(
                    channel_df if channel_df is not None else pd.DataFrame(), 
                    prev_channel_df if prev_channel_df is not None else pd.DataFrame(), 
                    anomaly, date_str, conn
                )
                if quality_users_channel_analysis and isinstance(quality_users_channel_analysis, dict) and quality_users_channel_analysis.get('channel_impacts'):
                    analysis_results['root_causes'].append({
                        'dimension': 'Goodä¸”è®¤è¯ç”¨æˆ·æ•°æ¸ é“åˆ†æ',
                        'finding': quality_users_channel_analysis.get('analysis_summary', 'Goodä¸”è®¤è¯ç”¨æˆ·æ•°å¼‚å¸¸æ¸ é“å®šä½å®Œæˆ'),
                        'impact_score': 0.8,
                        'details': [f"è¯†åˆ«{len(quality_users_channel_analysis.get('channel_impacts', []))}ä¸ªé«˜å½±å“æ¸ é“"],
                        'quality_users_channel_detail': quality_users_channel_analysis
                    })
                    analysis_results['confidence_score'] += 0.3
            
            # 9. Goodä¸”è®¤è¯ç‡ä¸“é¡¹åˆ†æ  
            elif metric_name == 'Goodä¸”è®¤è¯ç‡':
                print(f"DEBUG: æ£€æµ‹åˆ°{metric_name}å¼‚å¸¸ï¼Œå¯åŠ¨ä¸“é¡¹æ¸ é“åˆ†æ")
                quality_rate_channel_analysis = self._analyze_quality_rate_by_channel(
                    channel_df if channel_df is not None else pd.DataFrame(), 
                    prev_channel_df if prev_channel_df is not None else pd.DataFrame(), 
                    anomaly, date_str, conn
                )
                if quality_rate_channel_analysis and isinstance(quality_rate_channel_analysis, dict) and quality_rate_channel_analysis.get('channel_impacts'):
                    analysis_results['root_causes'].append({
                        'dimension': 'Goodä¸”è®¤è¯ç‡æ¸ é“åˆ†æ',
                        'finding': quality_rate_channel_analysis.get('analysis_summary', 'Goodä¸”è®¤è¯ç‡å¼‚å¸¸æ¸ é“å®šä½å®Œæˆ'),
                        'impact_score': 0.8,
                        'details': [f"è¯†åˆ«{len(quality_rate_channel_analysis.get('channel_impacts', []))}ä¸ªé«˜å½±å“æ¸ é“"],
                        'quality_rate_channel_detail': quality_rate_channel_analysis
                    })
                    analysis_results['confidence_score'] += 0.3

            # 10. å…¶ä»–æŒ‡æ ‡çš„é€šç”¨æ¸ é“åˆ†æ
            elif metric_name in ['å¥³æ€§å æ¯”', 'å¹´è½»å æ¯”ï¼ˆ20-23å²ï¼‰', 'é«˜çº¿åŸå¸‚å æ¯”', 'æ¬¡ç•™ç‡']:
                print(f"DEBUG: æ£€æµ‹åˆ°{metric_name}å¼‚å¸¸ï¼Œå¯åŠ¨ä¸“é¡¹æ¸ é“åˆ†æ")
                metric_field_map = {
                    'å¥³æ€§å æ¯”': 'female_ratio',
                    'å¹´è½»å æ¯”ï¼ˆ20-23å²ï¼‰': 'young_ratio', 
                    'é«˜çº¿åŸå¸‚å æ¯”': 'high_tier_ratio',
                    'æ¬¡ç•™ç‡': 'retention_rate'
                }
                metric_field = metric_field_map.get(metric_name)
                if metric_field:
                    generic_channel_analysis = self._analyze_generic_metric_by_channel(
                        metric_field, metric_name, date_str, conn, anomaly
                    )
                    if generic_channel_analysis and isinstance(generic_channel_analysis, dict) and generic_channel_analysis.get('channel_impacts'):
                        analysis_results['root_causes'].append({
                            'dimension': f'{metric_name}æ¸ é“åˆ†æ',
                            'finding': generic_channel_analysis.get('analysis_summary', f'{metric_name}å¼‚å¸¸æ¸ é“å®šä½å®Œæˆ'),
                            'impact_score': 0.8,
                            'details': [f"è¯†åˆ«{len(generic_channel_analysis.get('channel_impacts', []))}ä¸ªå¼‚å¸¸æ¸ é“"],
                            'generic_channel_detail': generic_channel_analysis
                        })
                        analysis_results['confidence_score'] += 0.3
            
            # ç”Ÿæˆç»¼åˆåˆ†ææ€»ç»“
            analysis_results['analysis_summary'] = self._generate_analysis_summary(analysis_results)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜æ˜¾çš„æ ¹å› ï¼Œè¿›è¡Œå…œåº•åˆ†æ
            if not analysis_results['root_causes']:
                fallback_analysis = self._fallback_analysis(metric_name, anomaly, date_str, conn)
                analysis_results['root_causes'].append(fallback_analysis)
                analysis_results['confidence_score'] = 0.3
            
            # æŒ‰å½±å“åˆ†æ•°æ’åº
            analysis_results['root_causes'].sort(key=lambda x: x['impact_score'], reverse=True)
            
            return analysis_results if analysis_results['root_causes'] else None
        
        except Exception as e:
            print(f"æ™ºèƒ½æ ¹å› åˆ†æå¤±è´¥: {e}")
            return None
    
    def _fallback_analysis(self, metric_name: str, anomaly: dict, date_str: str, conn: sqlite3.Connection) -> dict:
        """å…œåº•åˆ†æ - å½“å…¶ä»–åˆ†æéƒ½æ²¡æœ‰å‘ç°æ˜æ˜¾åŸå› æ—¶"""
        return {
            'dimension': 'åŸºç¡€åˆ†æ',
            'finding': f"{metric_name}å‡ºç°{anomaly.get('type', 'å¼‚å¸¸')}ï¼Œå½“å‰å€¼ä¸º{anomaly.get('current_value', 'N/A')}",
            'impact_score': 0.3,
            'details': ['å»ºè®®è¿›ä¸€æ­¥äººå·¥åˆ†æ', 'å…³æ³¨åç»­æ•°æ®å˜åŒ–è¶‹åŠ¿']
        }
    
    def _generate_analysis_summary(self, analysis_results: dict) -> str:
        """ç”Ÿæˆåˆ†ææ€»ç»“"""
        if not analysis_results['root_causes']:
            return "æœªå‘ç°æ˜æ˜¾å¼‚å¸¸åŸå› "
        
        # è·å–æœ€é«˜å½±å“åˆ†æ•°çš„æ ¹å› 
        top_cause = max(analysis_results['root_causes'], key=lambda x: x['impact_score'])
        confidence_level = "é«˜" if analysis_results['confidence_score'] > 0.7 else "ä¸­" if analysis_results['confidence_score'] > 0.4 else "ä½"
        
        return f"ä¸»è¦åŸå› ï¼š{top_cause['finding']} (ç½®ä¿¡åº¦ï¼š{confidence_level})"
    
    def _analyze_time_trend(self, metric_name: str, date_str: str, conn: sqlite3.Connection) -> dict:
        """åˆ†ææ—¶é—´è¶‹åŠ¿å¼‚å¸¸ï¼ˆåŸºäºå®é™…æ•°æ®è¡¨ï¼‰"""
        try:
            # è·å–æœ€è¿‘7å¤©çš„è¶‹åŠ¿æ•°æ®
            if 'ARPU' in metric_name:
                trend_query = f"""
                    SELECT dt,
                           SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN zizhu_revenue_1_aftertax ELSE 0 END) / 
                           SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END) as metric_value
                    FROM cpz_qs_newuser_channel_i_d 
                    WHERE dt >= date('{date_str}', '-6 days') AND dt <= '{date_str}'
                    GROUP BY dt
                    ORDER BY dt
                """
            elif 'å¥³æ€§å æ¯”' in metric_name:
                trend_query = f"""
                    SELECT dt,
                           SUM(CASE WHEN gender = 'female' AND status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END) * 100.0 / 
                           SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END) as metric_value
                    FROM cpz_qs_newuser_channel_i_d 
                    WHERE dt >= date('{date_str}', '-6 days') AND dt <= '{date_str}'
                    GROUP BY dt
                    ORDER BY dt
                """
            elif 'Goodä¸”è®¤è¯ç”¨æˆ·æ•°' in metric_name:
                trend_query = f"""
                    SELECT dt,
                           SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END) as metric_value
                    FROM cpz_qs_newuser_channel_i_d 
                    WHERE dt >= date('{date_str}', '-6 days') AND dt <= '{date_str}'
                    GROUP BY dt
                    ORDER BY dt
                """
            else:
                return {'significance': 0.1, 'description': f'ä¸æ”¯æŒ{metric_name}çš„æ—¶é—´è¶‹åŠ¿åˆ†æ', 'details': []}
            
            trend_df = pd.read_sql_query(trend_query, conn)
            
            if len(trend_df) < 3:
                return {'significance': 0.2, 'description': 'å†å²æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œè¶‹åŠ¿åˆ†æ', 'details': []}
            
            # åˆ†æè¶‹åŠ¿æ¨¡å¼
            values = trend_df['metric_value'].values
            dates = trend_df['dt'].values
            current_value = values[-1]
            
            # è®¡ç®—è¶‹åŠ¿å˜åŒ–
            if len(values) >= 4:
                recent_avg = np.mean(values[-4:-1])  # å‰3å¤©å¹³å‡
                change_rate = (current_value - recent_avg) / recent_avg if recent_avg > 0 else 0
                
                # åˆ¤æ–­è¶‹åŠ¿æ˜¾è‘—æ€§ - é’ˆå¯¹æ•æ„Ÿå¼‚å¸¸ä½¿ç”¨æ›´ä½é˜ˆå€¼
                threshold = 0.05 if abs(change_rate) > 0.05 else 0.15  # 5%ä»¥ä¸Šå˜åŒ–å³è®¤ä¸ºæ˜¾è‘—
                if abs(change_rate) > threshold:
                    trend_direction = "ä¸Šå‡" if change_rate > 0 else "ä¸‹é™"
                    significance = min(abs(change_rate) * 3, 1.0)  # æé«˜æ˜¾è‘—æ€§è®¡ç®—
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯è¿ç»­è¶‹åŠ¿
                    is_continuous = self._check_continuous_trend(values)
                    
                    description = f"è¿‘æœŸ{trend_direction}è¶‹åŠ¿æ˜æ˜¾ï¼Œç›¸æ¯”å‰3å¤©å¹³å‡å€¼{trend_direction}{abs(change_rate)*100:.1f}%"
                    if is_continuous:
                        description += "ï¼Œå‘ˆç°è¿ç»­æ€§å˜åŒ–ç‰¹å¾"
                        significance += 0.2
                    
                    return {
                        'significance': min(significance, 1.0),
                        'description': description,
                        'details': [
                            f"å½“å‰å€¼: {current_value:.2f}",
                            f"å‰3å¤©å‡å€¼: {recent_avg:.2f}",
                            f"å˜åŒ–å¹…åº¦: {change_rate*100:+.1f}%",
                            f"è¿ç»­è¶‹åŠ¿: {'æ˜¯' if is_continuous else 'å¦'}",
                            f"æ•°æ®æ—¶é—´èŒƒå›´: {dates[0]} è‡³ {dates[-1]}"
                        ]
                    }
            
            return {'significance': 0.3, 'description': 'æ—¶é—´è¶‹åŠ¿ç›¸å¯¹ç¨³å®š', 'details': [f'è¿‘{len(values)}å¤©æ•°æ®æ³¢åŠ¨è¾ƒå°']}
            
        except Exception as e:
            return {'significance': 0, 'description': f'æ—¶é—´è¶‹åŠ¿åˆ†æå¤±è´¥: {e}', 'details': []}
    
    def _analyze_structural_changes(self, metric_name: str, date_str: str, conn: sqlite3.Connection) -> dict:
        """åˆ†æç»“æ„æ€§å˜åŒ–ï¼ˆåŸºäºçœŸå®æ•°æ®åº“ç»´åº¦ï¼šå¹´é¾„æ®µã€æ“ä½œç³»ç»Ÿã€åŸå¸‚ç­‰çº§ç­‰ï¼‰"""
        try:
            findings = []
            max_significance = 0
            details = []
            
            # è·å–å‰ä¸€å¤©æ—¥æœŸ
            prev_date = (datetime.strptime(date_str, '%Y-%m-%d') - timedelta(days=1)).strftime('%Y-%m-%d')
            
            # 1. å¹´é¾„æ®µç»“æ„å˜åŒ–åˆ†æ
            age_analysis = self._analyze_age_group_impact(metric_name, date_str, prev_date, conn)
            if age_analysis['significance'] > 0.6:
                findings.append(f"å¹´é¾„æ®µç»“æ„: {age_analysis['description']}")
                max_significance = max(max_significance, age_analysis['significance'])
                details.extend(age_analysis['details'])
            
            # 2. æ“ä½œç³»ç»Ÿåˆ†å¸ƒå˜åŒ–
            os_analysis = self._analyze_os_type_impact(metric_name, date_str, prev_date, conn)
            if os_analysis['significance'] > 0.6:
                findings.append(f"æ“ä½œç³»ç»Ÿåˆ†å¸ƒ: {os_analysis['description']}")
                max_significance = max(max_significance, os_analysis['significance'])
                details.extend(os_analysis['details'])
            
            # 3. åŸå¸‚ç­‰çº§åˆ†å¸ƒå˜åŒ–
            city_analysis = self._analyze_city_level_impact(metric_name, date_str, prev_date, conn)
            if city_analysis['significance'] > 0.6:
                findings.append(f"åŸå¸‚ç­‰çº§åˆ†å¸ƒ: {city_analysis['description']}")
                max_significance = max(max_significance, city_analysis['significance'])
                details.extend(city_analysis['details'])
            
            # 4. æ€§åˆ«åˆ†å¸ƒå˜åŒ–ï¼ˆé’ˆå¯¹å¥³æ€§å æ¯”å¼‚å¸¸ï¼‰
            if 'å¥³æ€§å æ¯”' in metric_name:
                gender_analysis = self._analyze_gender_distribution_change(date_str, prev_date, conn)
                if gender_analysis['significance'] > 0.6:
                    findings.append(f"æ€§åˆ«åˆ†å¸ƒ: {gender_analysis['description']}")
                    max_significance = max(max_significance, gender_analysis['significance'])
                    details.extend(gender_analysis['details'])
            
            if findings:
                return {
                    'significance': max_significance,
                    'description': '; '.join(findings),
                    'details': details
                }
            
            return {'significance': 0.3, 'description': 'æœªå‘ç°æ˜¾è‘—ç»“æ„æ€§å˜åŒ–', 'details': ['å„ç»´åº¦åˆ†å¸ƒç›¸å¯¹ç¨³å®š']}
            
        except Exception as e:
            return {'significance': 0, 'description': f'ç»“æ„å˜åŒ–åˆ†æå¤±è´¥: {e}', 'details': []}
    
    def _analyze_external_factors(self, metric_name: str, date_str: str, conn: sqlite3.Connection) -> dict:
        """åˆ†æå¤–éƒ¨å› ç´ å½±å“"""
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç‰¹æ®Šæ—¥æœŸ
            date_obj = datetime.strptime(date_str, '%Y-%m-%d')
            weekday = date_obj.weekday()
            
            findings = []
            significance = 0
            
            # å‘¨æœ«æ•ˆåº”åˆ†æ
            if weekday >= 5:  # å‘¨å…­ã€å‘¨æ—¥
                weekend_impact = self._analyze_weekend_effect(metric_name, date_str, conn)
                if weekend_impact['significance'] > 0.3:
                    findings.append(f"å‘¨æœ«æ•ˆåº”: {weekend_impact['description']}")
                    significance = max(significance, weekend_impact['significance'])
            
            # èŠ‚å‡æ—¥æ•ˆåº”ï¼ˆç®€åŒ–ç‰ˆï¼Œå¯ä»¥æ‰©å±•ï¼‰
            holiday_impact = self._check_holiday_effect(date_str)
            if holiday_impact['significance'] > 0.3:
                findings.append(f"èŠ‚å‡æ—¥æ•ˆåº”: {holiday_impact['description']}")
                significance = max(significance, holiday_impact['significance'])
            
            # å­£èŠ‚æ€§æ•ˆåº”
            seasonal_impact = self._analyze_seasonal_effect(metric_name, date_str, conn)
            if seasonal_impact['significance'] > 0.3:
                findings.append(f"å­£èŠ‚æ€§æ•ˆåº”: {seasonal_impact['description']}")
                significance = max(significance, seasonal_impact['significance'])
            
            if findings:
                return {
                    'significance': significance,
                    'description': '; '.join(findings),
                    'details': findings
                }
            
            return {'significance': 0.1, 'description': 'æœªå‘ç°æ˜¾è‘—å¤–éƒ¨å› ç´ å½±å“', 'details': []}
            
        except Exception as e:
            return {'significance': 0, 'description': f'å¤–éƒ¨å› ç´ åˆ†æå¤±è´¥: {e}', 'details': []}
    
    def _analyze_data_quality_issues(self, metric_name: str, date_str: str, conn: sqlite3.Connection) -> dict:
        """åˆ†ææ•°æ®è´¨é‡å¼‚å¸¸"""
        try:
            findings = []
            significance = 0
            
            # 1. æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
            completeness_check = self._check_data_completeness(date_str, conn)
            if completeness_check['significance'] > 0.5:
                findings.append(f"æ•°æ®å®Œæ•´æ€§: {completeness_check['description']}")
                significance = max(significance, completeness_check['significance'])
            
            # 2. å¼‚å¸¸å€¼æ£€æµ‹
            outlier_check = self._check_outliers(metric_name, date_str, conn)
            if outlier_check['significance'] > 0.5:
                findings.append(f"å¼‚å¸¸å€¼: {outlier_check['description']}")
                significance = max(significance, outlier_check['significance'])
            
            # 3. æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
            consistency_check = self._check_data_consistency(date_str, conn)
            if consistency_check['significance'] > 0.5:
                findings.append(f"æ•°æ®ä¸€è‡´æ€§: {consistency_check['description']}")
                significance = max(significance, consistency_check['significance'])
            
            if findings:
                return {
                    'significance': significance,
                    'description': '; '.join(findings),
                    'details': findings
                }
            
            return {'significance': 0.1, 'description': 'æ•°æ®è´¨é‡æ­£å¸¸', 'details': []}
            
        except Exception as e:
            return {'significance': 0, 'description': f'æ•°æ®è´¨é‡åˆ†æå¤±è´¥: {e}', 'details': []}
    
    def _check_continuous_trend(self, values) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºè¿ç»­è¶‹åŠ¿"""
        if len(values) < 3:
            return False
        
        # æ£€æŸ¥è¿ç»­ä¸Šå‡æˆ–ä¸‹é™
        increasing = all(values[i] <= values[i+1] for i in range(len(values)-1))
        decreasing = all(values[i] >= values[i+1] for i in range(len(values)-1))
        
        return increasing or decreasing
    
    def _analyze_channel_structure_change(self, metric_name: str, date_str: str, conn: sqlite3.Connection) -> dict:
        """åˆ†ææ¸ é“ç»“æ„å˜åŒ–"""
        try:
            # æ¯”è¾ƒä»Šæ—¥å’Œå‰æ—¥çš„æ¸ é“åˆ†å¸ƒ
            today_query = """
                SELECT ad_channel, SUM(quality_users) as users
                FROM aggregated_daily_data 
                WHERE dt = ?
                GROUP BY ad_channel
            """
            
            yesterday_query = """
                SELECT ad_channel, SUM(quality_users) as users
                FROM aggregated_daily_data 
                WHERE dt = date(?, '-1 day')
                GROUP BY ad_channel
            """
            
            today_df = pd.read_sql_query(today_query, conn, params=[date_str])
            yesterday_df = pd.read_sql_query(yesterday_query, conn, params=[date_str])
            
            if today_df.empty or yesterday_df.empty:
                return {'significance': 0, 'description': 'æ•°æ®ä¸è¶³', 'details': []}
            
            # è®¡ç®—æ¸ é“åˆ†å¸ƒå˜åŒ–
            today_total = today_df['users'].sum()
            yesterday_total = yesterday_df['users'].sum()
            
            major_changes = []
            for _, row in today_df.iterrows():
                channel = row['ad_channel']
                today_pct = row['users'] / today_total if today_total > 0 else 0
                
                yesterday_row = yesterday_df[yesterday_df['ad_channel'] == channel]
                if not yesterday_row.empty:
                    yesterday_pct = yesterday_row.iloc[0]['users'] / yesterday_total if yesterday_total > 0 else 0
                    change = today_pct - yesterday_pct
                    
                    if abs(change) > 0.05 and today_pct > 0.1:  # 5%ä»¥ä¸Šå˜åŒ–ä¸”å æ¯”è¶…è¿‡10%
                        direction = "å¢åŠ " if change > 0 else "å‡å°‘"
                        major_changes.append(f"{channel}æ¸ é“å æ¯”{direction}{abs(change)*100:.1f}%")
            
            if major_changes:
                return {
                    'significance': min(len(major_changes) * 0.3, 1.0),
                    'description': '; '.join(major_changes[:3]),  # åªæ˜¾ç¤ºå‰3ä¸ª
                    'details': major_changes
                }
            
            return {'significance': 0.2, 'description': 'æ¸ é“ç»“æ„åŸºæœ¬ç¨³å®š', 'details': []}
            
        except Exception as e:
            return {'significance': 0, 'description': f'æ¸ é“ç»“æ„åˆ†æå¤±è´¥: {e}', 'details': []}
    
    def _analyze_user_group_changes(self, metric_name: str, date_str: str, conn: sqlite3.Connection) -> dict:
        """åˆ†æç”¨æˆ·ç¾¤ä½“å˜åŒ–"""
        try:
            # ç®€åŒ–ç‰ˆï¼šåˆ†æç”¨æˆ·è´¨é‡åˆ†å¸ƒå˜åŒ–
            today_query = """
                SELECT 
                    SUM(CASE WHEN female_users > 0 THEN quality_users ELSE 0 END) as female_users,
                    SUM(CASE WHEN young_users > 0 THEN quality_users ELSE 0 END) as young_users,
                    SUM(quality_users) as total_users
                FROM aggregated_daily_data 
                WHERE dt = ?
            """
            
            yesterday_query = """
                SELECT 
                    SUM(CASE WHEN female_users > 0 THEN quality_users ELSE 0 END) as female_users,
                    SUM(CASE WHEN young_users > 0 THEN quality_users ELSE 0 END) as young_users,
                    SUM(quality_users) as total_users
                FROM aggregated_daily_data 
                WHERE dt = date(?, '-1 day')
            """
            
            today_data = pd.read_sql_query(today_query, conn, params=[date_str]).iloc[0]
            yesterday_data = pd.read_sql_query(yesterday_query, conn, params=[date_str]).iloc[0]
            
            changes = []
            significance = 0
            
            # åˆ†æå¥³æ€§å æ¯”å˜åŒ–
            if today_data['total_users'] > 0 and yesterday_data['total_users'] > 0:
                today_female_pct = today_data['female_users'] / today_data['total_users']
                yesterday_female_pct = yesterday_data['female_users'] / yesterday_data['total_users']
                female_change = today_female_pct - yesterday_female_pct
                
                if abs(female_change) > 0.05:  # 5%ä»¥ä¸Šå˜åŒ–
                    direction = "æå‡" if female_change > 0 else "ä¸‹é™"
                    changes.append(f"å¥³æ€§ç”¨æˆ·å æ¯”{direction}{abs(female_change)*100:.1f}%")
                    significance = max(significance, abs(female_change) * 2)
            
            if changes:
                return {
                    'significance': min(significance, 1.0),
                    'description': '; '.join(changes),
                    'details': changes
                }
            
            return {'significance': 0.1, 'description': 'ç”¨æˆ·ç¾¤ä½“ç»“æ„åŸºæœ¬ç¨³å®š', 'details': []}
            
        except Exception as e:
            return {'significance': 0, 'description': f'ç”¨æˆ·ç¾¤ä½“åˆ†æå¤±è´¥: {e}', 'details': []}
    
    def _analyze_regional_changes(self, metric_name: str, date_str: str, conn: sqlite3.Connection) -> dict:
        """åˆ†æåœ°åŸŸåˆ†å¸ƒå˜åŒ–ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # ç”±äºæ²¡æœ‰å…·ä½“çš„åœ°åŸŸæ•°æ®ï¼Œè¿”å›åŸºç¡€åˆ†æ
        return {'significance': 0.1, 'description': 'åœ°åŸŸåˆ†å¸ƒæ•°æ®å¾…å®Œå–„', 'details': []}
    
    def _analyze_weekend_effect(self, metric_name: str, date_str: str, conn: sqlite3.Connection) -> dict:
        """åˆ†æå‘¨æœ«æ•ˆåº”"""
        try:
            # è·å–æœ€è¿‘å‡ ä¸ªå‘¨æœ«çš„æ•°æ®è¿›è¡Œå¯¹æ¯”
            weekend_query = """
                SELECT dt, 
                       CASE 
                           WHEN ? = 'ARPU' THEN CAST(SUM(revenue) AS FLOAT) / CAST(SUM(quality_users) AS FLOAT)
                           WHEN ? = 'Goodä¸”è®¤è¯ç”¨æˆ·æ•°' THEN CAST(SUM(quality_users) AS FLOAT)
                           ELSE 0
                       END as metric_value
                FROM aggregated_daily_data 
                WHERE dt >= date(?, '-14 days') AND dt <= ?
                AND strftime('%w', dt) IN ('0', '6')  -- å‘¨æœ«
                GROUP BY dt
                ORDER BY dt DESC
                LIMIT 3
            """
            
            weekend_df = pd.read_sql_query(weekend_query, conn, params=[metric_name, metric_name, date_str, date_str])
            
            if len(weekend_df) >= 2:
                current_value = weekend_df.iloc[0]['metric_value']
                avg_prev_weekends = weekend_df.iloc[1:]['metric_value'].mean()
                
                if avg_prev_weekends > 0:
                    change_rate = (current_value - avg_prev_weekends) / avg_prev_weekends
                    
                    if abs(change_rate) > 0.2:  # 20%ä»¥ä¸Šå˜åŒ–
                        direction = "é«˜äº" if change_rate > 0 else "ä½äº"
                        return {
                            'significance': min(abs(change_rate), 1.0),
                            'description': f'å‘¨æœ«è¡¨ç°{direction}å†å²åŒæœŸ{abs(change_rate)*100:.1f}%',
                            'details': [f'å½“å‰å€¼: {current_value:.2f}', f'å†å²å‘¨æœ«å‡å€¼: {avg_prev_weekends:.2f}']
                        }
            
            return {'significance': 0.2, 'description': 'å‘¨æœ«æ•ˆåº”ä¸æ˜æ˜¾', 'details': []}
            
        except Exception as e:
            return {'significance': 0, 'description': f'å‘¨æœ«æ•ˆåº”åˆ†æå¤±è´¥: {e}', 'details': []}
    
    def _check_holiday_effect(self, date_str: str) -> dict:
        """æ£€æŸ¥èŠ‚å‡æ—¥æ•ˆåº”ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # è¿™é‡Œå¯ä»¥æ‰©å±•ä¸ºå®Œæ•´çš„èŠ‚å‡æ—¥æ•°æ®åº“
        date_obj = datetime.strptime(date_str, '%Y-%m-%d')
        month_day = f"{date_obj.month:02d}-{date_obj.day:02d}"
        
        holidays = {
            '01-01': 'å…ƒæ—¦',
            '02-14': 'æƒ…äººèŠ‚',
            '05-01': 'åŠ³åŠ¨èŠ‚',
            '10-01': 'å›½åº†èŠ‚',
            '12-25': 'åœ£è¯èŠ‚'
        }
        
        if month_day in holidays:
            return {
                'significance': 0.8,
                'description': f'{holidays[month_day]}èŠ‚å‡æ—¥æ•ˆåº”',
                'details': [f'èŠ‚å‡æ—¥: {holidays[month_day]}']
            }
        
        return {'significance': 0, 'description': 'éèŠ‚å‡æ—¥', 'details': []}
    
    def _analyze_seasonal_effect(self, metric_name: str, date_str: str, conn: sqlite3.Connection) -> dict:
        """åˆ†æå­£èŠ‚æ€§æ•ˆåº”ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        try:
            date_obj = datetime.strptime(date_str, '%Y-%m-%d')
            current_month = date_obj.month
            
            # ç®€åŒ–çš„å­£èŠ‚æ€§åˆ¤æ–­
            seasonal_patterns = {
                'ARPU': {1: 'å¹´åˆæ•ˆåº”', 2: 'æ˜¥èŠ‚æ•ˆåº”', 12: 'å¹´åº•æ•ˆåº”'},
                'Goodä¸”è®¤è¯ç”¨æˆ·æ•°': {9: 'å¼€å­¦å­£æ•ˆåº”', 12: 'å¹´åº•å†²é‡'}
            }
            
            if metric_name in seasonal_patterns and current_month in seasonal_patterns[metric_name]:
                return {
                    'significance': 0.6,
                    'description': seasonal_patterns[metric_name][current_month],
                    'details': [f'æœˆä»½: {current_month}æœˆ']
                }
            
            return {'significance': 0.1, 'description': 'æ— æ˜æ˜¾å­£èŠ‚æ€§æ•ˆåº”', 'details': []}
            
        except Exception as e:
            return {'significance': 0, 'description': f'å­£èŠ‚æ€§åˆ†æå¤±è´¥: {e}', 'details': []}
    
    def _check_data_completeness(self, date_str: str, conn: sqlite3.Connection) -> dict:
        """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸çš„æ•°æ®ç¼ºå¤±
            completeness_query = """
                SELECT 
                    COUNT(*) as record_count,
                    COUNT(DISTINCT ad_channel) as channel_count,
                    SUM(CASE WHEN quality_users = 0 THEN 1 ELSE 0 END) as zero_user_records
                FROM aggregated_daily_data 
                WHERE dt = ?
            """
            
            result = pd.read_sql_query(completeness_query, conn, params=[date_str]).iloc[0]
            
            # ä¸å‰ä¸€å¤©å¯¹æ¯”
            prev_day_query = """
                SELECT 
                    COUNT(*) as record_count,
                    COUNT(DISTINCT ad_channel) as channel_count
                FROM aggregated_daily_data 
                WHERE dt = date(?, '-1 day')
            """
            
            prev_result = pd.read_sql_query(prev_day_query, conn, params=[date_str]).iloc[0]
            
            issues = []
            significance = 0
            
            # æ£€æŸ¥è®°å½•æ•°é‡å˜åŒ–
            if prev_result['record_count'] > 0:
                record_change = (result['record_count'] - prev_result['record_count']) / prev_result['record_count']
                if abs(record_change) > 0.3:  # 30%ä»¥ä¸Šå˜åŒ–
                    direction = "å¢åŠ " if record_change > 0 else "å‡å°‘"
                    issues.append(f"æ•°æ®è®°å½•{direction}{abs(record_change)*100:.1f}%")
                    significance = max(significance, abs(record_change))
            
            # æ£€æŸ¥æ¸ é“æ•°é‡å˜åŒ–  
            if prev_result['channel_count'] > 0:
                channel_change = (result['channel_count'] - prev_result['channel_count']) / prev_result['channel_count']
                if abs(channel_change) > 0.2:  # 20%ä»¥ä¸Šå˜åŒ–
                    direction = "å¢åŠ " if channel_change > 0 else "å‡å°‘"
                    issues.append(f"æ¸ é“æ•°é‡{direction}{abs(channel_change)*100:.1f}%")
                    significance = max(significance, abs(channel_change))
            
            # æ£€æŸ¥é›¶ç”¨æˆ·è®°å½•
            if result['zero_user_records'] > result['record_count'] * 0.3:  # è¶…è¿‡30%çš„è®°å½•ä¸ºé›¶ç”¨æˆ·
                issues.append(f"å­˜åœ¨{result['zero_user_records']}æ¡é›¶ç”¨æˆ·è®°å½•")
                significance = max(significance, 0.7)
            
            if issues:
                return {
                    'significance': min(significance, 1.0),
                    'description': '; '.join(issues),
                    'details': issues
                }
            
            return {'significance': 0.1, 'description': 'æ•°æ®å®Œæ•´æ€§æ­£å¸¸', 'details': []}
            
        except Exception as e:
            return {'significance': 0, 'description': f'æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥: {e}', 'details': []}
    
    def _check_outliers(self, metric_name: str, date_str: str, conn: sqlite3.Connection) -> dict:
        """æ£€æŸ¥å¼‚å¸¸å€¼"""
        # ç®€åŒ–ç‰ˆå¼‚å¸¸å€¼æ£€æµ‹
        return {'significance': 0.1, 'description': 'å¼‚å¸¸å€¼æ£€æµ‹æ­£å¸¸', 'details': []}
    
    def _check_data_consistency(self, date_str: str, conn: sqlite3.Connection) -> dict:
        """æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§"""
        # ç®€åŒ–ç‰ˆä¸€è‡´æ€§æ£€æŸ¥
        return {'significance': 0.1, 'description': 'æ•°æ®ä¸€è‡´æ€§æ­£å¸¸', 'details': []}
    
    def _generate_analysis_summary(self, analysis_results: dict) -> str:
        """ç”Ÿæˆç»¼åˆåˆ†ææ€»ç»“"""
        if not analysis_results['root_causes']:
            return "æœªå‘ç°æ˜æ˜¾å¼‚å¸¸åŸå› "
        
        # æŒ‰å½±å“åˆ†æ•°æ’åº
        top_causes = sorted(analysis_results['root_causes'], key=lambda x: x['impact_score'], reverse=True)
        
        summary_parts = []
        for cause in top_causes[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
            summary_parts.append(f"{cause['dimension']}: {cause['finding']}")
        
        confidence_level = "é«˜" if analysis_results['confidence_score'] > 0.8 else "ä¸­" if analysis_results['confidence_score'] > 0.5 else "ä½"
        
        return f"ä¸»è¦åŸå› : {'; '.join(summary_parts)} (ç½®ä¿¡åº¦: {confidence_level})"
    
    def _fallback_analysis(self, metric_name: str, anomaly: dict, date_str: str, conn: sqlite3.Connection) -> dict:
        """å…œåº•åˆ†æ - å½“å…¶ä»–åˆ†æéƒ½æ²¡æœ‰å‘ç°æ˜æ˜¾åŸå› æ—¶"""
        return {
            'dimension': 'åŸºç¡€åˆ†æ',
            'finding': f"{metric_name}å‡ºç°{anomaly.get('type', 'å¼‚å¸¸')}ï¼Œå½“å‰å€¼ä¸º{anomaly.get('current_value', 'N/A')}",
            'impact_score': 0.3,
            'details': ['å»ºè®®è¿›ä¸€æ­¥äººå·¥åˆ†æ', 'å…³æ³¨åç»­æ•°æ®å˜åŒ–è¶‹åŠ¿']
        }
    
    def _analyze_cpa_by_channel(self, channel_df: pd.DataFrame, prev_channel_df: pd.DataFrame, 
                              anomaly: dict, date_str: str, conn: sqlite3.Connection) -> dict:
        """åˆ†æCPAå¼‚å¸¸çš„æ¸ é“å½±å“ - æ”¹è¿›ç‰ˆå¼‚å¸¸æŒ‡æ ‡æ¸ é“å®šä½åˆ†æ"""
        
        print(f"DEBUG: CPAå¼‚å¸¸æ¸ é“åˆ†æå¼€å§‹ - {date_str}")
        
        # 1. è·å–CPAå†å²è¶‹åŠ¿æ•°æ®
        prev_date = (datetime.strptime(date_str, '%Y-%m-%d') - timedelta(days=1)).strftime('%Y-%m-%d')
        
        # ä¿®æ­£çš„CPAæŸ¥è¯¢ - é¿å…JOINå¯¼è‡´çš„æ•°æ®é‡å¤
        current_cpa_sql = f"""
            WITH user_data AS (
                SELECT 
                    ad_channel,
                    SUM(CASE WHEN status='good' AND verification_status='verified' THEN newuser ELSE 0 END) as quality_users
                FROM cpz_qs_newuser_channel_i_d 
                WHERE dt = '{date_str}'
                GROUP BY ad_channel
                HAVING quality_users > 50
            ),
            cost_data AS (
                SELECT 
                    channel,
                    SUM(cash_cost) as channel_cost
                FROM dwd_ttx_market_cash_cost_i_d 
                WHERE dt = '{date_str}'
                GROUP BY channel
            )
            SELECT 
                u.ad_channel,
                u.quality_users,
                COALESCE(c.channel_cost, 0) as channel_cost,
                CASE 
                    WHEN u.quality_users > 0 AND c.channel_cost > 0
                    THEN ROUND(c.channel_cost / u.quality_users, 2)
                    ELSE NULL
                END as channel_cpa
            FROM user_data u
            LEFT JOIN cost_data c ON u.ad_channel = c.channel
            WHERE COALESCE(c.channel_cost, 0) > 0
            ORDER BY channel_cpa DESC
        """
        
        prev_cpa_sql = f"""
            WITH user_data AS (
                SELECT 
                    ad_channel,
                    SUM(CASE WHEN status='good' AND verification_status='verified' THEN newuser ELSE 0 END) as quality_users
                FROM cpz_qs_newuser_channel_i_d 
                WHERE dt = '{prev_date}'
                GROUP BY ad_channel
                HAVING quality_users > 50
            ),
            cost_data AS (
                SELECT 
                    channel,
                    SUM(cash_cost) as channel_cost
                FROM dwd_ttx_market_cash_cost_i_d 
                WHERE dt = '{prev_date}'
                GROUP BY channel
            )
            SELECT 
                u.ad_channel,
                u.quality_users,
                COALESCE(c.channel_cost, 0) as channel_cost,
                CASE 
                    WHEN u.quality_users > 0 AND c.channel_cost > 0
                    THEN ROUND(c.channel_cost / u.quality_users, 2)
                    ELSE NULL
                END as channel_cpa
            FROM user_data u
            LEFT JOIN cost_data c ON u.ad_channel = c.channel
            WHERE COALESCE(c.channel_cost, 0) > 0
        """
        
        try:
            current_df = pd.read_sql_query(current_cpa_sql, conn)
            prev_df = pd.read_sql_query(prev_cpa_sql, conn)
            
            # 2. è®¡ç®—æ•´ä½“CPA
            total_cost_current = current_df['channel_cost'].sum()
            total_users_current = current_df['quality_users'].sum()
            overall_cpa_current = total_cost_current / total_users_current if total_users_current > 0 else 0
            
            total_cost_prev = prev_df['channel_cost'].sum() if not prev_df.empty else 0
            total_users_prev = prev_df['quality_users'].sum() if not prev_df.empty else 0
            overall_cpa_prev = total_cost_prev / total_users_prev if total_users_prev > 0 else 0
            
            cpa_change = overall_cpa_current - overall_cpa_prev
            
            print(f"DEBUG: æ•´ä½“CPA - å½“å‰:{overall_cpa_current:.2f}, å‰æ—¥:{overall_cpa_prev:.2f}, å˜åŒ–:{cpa_change:+.2f}")
            
            # 3. æ¸ é“å±‚é¢åˆ†æ
            channel_analysis = []
            
            # åˆå¹¶å½“å‰å’Œå†å²æ•°æ®
            if not prev_df.empty:
                merged_df = pd.merge(current_df, prev_df, on='ad_channel', suffixes=('_current', '_prev'), how='outer').fillna(0)
            else:
                merged_df = current_df.copy()
                merged_df.columns = [col + '_current' if col != 'ad_channel' else col for col in merged_df.columns]
                merged_df['channel_cpa_prev'] = 0
                merged_df['quality_users_prev'] = 0
            
            # 4. åˆ†æå„æ¸ é“å¯¹CPAå¼‚å¸¸çš„è´¡çŒ®
            for _, row in merged_df.iterrows():
                channel = row['ad_channel']
                cpa_current = row.get('channel_cpa_current', 0)
                cpa_prev = row.get('channel_cpa_prev', 0)
                users_current = row.get('quality_users_current', 0)
                
                if users_current > 0 and cpa_current > 0:
                    # è®¡ç®—æ¸ é“CPAå˜åŒ–
                    cpa_channel_change = cpa_current - cpa_prev if cpa_prev > 0 else 0
                    user_weight = users_current / total_users_current if total_users_current > 0 else 0
                    
                    # åˆ†ææ¸ é“ç‰¹å¾
                    analysis_reasons = []
                    
                    # é«˜CPAæ¸ é“è¯†åˆ«
                    if cpa_current > overall_cpa_current:
                        excess_cpa = cpa_current - overall_cpa_current
                        analysis_reasons.append(f"CPA{cpa_current:.2f}å…ƒï¼Œé«˜äºæ•´ä½“{excess_cpa:.2f}å…ƒ")
                    
                    # CPAå˜åŒ–åˆ†æ
                    if abs(cpa_channel_change) > 1:
                        direction = "ä¸Šæ¶¨" if cpa_channel_change > 0 else "ä¸‹é™"
                        analysis_reasons.append(f"è¾ƒå‰æ—¥{direction}{abs(cpa_channel_change):.2f}å…ƒ")
                    
                    # ç”¨æˆ·å æ¯”å½±å“
                    if user_weight > 0.1:  # ç”¨æˆ·å æ¯”è¶…è¿‡10%
                        analysis_reasons.append(f"ç”¨æˆ·å æ¯”{user_weight*100:.1f}%ï¼Œå½±å“æƒé‡å¤§")
                    
                    # åªä¿ç•™æœ‰æ˜æ˜¾é—®é¢˜çš„æ¸ é“
                    if (cpa_current > overall_cpa_current * 1.2 or  # CPAé«˜äºæ•´ä½“20%ä»¥ä¸Š
                        abs(cpa_channel_change) > 2 or  # CPAå˜åŒ–è¶…è¿‡2å…ƒ
                        (user_weight > 0.2 and cpa_current > overall_cpa_current)):  # å¤§æ¸ é“ä¸”CPAåé«˜
                        
                        channel_analysis.append({
                            'channel': channel,
                            'cpa_current': cpa_current,
                            'cpa_prev': cpa_prev, 
                            'cpa_change': cpa_channel_change,
                            'users_current': int(users_current),
                            'user_weight': user_weight,
                            'analysis_reasons': analysis_reasons,
                            'impact_level': 'é«˜' if user_weight > 0.3 else 'ä¸­' if user_weight > 0.1 else 'ä½'
                        })
            
            # 5. æŒ‰ä¸¥é‡ç¨‹åº¦æ’åº - æƒé‡ Ã— ç»å¯¹å˜åŒ–
            channel_analysis.sort(key=lambda x: (x['user_weight'] * abs(x['cpa_change'])), reverse=True)
            
            # 6. ç”Ÿæˆåˆ†ææ€»ç»“
            summary_parts = []
            if cpa_change > 0:
                summary_parts.append(f"æ•´ä½“CPAä¸Šå‡{cpa_change:.2f}å…ƒ")
            
            high_impact_channels = [c for c in channel_analysis if c['impact_level'] == 'é«˜']
            if high_impact_channels:
                top_channel = high_impact_channels[0]
                summary_parts.append(f"ä¸»è¦ç”±{top_channel['channel']}ç­‰é«˜å½±å“æ¸ é“æ‹‰åŠ¨")
            
            analysis_summary = "ï¼›".join(summary_parts) if summary_parts else "CPAå¼‚å¸¸åŸå› å¾…è¿›ä¸€æ­¥åˆ†æ"
            
            # ä¸ºHTMLæ˜¾ç¤ºå‡†å¤‡æ¸ é“æ•°æ® - æŒ‰ä¸¥é‡ç¨‹åº¦æ’åºï¼ˆå¢åŠ æ•°æ®éªŒè¯ï¼‰
            channel_display_data = []
            for _, row in merged_df.iterrows():
                channel = row['ad_channel']
                cpa_current = row.get('channel_cpa_current', 0)
                users_current = row.get('quality_users_current', 0)
                
                # æ•°æ®è¾¹ç•Œæ£€æŸ¥
                if (cpa_current > 0 and users_current > 0 and 
                    cpa_current < 10000 and users_current < 1000000):  # é˜²æ­¢å¼‚å¸¸å¤§çš„æ•°æ®
                    
                    excess_cpa = max(0, cpa_current - overall_cpa_current)
                    if excess_cpa > 5:  # åªæ˜¾ç¤ºæ˜æ˜¾é«˜äºæ•´ä½“CPAçš„æ¸ é“
                        # è®¡ç®—ä¸¥é‡ç¨‹åº¦ = CPAè¶…å‡ºç¨‹åº¦ * ç”¨æˆ·æƒé‡
                        user_weight = users_current / total_users_current if total_users_current > 0 else 0
                        severity_score = excess_cpa * (1 + user_weight * 10)  # ç”¨æˆ·å¤šçš„æ¸ é“æƒé‡æ›´é«˜
                        
                        # éªŒè¯æ•°æ®åˆç†æ€§
                        if user_weight <= 1.0 and severity_score > 0:  # æƒé‡ä¸åº”è¶…è¿‡100%
                            # è®¡ç®—è´¡çŒ®åº¦ = (è¯¥æ¸ é“è¶…é¢æˆæœ¬ Ã· æ€»æˆæœ¬) Ã— 100%
                            channel_cost = cpa_current * users_current
                            total_cost = overall_cpa_current * total_users_current if total_users_current > 0 else 1
                            contribution_pct = (channel_cost / total_cost) * 100 if total_cost > 0 else 0
                            
                            channel_display_data.append({
                                'channel': channel,
                                'channel_cpa': cpa_current,
                                'excess_cpa': excess_cpa,
                                'quality_users': users_current,
                                'severity_score': severity_score,
                                'contribution_pct': contribution_pct  # æ–°å¢è´¡çŒ®åº¦
                            })
                        else:
                            print(f"âš ï¸  æ¸ é“æ•°æ®å¼‚å¸¸: {channel} - æƒé‡:{user_weight:.3f}, ä¸¥é‡ç¨‹åº¦:{severity_score:.2f}")
                else:
                    if cpa_current >= 10000 or users_current >= 1000000:
                        print(f"âš ï¸  æ¸ é“æ•°æ®è¶…å‡ºåˆç†èŒƒå›´: {channel} - CPA:Â¥{cpa_current:.2f}, ç”¨æˆ·:{users_current:,}")
            
            # æŒ‰ä¸¥é‡ç¨‹åº¦é™åºæ’åºï¼Œåªå–å‰3ä¸ªæœ€ä¸¥é‡çš„
            channel_display_data.sort(key=lambda x: x['severity_score'], reverse=True)
            channel_display_data = channel_display_data[:3]
            
            # æœ€ç»ˆéªŒè¯
            if len(channel_display_data) > 0:
                print(f"âœ… æ¸ é“å¼‚å¸¸åˆ†æå®Œæˆï¼Œè¯†åˆ«{len(channel_display_data)}ä¸ªé«˜å½±å“æ¸ é“")
            else:
                print("â„¹ï¸  æœªå‘ç°æ˜æ˜¾çš„CPAå¼‚å¸¸æ¸ é“")
            
            return {
                'metric': 'CPAå¼‚å¸¸æ¸ é“å®šä½',
                'anomaly_direction': f"CPA{cpa_change:+.2f}å…ƒ",
                'anomaly_value': f"{overall_cpa_current:.2f}å…ƒ",
                'channel_impacts': channel_analysis[:8],  # æ˜¾ç¤ºå‰8ä¸ªæ¸ é“
                'channel_data': channel_display_data,  # æ–°å¢ï¼šç”¨äºHTMLæ˜¾ç¤ºçš„ç®€åŒ–æ•°æ®
                'analysis_type': 'cpa_channel_analysis',
                'overall_cpa_current': overall_cpa_current,
                'overall_cpa_prev': overall_cpa_prev,
                'analysis_summary': analysis_summary
            }
        
        except Exception as e:
            print(f"ERROR: CPAå¼‚å¸¸æ¸ é“åˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'metric': 'CPAå¼‚å¸¸æ¸ é“å®šä½',
                'anomaly_direction': 'åˆ†æå¤±è´¥',
                'anomaly_value': 'N/A',
                'channel_impacts': [],
                'analysis_type': 'cpa_channel_analysis',
                'analysis_summary': f'CPAå¼‚å¸¸åˆ†æå¤±è´¥: {str(e)}'
            }
    
    def _analyze_arpu_by_channel(self, channel_df: pd.DataFrame, prev_channel_df: pd.DataFrame, 
                               anomaly: dict, date_str: str, conn: sqlite3.Connection) -> dict:
        """åˆ†æARPUå¼‚å¸¸çš„æ¸ é“å½±å“ - åŸºäºCPAåˆ†æçš„æ”¹è¿›ç‰ˆ"""
        
        print(f"DEBUG: ARPUå¼‚å¸¸æ¸ é“åˆ†æå¼€å§‹ - {date_str}")
        
        # 1. è·å–ARPUå†å²è¶‹åŠ¿æ•°æ®
        prev_date = (datetime.strptime(date_str, '%Y-%m-%d') - timedelta(days=1)).strftime('%Y-%m-%d')
        
        # å½“å‰æ—¥æœŸARPUæŸ¥è¯¢
        current_arpu_sql = f"""
            SELECT 
                ad_channel,
                SUM(CASE WHEN status='good' AND verification_status='verified' THEN newuser ELSE 0 END) as quality_users,
                SUM(CASE WHEN status='good' AND verification_status='verified' THEN zizhu_revenue_1 ELSE 0 END) as revenue,
                CASE 
                    WHEN SUM(CASE WHEN status='good' AND verification_status='verified' THEN newuser ELSE 0 END) > 0
                    THEN ROUND(SUM(CASE WHEN status='good' AND verification_status='verified' THEN zizhu_revenue_1 ELSE 0 END) * 1.0 / 
                               SUM(CASE WHEN status='good' AND verification_status='verified' THEN newuser ELSE 0 END), 2)
                    ELSE NULL
                END as channel_arpu
            FROM cpz_qs_newuser_channel_i_d 
            WHERE dt = '{date_str}'
            GROUP BY ad_channel
            HAVING quality_users >= 50
            ORDER BY channel_arpu DESC
        """
        
        # å‰ä¸€æ—¥ARPUæŸ¥è¯¢
        prev_arpu_sql = f"""
            SELECT 
                ad_channel,
                SUM(CASE WHEN status='good' AND verification_status='verified' THEN newuser ELSE 0 END) as quality_users,
                SUM(CASE WHEN status='good' AND verification_status='verified' THEN zizhu_revenue_1 ELSE 0 END) as revenue,
                CASE 
                    WHEN SUM(CASE WHEN status='good' AND verification_status='verified' THEN newuser ELSE 0 END) > 0
                    THEN ROUND(SUM(CASE WHEN status='good' AND verification_status='verified' THEN zizhu_revenue_1 ELSE 0 END) * 1.0 / 
                               SUM(CASE WHEN status='good' AND verification_status='verified' THEN newuser ELSE 0 END), 2)
                    ELSE NULL
                END as channel_arpu
            FROM cpz_qs_newuser_channel_i_d 
            WHERE dt = '{prev_date}'
            GROUP BY ad_channel
            HAVING quality_users >= 50
        """
        
        try:
            current_arpu_df = pd.read_sql_query(current_arpu_sql, conn)
            prev_arpu_df = pd.read_sql_query(prev_arpu_sql, conn)
            
            if current_arpu_df.empty:
                print(f"â„¹ï¸  å½“å‰æ—¥æœŸ {date_str} æ— æœ‰æ•ˆARPUæ•°æ®")
                return {}
            
            # 2. åˆå¹¶æ•°æ®åˆ†æå˜åŒ–
            merged_df = pd.merge(
                current_arpu_df, prev_arpu_df, 
                on='ad_channel', 
                how='left', 
                suffixes=('_current', '_prev')
            )
            
            merged_df['arpu_change'] = merged_df['channel_arpu_current'] - merged_df['channel_arpu_prev'].fillna(merged_df['channel_arpu_current'])
            merged_df['arpu_change_pct'] = ((merged_df['channel_arpu_current'] - merged_df['channel_arpu_prev']) / 
                                          merged_df['channel_arpu_prev'].replace(0, np.nan) * 100).fillna(0)
            
            # 3. è®¡ç®—æ•´ä½“ARPUå˜åŒ–
            total_current_arpu = (current_arpu_df['revenue'].sum() / 
                                current_arpu_df['quality_users'].sum() if current_arpu_df['quality_users'].sum() > 0 else 0)
            total_prev_arpu = (prev_arpu_df['revenue'].sum() / 
                             prev_arpu_df['quality_users'].sum() if not prev_arpu_df.empty and prev_arpu_df['quality_users'].sum() > 0 else total_current_arpu)
            
            overall_arpu_change = total_current_arpu - total_prev_arpu
            print(f"DEBUG: æ•´ä½“ARPU - å½“å‰:{total_current_arpu:.2f}, å‰æ—¥:{total_prev_arpu:.2f}, å˜åŒ–:{overall_arpu_change:+.2f}")
            
            # 4. è¯†åˆ«å¼‚å¸¸æ¸ é“
            channel_impacts = []
            for idx, row in merged_df.iterrows():
                if pd.isna(row['channel_arpu_current']) or row['channel_arpu_current'] is None:
                    continue
                    
                arpu_current = float(row['channel_arpu_current'])
                arpu_prev = float(row['channel_arpu_prev']) if pd.notna(row['channel_arpu_prev']) else arpu_current
                arpu_change = float(row['arpu_change'])
                arpu_change_pct = float(row['arpu_change_pct'])
                quality_users_current = int(row['quality_users_current'])
                
                # æƒé‡è®¡ç®—ï¼šç”¨æˆ·æ•°æƒé‡
                total_users = current_arpu_df['quality_users'].sum()
                weight = quality_users_current / total_users if total_users > 0 else 0
                
                # åŠ æƒå½±å“ï¼šæ¸ é“ARPUå˜åŒ–å¯¹æ•´ä½“ARPUçš„å½±å“
                weighted_impact = arpu_change * weight
                
                # è´¡çŒ®åº¦è®¡ç®—
                contribution_pct = weight * 100
                
                # å¼‚å¸¸åˆ¤æ–­ï¼šARPUå˜åŒ–å¼‚å¸¸çš„æ¸ é“
                is_abnormal = False
                channel_contribution_reasons = []
                
                # åˆ¤æ–­æ¡ä»¶
                if abs(arpu_change_pct) > 10:  # ARPUå˜åŒ–è¶…è¿‡10%
                    is_abnormal = True
                    direction = "ä¸‹é™" if arpu_change < 0 else "ä¸Šå‡"
                    channel_contribution_reasons.append(f"ARPU{direction}{abs(arpu_change_pct):.1f}%")
                
                if abs(arpu_change) > 1.0:  # ARPUç»å¯¹å˜åŒ–è¶…è¿‡1å…ƒ
                    is_abnormal = True
                    direction = "é™ä½" if arpu_change < 0 else "æé«˜"
                    channel_contribution_reasons.append(f"ARPU{direction}{abs(arpu_change):.2f}å…ƒ")
                
                if is_abnormal or abs(weighted_impact) >= 0.05:  # åŠ æƒå½±å“è¾ƒå¤§
                    channel_impacts.append({
                        'channel': row['ad_channel'],
                        'arpu_current': arpu_current,
                        'arpu_prev': arpu_prev,
                        'arpu_change': arpu_change,
                        'arpu_change_pct': arpu_change_pct,
                        'quality_users': quality_users_current,
                        'weight': weight,
                        'weighted_impact': weighted_impact,
                        'contribution_reasons': channel_contribution_reasons
                    })
            
            # è®¡ç®—ä¸¥é‡ç¨‹åº¦åˆ†æ•°å¹¶æ’åº
            for impact in channel_impacts:
                # æ”¹è¿›çš„ä¸¥é‡ç¨‹åº¦ç®—æ³•ï¼šè€ƒè™‘å¯¹æ•´ä½“çš„å®é™…å½±å“
                # ä¸¥é‡ç¨‹åº¦ = |åŠ æƒå½±å“| * 100 + |ARPUå˜åŒ–ç‡| * ç”¨æˆ·æƒé‡ * 10
                weighted_impact_score = abs(impact['weighted_impact']) * 100
                change_rate_score = abs(impact['arpu_change_pct']) * impact['weight'] * 10
                severity_score = weighted_impact_score + change_rate_score
                impact['severity_score'] = severity_score
                
                # è®¡ç®—å¯¹æ•´ä½“ARPUå˜åŒ–çš„è´¡çŒ®åº¦
                if overall_arpu_change != 0:
                    impact['contribution_to_overall'] = (impact['weighted_impact'] / abs(overall_arpu_change)) * 100
                else:
                    impact['contribution_to_overall'] = 0
            
            # æŒ‰å¯¹æ•´ä½“ARPUçš„å®é™…è´¡çŒ®åº¦æ’åºï¼ˆä¼˜å…ˆæ˜¾ç¤ºè´¡çŒ®å¤§çš„æ¸ é“ï¼‰
            channel_impacts.sort(key=lambda x: abs(x.get('contribution_to_overall', 0)), reverse=True)
            
            # åªä¿ç•™å‰3ä¸ªæœ€ä¸¥é‡çš„æ¸ é“ï¼Œç”¨äºæ˜¾ç¤º
            significant_impacts = channel_impacts[:3]
            
            # è®¡ç®—å‰3ä¸ªæ¸ é“çš„æ€»è´¡çŒ®åº¦
            total_contribution = sum([impact.get('contribution_to_overall', 0) for impact in significant_impacts])
            
            print(f"âœ… ARPUæ¸ é“å¼‚å¸¸åˆ†æå®Œæˆï¼Œè¯†åˆ«{len(significant_impacts)}ä¸ªé«˜å½±å“æ¸ é“ï¼Œæ€»è´¡çŒ®åº¦{total_contribution:.1f}%")
            
            # ç§»é™¤å•ç‹¬å¼ºè°ƒæŸä¸ªæ¸ é“çš„æ¶ˆæ¯
            main_contributor_msg = ""
            
            return {
                'metric': 'ARPUå¼‚å¸¸',
                'anomaly_direction': 'æ¸ é“åˆ†æ',
                'anomaly_value': f'Â¥{total_current_arpu:.2f}',
                'channel_impacts': significant_impacts,
                'analysis_type': 'arpu_channel_analysis',
                'analysis_summary': f'è¯†åˆ«{len(significant_impacts)}ä¸ªARPUå¼‚å¸¸æ¸ é“',
                'total_contribution': total_contribution,
                'main_contributor_msg': main_contributor_msg,
                'overall_arpu_change': overall_arpu_change
            }
            
        except Exception as e:
            print(f"ERROR: ARPUå¼‚å¸¸æ¸ é“åˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'metric': 'ARPUå¼‚å¸¸æ¸ é“å®šä½',  
                'anomaly_direction': 'åˆ†æå¤±è´¥',
                'anomaly_value': 'N/A',
                'channel_impacts': [],
                'analysis_type': 'arpu_channel_analysis',
                'analysis_summary': f'ARPUå¼‚å¸¸åˆ†æå¤±è´¥: {str(e)}'
            }
    
    def _analyze_generic_metric_by_channel(self, metric_field: str, metric_name: str, date_str: str, conn: sqlite3.Connection, anomaly: dict) -> dict:
        """é€šç”¨æŒ‡æ ‡æ¸ é“åˆ†æå‡½æ•°"""
        
        print(f"DEBUG: {metric_name}æ¸ é“åˆ†æå¼€å§‹ - {date_str}")
        
        prev_date = (datetime.strptime(date_str, '%Y-%m-%d') - timedelta(days=1)).strftime('%Y-%m-%d')
        
        # æ ¹æ®ä¸åŒæŒ‡æ ‡æ„å»ºæŸ¥è¯¢SQL
        if metric_field == 'female_ratio':
            sql_template = """
                SELECT 
                    ad_channel,
                    SUM(CASE WHEN status='good' AND verification_status='verified' THEN newuser ELSE 0 END) as quality_users,
                    SUM(CASE WHEN status='good' AND verification_status='verified' AND gender='female' THEN newuser ELSE 0 END) as metric_numerator,
                    CASE 
                        WHEN SUM(CASE WHEN status='good' AND verification_status='verified' THEN newuser ELSE 0 END) > 0
                        THEN ROUND(SUM(CASE WHEN status='good' AND verification_status='verified' AND gender='female' THEN newuser ELSE 0 END) * 100.0 / SUM(CASE WHEN status='good' AND verification_status='verified' THEN newuser ELSE 0 END), 2)
                        ELSE 0
                    END as metric_value
                FROM cpz_qs_newuser_channel_i_d 
                WHERE dt = '{date}'
                GROUP BY ad_channel
                HAVING quality_users >= 50
                ORDER BY metric_value DESC
            """
        elif metric_field == 'young_ratio':
            sql_template = """
                SELECT 
                    ad_channel,
                    SUM(CASE WHEN status='good' AND verification_status='verified' THEN newuser ELSE 0 END) as quality_users,
                    SUM(CASE WHEN status='good' AND verification_status='verified' AND age_group IN ('20-', '20~23') THEN newuser ELSE 0 END) as metric_numerator,
                    CASE 
                        WHEN SUM(CASE WHEN status='good' AND verification_status='verified' THEN newuser ELSE 0 END) > 0
                        THEN ROUND(SUM(CASE WHEN status='good' AND verification_status='verified' AND age_group IN ('20-', '20~23') THEN newuser ELSE 0 END) * 100.0 / SUM(CASE WHEN status='good' AND verification_status='verified' THEN newuser ELSE 0 END), 2)
                        ELSE 0
                    END as metric_value
                FROM cpz_qs_newuser_channel_i_d 
                WHERE dt = '{date}'
                GROUP BY ad_channel
                HAVING quality_users >= 50
                ORDER BY metric_value DESC
            """
        elif metric_field == 'high_tier_ratio':
            sql_template = """
                SELECT 
                    ad_channel,
                    SUM(CASE WHEN status='good' AND verification_status='verified' THEN newuser ELSE 0 END) as quality_users,
                    SUM(CASE WHEN status='good' AND verification_status='verified' AND dengji IN ('è¶…ä¸€çº¿', 'ä¸€çº¿', 'äºŒçº¿') THEN newuser ELSE 0 END) as metric_numerator,
                    CASE 
                        WHEN SUM(CASE WHEN status='good' AND verification_status='verified' THEN newuser ELSE 0 END) > 0
                        THEN ROUND(SUM(CASE WHEN status='good' AND verification_status='verified' AND dengji IN ('è¶…ä¸€çº¿', 'ä¸€çº¿', 'äºŒçº¿') THEN newuser ELSE 0 END) * 100.0 / SUM(CASE WHEN status='good' AND verification_status='verified' THEN newuser ELSE 0 END), 2)
                        ELSE 0
                    END as metric_value
                FROM cpz_qs_newuser_channel_i_d 
                WHERE dt = '{date}'
                GROUP BY ad_channel
                HAVING quality_users >= 50
                ORDER BY metric_value DESC
            """
        elif metric_field == 'good_rate':
            sql_template = """
                SELECT 
                    ad_channel,
                    SUM(newuser) as total_users,
                    SUM(CASE WHEN status='good' THEN newuser ELSE 0 END) as metric_numerator,
                    CASE 
                        WHEN SUM(newuser) > 0
                        THEN ROUND(SUM(CASE WHEN status='good' THEN newuser ELSE 0 END) * 100.0 / SUM(newuser), 2)
                        ELSE 0
                    END as metric_value
                FROM cpz_qs_newuser_channel_i_d 
                WHERE dt = '{date}'
                GROUP BY ad_channel
                HAVING total_users >= 50
                ORDER BY metric_value DESC
            """
        elif metric_field == 'verified_rate':
            sql_template = """
                SELECT 
                    ad_channel,
                    SUM(CASE WHEN status='good' THEN newuser ELSE 0 END) as good_users,
                    SUM(CASE WHEN status='good' AND verification_status='verified' THEN newuser ELSE 0 END) as metric_numerator,
                    CASE 
                        WHEN SUM(CASE WHEN status='good' THEN newuser ELSE 0 END) > 0
                        THEN ROUND(SUM(CASE WHEN status='good' AND verification_status='verified' THEN newuser ELSE 0 END) * 100.0 / SUM(CASE WHEN status='good' THEN newuser ELSE 0 END), 2)
                        ELSE 0
                    END as metric_value
                FROM cpz_qs_newuser_channel_i_d 
                WHERE dt = '{date}'
                GROUP BY ad_channel
                HAVING good_users >= 50
                ORDER BY metric_value DESC
            """
        elif metric_field == 'retention_rate':
            sql_template = """
                SELECT 
                    ad_channel,
                    SUM(CASE WHEN status='good' AND verification_status='verified' THEN newuser ELSE 0 END) as quality_users,
                    SUM(CASE WHEN status='good' AND verification_status='verified' THEN is_returned_1_day ELSE 0 END) as metric_numerator,
                    CASE 
                        WHEN SUM(CASE WHEN status='good' AND verification_status='verified' THEN newuser ELSE 0 END) > 0
                        THEN ROUND(SUM(CASE WHEN status='good' AND verification_status='verified' THEN is_returned_1_day ELSE 0 END) * 100.0 / SUM(CASE WHEN status='good' AND verification_status='verified' THEN newuser ELSE 0 END), 2)
                        ELSE 0
                    END as metric_value
                FROM cpz_qs_newuser_channel_i_d 
                WHERE dt = '{date}'
                GROUP BY ad_channel
                HAVING quality_users >= 50
                ORDER BY metric_value DESC
            """
        else:
            return {}
        
        try:
            current_sql = sql_template.format(date=date_str)
            prev_sql = sql_template.format(date=prev_date)
            
            current_df = pd.read_sql_query(current_sql, conn)
            prev_df = pd.read_sql_query(prev_sql, conn)
            
            if current_df.empty:
                print(f"â„¹ï¸  å½“å‰æ—¥æœŸ {date_str} æ— æœ‰æ•ˆ{metric_name}æ•°æ®")
                return {}
            
            # åˆå¹¶æ•°æ®åˆ†æå˜åŒ–
            merged_df = pd.merge(
                current_df, prev_df, 
                on='ad_channel', 
                how='left', 
                suffixes=('_current', '_prev')
            )
            
            merged_df['metric_change'] = merged_df['metric_value_current'] - merged_df['metric_value_prev'].fillna(merged_df['metric_value_current'])
            merged_df['metric_change_pct'] = ((merged_df['metric_value_current'] - merged_df['metric_value_prev']) / 
                                             merged_df['metric_value_prev'].replace(0, np.nan) * 100).fillna(0)
            
            # è¯†åˆ«å¼‚å¸¸æ¸ é“
            channel_impacts = []
            for idx, row in merged_df.iterrows():
                if pd.isna(row['metric_value_current']) or row['metric_value_current'] is None:
                    continue
                    
                metric_current = float(row['metric_value_current'])
                metric_prev = float(row['metric_value_prev']) if pd.notna(row['metric_value_prev']) else metric_current
                metric_change = float(row['metric_change'])
                metric_change_pct = float(row['metric_change_pct'])
                
                # è·å–ç”¨æˆ·æ•°
                if 'quality_users_current' in row:
                    users = int(row['quality_users_current'])
                    total_users = current_df['quality_users'].sum()
                elif 'good_users_current' in row:
                    users = int(row['good_users_current'])
                    total_users = current_df['good_users'].sum()
                elif 'total_users_current' in row:
                    users = int(row['total_users_current'])
                    total_users = current_df['total_users'].sum()
                else:
                    users = 0
                    total_users = 1
                
                # è®¡ç®—æƒé‡
                weight = users / total_users if total_users > 0 else 0
                
                # å¼‚å¸¸åˆ¤æ–­
                is_abnormal = False
                channel_contribution_reasons = []
                
                if abs(metric_change_pct) > 15:  # æŒ‡æ ‡å˜åŒ–è¶…è¿‡15%
                    is_abnormal = True
                    direction = "ä¸‹é™" if metric_change < 0 else "ä¸Šå‡"
                    channel_contribution_reasons.append(f"{metric_name}{direction}{abs(metric_change_pct):.1f}%")
                
                if abs(metric_change) > 5.0:  # æŒ‡æ ‡ç»å¯¹å˜åŒ–è¶…è¿‡5ä¸ªç™¾åˆ†ç‚¹
                    is_abnormal = True
                    direction = "é™ä½" if metric_change < 0 else "æé«˜"
                    channel_contribution_reasons.append(f"{metric_name}{direction}{abs(metric_change):.1f}pp")
                
                if is_abnormal:
                    channel_impacts.append({
                        'channel': row['ad_channel'],
                        'metric_current': metric_current,
                        'metric_prev': metric_prev,
                        'metric_change': metric_change,
                        'metric_change_pct': metric_change_pct,
                        'users': users,
                        'weight': weight,
                        'contribution_reasons': channel_contribution_reasons
                    })
            
            # è®¡ç®—ä¸¥é‡ç¨‹åº¦åˆ†æ•°å¹¶æ’åº
            for impact in channel_impacts:
                # è®¡ç®—ä¸¥é‡ç¨‹åº¦ = ç»å¯¹å˜åŒ–é‡ * (1 + ç”¨æˆ·æƒé‡ * 10)
                severity_score = abs(impact['metric_change']) * (1 + impact['weight'] * 10)
                impact['severity_score'] = severity_score
            
            # æŒ‰ä¸¥é‡ç¨‹åº¦é™åºæ’åº
            channel_impacts.sort(key=lambda x: x['severity_score'], reverse=True)
            significant_impacts = channel_impacts[:3]  # åªä¿ç•™å‰3ä¸ªæœ€ä¸¥é‡çš„
            
            print(f"âœ… {metric_name}æ¸ é“å¼‚å¸¸åˆ†æå®Œæˆï¼Œè¯†åˆ«{len(significant_impacts)}ä¸ªå¼‚å¸¸æ¸ é“")
            
            return {
                'metric': f'{metric_name}å¼‚å¸¸',
                'anomaly_direction': 'æ¸ é“åˆ†æ',
                'channel_impacts': significant_impacts,
                'analysis_type': 'generic_channel_analysis',
                'analysis_summary': f'è¯†åˆ«{len(significant_impacts)}ä¸ª{metric_name}å¼‚å¸¸æ¸ é“'
            }
            
        except Exception as e:
            print(f"ERROR: {metric_name}æ¸ é“åˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'metric': f'{metric_name}å¼‚å¸¸æ¸ é“å®šä½',
                'anomaly_direction': 'åˆ†æå¤±è´¥',
                'channel_impacts': [],
                'analysis_type': 'generic_channel_analysis',
                'analysis_summary': f'{metric_name}å¼‚å¸¸åˆ†æå¤±è´¥: {str(e)}'
            }
    
    def _analyze_quality_users_by_channel(self, channel_df: pd.DataFrame, prev_channel_df: pd.DataFrame, 
                                        anomaly: dict, date_str: str, conn: sqlite3.Connection) -> dict:
        """åˆ†æä¼˜è´¨ç”¨æˆ·æ•°å¼‚å¸¸çš„æ¸ é“å½±å“"""
        
        # åˆå¹¶å½“å‰å’Œå†å²æ¸ é“æ•°æ®
        if not prev_channel_df.empty:
            merged_df = pd.merge(
                channel_df, prev_channel_df, 
                on='ad_channel', suffixes=('_current', '_prev'), how='outer'
            ).fillna(0)
        else:
            merged_df = channel_df.copy()
            merged_df.columns = [col + '_current' if col != 'ad_channel' else col for col in merged_df.columns]
        
        # è®¡ç®—ç”¨æˆ·æ•°å˜åŒ–
        channel_impacts = []
        total_users = merged_df['quality_users_current'].sum() if 'quality_users_current' in merged_df.columns else 1
        
        for _, row in merged_df.iterrows():
            if 'quality_users_prev' in merged_df.columns:
                users_current = row.get('quality_users_current', 0)
                users_prev = row.get('quality_users_prev', 0)
                
                users_change = users_current - users_prev
                users_change_pct = (users_change / users_prev) * 100 if users_prev > 0 else 0
                
                # è®¡ç®—æƒé‡å’Œä¸¥é‡ç¨‹åº¦
                weight = users_current / total_users if total_users > 0 else 0
                severity_score = abs(users_change) * (1 + weight * 10)
                
                # åªä¿ç•™ç”¨æˆ·æ•°ä¸‹é™çš„æ¸ é“ï¼ˆè´Ÿå‘å½±å“ï¼‰
                if users_change < 0 and abs(users_change) >= 10:
                    channel_impacts.append({
                        'channel': row['ad_channel'],
                        'users_current': users_current,
                        'users_prev': users_prev,
                        'users_change': users_change,
                        'users_change_pct': users_change_pct,
                        'weight': weight,
                        'severity_score': severity_score
                    })
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åº
        channel_impacts.sort(key=lambda x: x['severity_score'], reverse=True)
        
        return {
            'metric': 'Goodä¸”è®¤è¯ç”¨æˆ·æ•°',
            'anomaly_direction': anomaly.get('direction', ''),
            'anomaly_value': anomaly.get('current_value', ''),
            'channel_impacts': channel_impacts[:10],
            'analysis_type': 'users_change'
        }
    
    def _analyze_retention_by_channel(self, channel_df: pd.DataFrame, prev_channel_df: pd.DataFrame, 
                                    anomaly: dict, date_str: str, conn: sqlite3.Connection) -> dict:
        """åˆ†ææ¬¡ç•™ç‡å¼‚å¸¸çš„æ¸ é“å½±å“"""
        
        # è®¡ç®—æ•´ä½“æ¬¡ç•™ç‡ä½œä¸ºåŸºå‡†
        total_quality_users = channel_df['quality_users'].sum()
        total_retained = channel_df['retained_users'].sum()
        overall_retention = (total_retained / total_quality_users * 100) if total_quality_users > 0 else 0
        
        # è®¡ç®—å„æ¸ é“çš„æ¬¡ç•™ç‡å’Œæƒé‡å½±å“
        channel_retention_analysis = []
        
        for _, row in channel_df.iterrows():
            quality_users = row.get('quality_users', 0)
            retained_users = row.get('retained_users', 0)
            
            if quality_users >= 30:  # æ ·æœ¬é‡è¦æ±‚
                retention_rate = (retained_users / quality_users * 100) if quality_users > 0 else 0
                
                # åªä¿ç•™æ¬¡ç•™ç‡ä½äºæ•´ä½“æ°´å¹³çš„æ¸ é“ï¼ˆè´Ÿå‘å½±å“ï¼‰
                if retention_rate < overall_retention:
                    # è®¡ç®—æƒé‡å½±å“ï¼šè¯¥æ¸ é“å¯¹æ•´ä½“æ¬¡ç•™ç‡ä¸‹é™çš„è´¡çŒ®
                    weight = quality_users / total_quality_users if total_quality_users > 0 else 0
                    retention_impact = (retention_rate - overall_retention) * weight
                    severity_score = abs(retention_impact) * (1 + weight * 10)
                    
                    channel_retention_analysis.append({
                        'channel': row['ad_channel'],
                        'quality_users': quality_users,
                        'retained_users': retained_users,
                        'retention_rate': retention_rate,
                        'vs_overall': retention_rate - overall_retention,
                        'weight': weight,
                        'weighted_impact': retention_impact,
                        'severity_score': severity_score
                    })
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦é™åºæ’åº
        channel_retention_analysis.sort(key=lambda x: x['severity_score'], reverse=True)
        
        return {
            'metric': 'æ¬¡ç•™ç‡',
            'anomaly_direction': anomaly.get('direction', ''),
            'anomaly_value': anomaly.get('current_value', ''),
            'channel_impacts': channel_retention_analysis[:10],
            'analysis_type': 'retention_analysis'
        }
    
    def _analyze_female_ratio_by_channel(self, channel_df: pd.DataFrame, prev_channel_df: pd.DataFrame, 
                                       anomaly: dict, date_str: str, conn: sqlite3.Connection) -> dict:
        """åˆ†æå¥³æ€§å æ¯”å¼‚å¸¸çš„æ¸ é“å½±å“"""
        
        # è·å–æ•æ„Ÿå¼‚å¸¸è¯¦ç»†ä¿¡æ¯
        sensitive_details = anomaly.get('sensitive_details', {})
        
        # è®¡ç®—æ•´ä½“å¥³æ€§å æ¯”ä½œä¸ºåŸºå‡†
        total_quality_users = channel_df['quality_users'].sum()
        total_female = channel_df['female_users'].sum()
        overall_female_ratio = (total_female / total_quality_users * 100) if total_quality_users > 0 else 0
        
        # è®¡ç®—å„æ¸ é“çš„å¥³æ€§å æ¯”å’Œæƒé‡å½±å“
        channel_female_analysis = []
        
        for _, row in channel_df.iterrows():
            quality_users = row.get('quality_users', 0)
            female_users = row.get('female_users', 0)
            
            if quality_users >= 30:  # æ ·æœ¬é‡è¦æ±‚
                female_ratio = (female_users / quality_users * 100) if quality_users > 0 else 0
                
                # åªä¿ç•™å¥³æ€§å æ¯”ä½äºæ•´ä½“æ°´å¹³çš„æ¸ é“ï¼ˆè´Ÿå‘å½±å“ï¼‰
                if female_ratio < overall_female_ratio:
                    # è®¡ç®—æƒé‡å½±å“ï¼šè¯¥æ¸ é“å¯¹æ•´ä½“å¥³æ€§å æ¯”ä¸‹é™çš„è´¡çŒ®
                    weight = quality_users / total_quality_users if total_quality_users > 0 else 0
                    female_impact = (female_ratio - overall_female_ratio) * weight
                    
                    # ç”Ÿæˆæ¸ é“è´¡çŒ®åŸå› 
                    contribution_reasons = []
                    
                    # å¦‚æœæœ‰æ•æ„Ÿå¼‚å¸¸è¯¦ç»†ä¿¡æ¯ï¼Œæ˜ å°„åˆ°å…·ä½“æ¸ é“è¡Œä¸º
                    if sensitive_details.get('has_extreme_value'):
                        recent_min = sensitive_details.get('recent_min', 0)
                        if female_ratio <= recent_min * 1.1:  # æ¥è¿‘æˆ–ä½äºå†å²æœ€ä½å€¼
                            contribution_reasons.append(f"å¥³æ€§å æ¯”{female_ratio:.1f}%ï¼Œæ¥è¿‘å†å²æœ€ä½å€¼")
                    
                    if sensitive_details.get('has_mean_deviation'):
                        recent_mean = sensitive_details.get('recent_mean', 0)
                        if female_ratio < recent_mean:
                            contribution_reasons.append(f"å¥³æ€§å æ¯”æ˜¾è‘—ä½äºè¿‘æœŸå‡å€¼")
                    
                    # è®¡ç®—ä¸æ•´ä½“çš„å·®å¼‚ç¨‹åº¦
                    deviation_pct = abs(female_ratio - overall_female_ratio)
                    if deviation_pct > 10:
                        contribution_reasons.append(f"ä½äºæ•´ä½“{deviation_pct:.1f}ä¸ªç™¾åˆ†ç‚¹ï¼Œæ˜¾è‘—æ‹‰ä½æ•´ä½“æŒ‡æ ‡")
                    elif deviation_pct > 5:
                        contribution_reasons.append(f"ä½äºæ•´ä½“{deviation_pct:.1f}ä¸ªç™¾åˆ†ç‚¹")
                    
                    if not contribution_reasons:
                        contribution_reasons.append("å¥³æ€§å æ¯”åä½ï¼Œå½±å“æ•´ä½“ç”¨æˆ·è´¨é‡")
                    
                    # è®¡ç®—ä¸¥é‡ç¨‹åº¦åˆ†æ•°
                    severity_score = abs(female_impact) * (1 + weight * 10)
                    
                    channel_female_analysis.append({
                        'channel': row['ad_channel'],
                        'quality_users': quality_users,
                        'female_users': female_users,
                        'female_ratio': female_ratio,
                        'vs_overall': female_ratio - overall_female_ratio,
                        'weight': weight,
                        'weighted_impact': female_impact,
                        'severity_score': severity_score,
                        'contribution_reasons': contribution_reasons
                    })
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†æ•°é™åºæ’åº
        channel_female_analysis.sort(key=lambda x: x['severity_score'], reverse=True)
        
        # åªä¿ç•™å½±å“è¾ƒå¤§çš„æ¸ é“ï¼ˆæƒé‡å½±å“ç»å¯¹å€¼ > 0.1%ï¼‰
        significant_impacts = []
        for impact in channel_female_analysis:
            if abs(impact['weighted_impact']) >= 0.1:  # å½±å“é˜ˆå€¼
                # æ ¹æ®å½±å“ç¨‹åº¦æ·»åŠ æ ‡ç­¾
                abs_impact = abs(impact['weighted_impact'])
                if abs_impact >= 1.0:
                    impact['impact_level'] = 'é«˜'
                    impact['contribution_reasons'].insert(0, f"ã€é«˜å½±å“ã€‘æƒé‡å½±å“{impact['weighted_impact']:.2f}%")
                elif abs_impact >= 0.5:
                    impact['impact_level'] = 'ä¸­'
                    impact['contribution_reasons'].insert(0, f"ã€ä¸­å½±å“ã€‘æƒé‡å½±å“{impact['weighted_impact']:.2f}%")
                else:
                    impact['impact_level'] = 'ä½'
                    impact['contribution_reasons'].insert(0, f"ã€ä½å½±å“ã€‘æƒé‡å½±å“{impact['weighted_impact']:.2f}%")
                    
                significant_impacts.append(impact)
        
        # å¦‚æœæ²¡æœ‰æ˜¾è‘—å½±å“çš„æ¸ é“ï¼Œè‡³å°‘ä¿ç•™å‰3ä¸ª
        if not significant_impacts and channel_female_analysis:
            significant_impacts = channel_female_analysis[:3]
            for impact in significant_impacts:
                impact['impact_level'] = 'å¾®'
                impact['contribution_reasons'].insert(0, f"ã€å¾®å½±å“ã€‘æƒé‡å½±å“{impact['weighted_impact']:.2f}%")
        
        return {
            'metric': 'å¥³æ€§å æ¯”',
            'anomaly_direction': anomaly.get('direction', ''),
            'anomaly_value': anomaly.get('current_value', ''),
            'sensitive_anomaly_summary': self._format_sensitive_anomaly_summary(sensitive_details),
            'channel_impacts': significant_impacts[:8],  # å‡å°‘æ˜¾ç¤ºæ•°é‡ï¼Œèšç„¦é«˜å½±å“æ¸ é“
            'analysis_type': 'female_ratio_analysis'
        }
    
    def _analyze_young_ratio_by_channel(self, channel_df: pd.DataFrame, prev_channel_df: pd.DataFrame, 
                                      anomaly: dict, date_str: str, conn: sqlite3.Connection) -> dict:
        """åˆ†æå¹´è½»å æ¯”å¼‚å¸¸çš„æ¸ é“å½±å“"""
        
        # è·å–æ•æ„Ÿå¼‚å¸¸è¯¦ç»†ä¿¡æ¯
        sensitive_details = anomaly.get('sensitive_details', {})
        
        # è®¡ç®—æ•´ä½“å¹´è½»å æ¯”ä½œä¸ºåŸºå‡†
        total_quality_users = channel_df['quality_users'].sum()
        total_young = channel_df['young_users'].sum()
        overall_young_ratio = (total_young / total_quality_users * 100) if total_quality_users > 0 else 0
        
        # è®¡ç®—å„æ¸ é“çš„å¹´è½»å æ¯”å’Œæƒé‡å½±å“
        channel_young_analysis = []
        
        for _, row in channel_df.iterrows():
            quality_users = row.get('quality_users', 0)
            young_users = row.get('young_users', 0)
            
            if quality_users >= 30:  # æ ·æœ¬é‡è¦æ±‚
                young_ratio = (young_users / quality_users * 100) if quality_users > 0 else 0
                
                # åªä¿ç•™å¹´è½»å æ¯”ä½äºæ•´ä½“æ°´å¹³çš„æ¸ é“ï¼ˆè´Ÿå‘å½±å“ï¼‰
                if young_ratio < overall_young_ratio:
                    # è®¡ç®—æƒé‡å½±å“ï¼šè¯¥æ¸ é“å¯¹æ•´ä½“å¹´è½»å æ¯”ä¸‹é™çš„è´¡çŒ®
                    weight = quality_users / total_quality_users if total_quality_users > 0 else 0
                    young_impact = (young_ratio - overall_young_ratio) * weight
                    
                    # ç”Ÿæˆæ¸ é“è´¡çŒ®åŸå› 
                    contribution_reasons = []
                    deviation_pct = abs(young_ratio - overall_young_ratio)
                    if deviation_pct > 8:
                        contribution_reasons.append(f"å¹´è½»å æ¯”{young_ratio:.1f}%ï¼Œä½äºæ•´ä½“{deviation_pct:.1f}ä¸ªç™¾åˆ†ç‚¹")
                    elif deviation_pct > 3:
                        contribution_reasons.append(f"ä½äºæ•´ä½“{deviation_pct:.1f}ä¸ªç™¾åˆ†ç‚¹")
                    
                    if sensitive_details.get('has_extreme_value'):
                        contribution_reasons.append("å¹´è½»ç”¨æˆ·è·å–èƒ½åŠ›åå¼±")
                    
                    if not contribution_reasons:
                        contribution_reasons.append("å¹´è½»å æ¯”åä½ï¼Œå½±å“ç”¨æˆ·è´¨é‡ç»“æ„")
                    
                    # è®¡ç®—ä¸¥é‡ç¨‹åº¦åˆ†æ•°
                    severity_score = abs(young_impact) * (1 + weight * 10)
                    
                    channel_young_analysis.append({
                        'channel': row['ad_channel'],
                        'quality_users': quality_users,
                        'young_users': young_users,
                        'young_ratio': young_ratio,
                        'vs_overall': young_ratio - overall_young_ratio,
                        'weight': weight,
                        'weighted_impact': young_impact,
                        'severity_score': severity_score,
                        'contribution_reasons': contribution_reasons
                    })
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†æ•°é™åºæ’åº
        channel_young_analysis.sort(key=lambda x: x['severity_score'], reverse=True)
        
        return {
            'metric': 'å¹´è½»å æ¯”',
            'anomaly_direction': anomaly.get('direction', ''),
            'anomaly_value': anomaly.get('current_value', ''),
            'sensitive_anomaly_summary': self._format_sensitive_anomaly_summary(sensitive_details),
            'channel_impacts': channel_young_analysis[:10],
            'analysis_type': 'young_ratio_analysis'
        }
    
    def _analyze_high_tier_ratio_by_channel(self, channel_df: pd.DataFrame, prev_channel_df: pd.DataFrame, 
                                          anomaly: dict, date_str: str, conn: sqlite3.Connection) -> dict:
        """åˆ†æé«˜çº¿åŸå¸‚å æ¯”å¼‚å¸¸çš„æ¸ é“å½±å“"""
        
        # è®¡ç®—æ•´ä½“é«˜çº¿åŸå¸‚å æ¯”ä½œä¸ºåŸºå‡†
        total_quality_users = channel_df['quality_users'].sum()
        total_high_tier = channel_df['high_tier_users'].sum()
        overall_high_tier_ratio = (total_high_tier / total_quality_users * 100) if total_quality_users > 0 else 0
        
        # è®¡ç®—å„æ¸ é“çš„é«˜çº¿åŸå¸‚å æ¯”å’Œæƒé‡å½±å“
        channel_high_tier_analysis = []
        
        for _, row in channel_df.iterrows():
            quality_users = row.get('quality_users', 0)
            high_tier_users = row.get('high_tier_users', 0)
            
            if quality_users >= 30:  # æ ·æœ¬é‡è¦æ±‚
                high_tier_ratio = (high_tier_users / quality_users * 100) if quality_users > 0 else 0
                
                # åªä¿ç•™é«˜çº¿åŸå¸‚å æ¯”ä½äºæ•´ä½“æ°´å¹³çš„æ¸ é“ï¼ˆè´Ÿå‘å½±å“ï¼‰
                if high_tier_ratio < overall_high_tier_ratio:
                    # è®¡ç®—æƒé‡å½±å“ï¼šè¯¥æ¸ é“å¯¹æ•´ä½“é«˜çº¿åŸå¸‚å æ¯”ä¸‹é™çš„è´¡çŒ®
                    weight = quality_users / total_quality_users if total_quality_users > 0 else 0
                    high_tier_impact = (high_tier_ratio - overall_high_tier_ratio) * weight
                    
                    # è®¡ç®—ä¸¥é‡ç¨‹åº¦åˆ†æ•°
                    severity_score = abs(high_tier_impact) * (1 + weight * 10)
                    
                    channel_high_tier_analysis.append({
                        'channel': row['ad_channel'],
                        'quality_users': quality_users,
                        'high_tier_users': high_tier_users,
                        'high_tier_ratio': high_tier_ratio,
                        'vs_overall': high_tier_ratio - overall_high_tier_ratio,
                        'weight': weight,
                        'weighted_impact': high_tier_impact,
                        'severity_score': severity_score
                    })
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†æ•°é™åºæ’åº
        channel_high_tier_analysis.sort(key=lambda x: x['severity_score'], reverse=True)
        
        return {
            'metric': 'é«˜çº¿åŸå¸‚å æ¯”',
            'anomaly_direction': anomaly.get('direction', ''),
            'anomaly_value': anomaly.get('current_value', ''),
            'channel_impacts': channel_high_tier_analysis[:10],
            'analysis_type': 'high_tier_ratio_analysis'
        }
    
    def _analyze_good_rate_by_channel(self, channel_df: pd.DataFrame, prev_channel_df: pd.DataFrame, 
                                    anomaly: dict, date_str: str, conn: sqlite3.Connection) -> dict:
        """åˆ†æGoodç‡å¼‚å¸¸çš„æ¸ é“å½±å“ - æ”¹è¿›ç‰ˆ"""
        
        print(f"DEBUG: Goodç‡æ¸ é“åˆ†æå¼€å§‹ - {date_str}")
        
        # è·å–å‰ä¸€æ—¥æ—¥æœŸ
        prev_date = (datetime.strptime(date_str, '%Y-%m-%d') - timedelta(days=1)).strftime('%Y-%m-%d')
        
        # å½“å‰æ—¥æœŸGoodç‡æŸ¥è¯¢
        current_sql = f"""
            SELECT 
                ad_channel,
                SUM(newuser) as total_users,
                SUM(CASE WHEN status = 'good' THEN newuser ELSE 0 END) as good_users,
                ROUND(SUM(CASE WHEN status = 'good' THEN newuser ELSE 0 END) * 100.0 / 
                      NULLIF(SUM(newuser), 0), 2) as good_rate
            FROM cpz_qs_newuser_channel_i_d 
            WHERE dt = '{date_str}'
            GROUP BY ad_channel
            HAVING total_users >= 100
        """
        
        # å‰ä¸€æ—¥Goodç‡æŸ¥è¯¢
        prev_sql = f"""
            SELECT 
                ad_channel,
                SUM(newuser) as total_users,
                SUM(CASE WHEN status = 'good' THEN newuser ELSE 0 END) as good_users,
                ROUND(SUM(CASE WHEN status = 'good' THEN newuser ELSE 0 END) * 100.0 / 
                      NULLIF(SUM(newuser), 0), 2) as good_rate
            FROM cpz_qs_newuser_channel_i_d 
            WHERE dt = '{prev_date}'
            GROUP BY ad_channel
            HAVING total_users >= 100
        """
        
        try:
            current_df = pd.read_sql_query(current_sql, conn)
            prev_df = pd.read_sql_query(prev_sql, conn)
            
            if current_df.empty:
                return {'channel_impacts': [], 'analysis_type': 'good_rate_analysis'}
            
            # åˆå¹¶æ•°æ®
            merged_df = pd.merge(current_df, prev_df, on='ad_channel', how='left', suffixes=('_current', '_prev'))
            merged_df['good_rate_change'] = merged_df['good_rate_current'] - merged_df['good_rate_prev'].fillna(merged_df['good_rate_current'])
            merged_df['good_rate_change_pct'] = ((merged_df['good_rate_current'] - merged_df['good_rate_prev']) / 
                                                  merged_df['good_rate_prev'].replace(0, np.nan) * 100).fillna(0)
            
            # è®¡ç®—æ•´ä½“Goodç‡å˜åŒ–
            total_current_good_rate = (current_df['good_users'].sum() / current_df['total_users'].sum() * 100 
                                       if current_df['total_users'].sum() > 0 else 0)
            total_prev_good_rate = (prev_df['good_users'].sum() / prev_df['total_users'].sum() * 100 
                                   if not prev_df.empty and prev_df['total_users'].sum() > 0 else total_current_good_rate)
            overall_good_rate_change = total_current_good_rate - total_prev_good_rate
            
            print(f"DEBUG: æ•´ä½“Goodç‡ - å½“å‰:{total_current_good_rate:.2f}%, å‰æ—¥:{total_prev_good_rate:.2f}%, å˜åŒ–:{overall_good_rate_change:+.2f}pp")
            
            # è¯†åˆ«å¼‚å¸¸æ¸ é“
            channel_impacts = []
            total_users = current_df['total_users'].sum()
            
            for _, row in merged_df.iterrows():
                if pd.isna(row['good_rate_current']):
                    continue
                
                good_rate_current = float(row['good_rate_current'])
                good_rate_prev = float(row['good_rate_prev']) if pd.notna(row['good_rate_prev']) else good_rate_current
                good_rate_change = float(row['good_rate_change'])
                users_current = int(row['total_users_current'])
                
                # æƒé‡è®¡ç®—
                weight = users_current / total_users if total_users > 0 else 0
                
                # åŠ æƒå½±å“
                weighted_impact = good_rate_change * weight
                
                # åˆ¤æ–­å¼‚å¸¸
                is_abnormal = False
                contribution_reasons = []
                
                if abs(good_rate_change) > 3:  # Goodç‡å˜åŒ–è¶…è¿‡3ä¸ªç™¾åˆ†ç‚¹
                    is_abnormal = True
                    direction = "ä¸‹é™" if good_rate_change < 0 else "ä¸Šå‡"
                    contribution_reasons.append(f"Goodç‡{direction}{abs(good_rate_change):.1f}pp")
                
                if is_abnormal or abs(weighted_impact) >= 0.1:
                    channel_impacts.append({
                        'channel': row['ad_channel'],
                        'good_rate_current': good_rate_current,
                        'good_rate_prev': good_rate_prev,
                        'good_rate_change': good_rate_change,
                        'total_users': users_current,
                        'weight': weight,
                        'weighted_impact': weighted_impact,
                        'contribution_reasons': contribution_reasons
                    })
            
            # è®¡ç®—è´¡çŒ®åº¦å¹¶æ’åº
            for impact in channel_impacts:
                if overall_good_rate_change != 0:
                    impact['contribution_to_overall'] = (impact['weighted_impact'] / abs(overall_good_rate_change)) * 100
                else:
                    impact['contribution_to_overall'] = 0
                
                # ä¸¥é‡ç¨‹åº¦è¯„åˆ†
                impact['severity_score'] = abs(impact['weighted_impact']) * 100 + abs(impact['good_rate_change']) * impact['weight'] * 10
            
            # åŒºåˆ†æ­£è´Ÿè´¡çŒ®ï¼Œåªå…³æ³¨çœŸæ­£çš„è´Ÿé¢è´¡çŒ®ï¼ˆè‡ªèº«ä¸‹é™åŠ å‰§æ•´ä½“ä¸‹é™çš„æ¸ é“ï¼‰
            if overall_good_rate_change < 0:
                # æ•´ä½“ä¸‹é™æ—¶ï¼Œåªæ˜¾ç¤ºå¯¹ä¸‹é™æœ‰è´Ÿé¢è´¡çŒ®çš„æ¸ é“ï¼ˆå³è‡ªèº«ä¹Ÿä¸‹é™çš„æ¸ é“ï¼‰
                # contribution < 0 è¡¨ç¤ºè¯¥æ¸ é“åŠ å‰§äº†æ•´ä½“ä¸‹é™
                negative_impacts = [impact for impact in channel_impacts if impact.get('contribution_to_overall', 0) < 0]
                # æŒ‰è´Ÿé¢è´¡çŒ®åº¦çš„ç»å¯¹å€¼æ’åºï¼ˆæœ€è´Ÿé¢çš„åœ¨å‰ï¼‰
                negative_impacts.sort(key=lambda x: x.get('contribution_to_overall', 0))
                significant_impacts = negative_impacts[:3]
            else:
                # æ•´ä½“ä¸Šå‡æ—¶ï¼Œæ˜¾ç¤ºæ‰€æœ‰å¼‚å¸¸æ¸ é“
                channel_impacts.sort(key=lambda x: abs(x.get('contribution_to_overall', 0)), reverse=True)
                significant_impacts = channel_impacts[:3]
            
            # è®¡ç®—è´Ÿé¢è´¡çŒ®åº¦æ€»å’Œï¼ˆç»å¯¹å€¼ï¼‰
            negative_contribution = sum([abs(min(0, impact.get('contribution_to_overall', 0))) for impact in significant_impacts])
            
            # ç§»é™¤å•ç‹¬å¼ºè°ƒæŸä¸ªæ¸ é“çš„æ¶ˆæ¯
            main_contributor_msg = ""
                
            print(f"âœ… Goodç‡æ¸ é“å¼‚å¸¸åˆ†æå®Œæˆï¼Œè¯†åˆ«{len(significant_impacts)}ä¸ªå¼‚å¸¸æ¸ é“")
            
            return {
                'metric': 'Goodç‡å¼‚å¸¸',
                'anomaly_direction': 'æ¸ é“åˆ†æ',
                'anomaly_value': f'{total_current_good_rate:.2f}%',
                'channel_impacts': significant_impacts,
                'analysis_type': 'good_rate_analysis',
                'analysis_summary': f'è¯†åˆ«{len(significant_impacts)}ä¸ªGoodç‡å¼‚å¸¸æ¸ é“',
                'total_contribution': negative_contribution,
                'main_contributor_msg': main_contributor_msg,
                'overall_change': overall_good_rate_change
            }
            
        except Exception as e:
            print(f"ERROR: Goodç‡å¼‚å¸¸åˆ†æå¤±è´¥: {e}")
            return {
                'metric': 'Goodç‡å¼‚å¸¸',
                'channel_impacts': [],
                'analysis_type': 'good_rate_analysis',
                'analysis_summary': f'Goodç‡å¼‚å¸¸åˆ†æå¤±è´¥: {str(e)}'
            }
    
    def _analyze_verified_rate_by_channel(self, channel_df: pd.DataFrame, prev_channel_df: pd.DataFrame, 
                                        anomaly: dict, date_str: str, conn: sqlite3.Connection) -> dict:
        """åˆ†æè®¤è¯ç‡å¼‚å¸¸çš„æ¸ é“å½±å“ - æ”¹è¿›ç‰ˆ"""
        
        print(f"DEBUG: è®¤è¯ç‡æ¸ é“åˆ†æå¼€å§‹ - {date_str}")
        
        # è·å–å‰ä¸€æ—¥æ—¥æœŸ
        prev_date = (datetime.strptime(date_str, '%Y-%m-%d') - timedelta(days=1)).strftime('%Y-%m-%d')
        
        # å½“å‰æ—¥æœŸè®¤è¯ç‡æŸ¥è¯¢
        current_sql = f"""
            SELECT 
                ad_channel,
                SUM(CASE WHEN status = 'good' THEN newuser ELSE 0 END) as good_users,
                SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END) as quality_users,
                ROUND(SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END) * 100.0 / 
                      NULLIF(SUM(CASE WHEN status = 'good' THEN newuser ELSE 0 END), 0), 2) as verified_rate
            FROM cpz_qs_newuser_channel_i_d 
            WHERE dt = '{date_str}'
            GROUP BY ad_channel
            HAVING good_users >= 50
        """
        
        # å‰ä¸€æ—¥è®¤è¯ç‡æŸ¥è¯¢
        prev_sql = f"""
            SELECT 
                ad_channel,
                SUM(CASE WHEN status = 'good' THEN newuser ELSE 0 END) as good_users,
                SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END) as quality_users,
                ROUND(SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END) * 100.0 / 
                      NULLIF(SUM(CASE WHEN status = 'good' THEN newuser ELSE 0 END), 0), 2) as verified_rate
            FROM cpz_qs_newuser_channel_i_d 
            WHERE dt = '{prev_date}'
            GROUP BY ad_channel
            HAVING good_users >= 50
        """
        
        try:
            current_df = pd.read_sql_query(current_sql, conn)
            prev_df = pd.read_sql_query(prev_sql, conn)
            
            if current_df.empty:
                return {'channel_impacts': [], 'analysis_type': 'verified_rate_analysis'}
            
            # åˆå¹¶æ•°æ®
            merged_df = pd.merge(current_df, prev_df, on='ad_channel', how='left', suffixes=('_current', '_prev'))
            merged_df['verified_rate_change'] = merged_df['verified_rate_current'] - merged_df['verified_rate_prev'].fillna(merged_df['verified_rate_current'])
            
            # è®¡ç®—æ•´ä½“è®¤è¯ç‡å˜åŒ–
            total_current_verified_rate = (current_df['quality_users'].sum() / current_df['good_users'].sum() * 100 
                                          if current_df['good_users'].sum() > 0 else 0)
            total_prev_verified_rate = (prev_df['quality_users'].sum() / prev_df['good_users'].sum() * 100 
                                       if not prev_df.empty and prev_df['good_users'].sum() > 0 else total_current_verified_rate)
            overall_verified_rate_change = total_current_verified_rate - total_prev_verified_rate
            
            print(f"DEBUG: æ•´ä½“è®¤è¯ç‡ - å½“å‰:{total_current_verified_rate:.2f}%, å‰æ—¥:{total_prev_verified_rate:.2f}%, å˜åŒ–:{overall_verified_rate_change:+.2f}pp")
            
            # è¯†åˆ«å¼‚å¸¸æ¸ é“
            channel_impacts = []
            total_good_users = current_df['good_users'].sum()
            
            for _, row in merged_df.iterrows():
                if pd.isna(row['verified_rate_current']):
                    continue
                
                verified_rate_current = float(row['verified_rate_current'])
                verified_rate_prev = float(row['verified_rate_prev']) if pd.notna(row['verified_rate_prev']) else verified_rate_current
                verified_rate_change = float(row['verified_rate_change'])
                good_users_current = int(row['good_users_current'])
                
                # æƒé‡è®¡ç®—ï¼ˆåŸºäºGoodç”¨æˆ·ï¼‰
                weight = good_users_current / total_good_users if total_good_users > 0 else 0
                
                # åŠ æƒå½±å“
                weighted_impact = verified_rate_change * weight
                
                # åˆ¤æ–­å¼‚å¸¸
                is_abnormal = False
                contribution_reasons = []
                
                if abs(verified_rate_change) > 3:  # è®¤è¯ç‡å˜åŒ–è¶…è¿‡3ä¸ªç™¾åˆ†ç‚¹
                    is_abnormal = True
                    direction = "ä¸‹é™" if verified_rate_change < 0 else "ä¸Šå‡"
                    contribution_reasons.append(f"è®¤è¯ç‡{direction}{abs(verified_rate_change):.1f}pp")
                
                if is_abnormal or abs(weighted_impact) >= 0.1:
                    channel_impacts.append({
                        'channel': row['ad_channel'],
                        'verified_rate_current': verified_rate_current,
                        'verified_rate_prev': verified_rate_prev,
                        'verified_rate_change': verified_rate_change,
                        'good_users': good_users_current,
                        'quality_users': int(row['quality_users_current']),
                        'weight': weight,
                        'weighted_impact': weighted_impact,
                        'contribution_reasons': contribution_reasons
                    })
            
            # è®¡ç®—è´¡çŒ®åº¦å¹¶æ’åº
            for impact in channel_impacts:
                if overall_verified_rate_change != 0:
                    impact['contribution_to_overall'] = (impact['weighted_impact'] / abs(overall_verified_rate_change)) * 100
                else:
                    impact['contribution_to_overall'] = 0
                
                # ä¸¥é‡ç¨‹åº¦è¯„åˆ†
                impact['severity_score'] = abs(impact['weighted_impact']) * 100 + abs(impact['verified_rate_change']) * impact['weight'] * 10
            
            # åŒºåˆ†æ­£è´Ÿè´¡çŒ®ï¼Œåªå…³æ³¨çœŸæ­£çš„è´Ÿé¢è´¡çŒ®ï¼ˆè‡ªèº«ä¸‹é™åŠ å‰§æ•´ä½“ä¸‹é™çš„æ¸ é“ï¼‰
            if overall_verified_rate_change < 0:
                # æ•´ä½“ä¸‹é™æ—¶ï¼Œåªæ˜¾ç¤ºå¯¹ä¸‹é™æœ‰è´Ÿé¢è´¡çŒ®çš„æ¸ é“ï¼ˆå³è‡ªèº«ä¹Ÿä¸‹é™çš„æ¸ é“ï¼‰
                # contribution < 0 è¡¨ç¤ºè¯¥æ¸ é“åŠ å‰§äº†æ•´ä½“ä¸‹é™
                negative_impacts = [impact for impact in channel_impacts if impact.get('contribution_to_overall', 0) < 0]
                # æŒ‰è´Ÿé¢è´¡çŒ®åº¦çš„ç»å¯¹å€¼æ’åºï¼ˆæœ€è´Ÿé¢çš„åœ¨å‰ï¼‰
                negative_impacts.sort(key=lambda x: x.get('contribution_to_overall', 0))
                significant_impacts = negative_impacts[:3]
            else:
                # æ•´ä½“ä¸Šå‡æ—¶ï¼Œæ˜¾ç¤ºæ‰€æœ‰å¼‚å¸¸æ¸ é“
                channel_impacts.sort(key=lambda x: abs(x.get('contribution_to_overall', 0)), reverse=True)
                significant_impacts = channel_impacts[:3]
            
            # è®¡ç®—è´Ÿé¢è´¡çŒ®åº¦æ€»å’Œï¼ˆç»å¯¹å€¼ï¼‰
            negative_contribution = sum([abs(min(0, impact.get('contribution_to_overall', 0))) for impact in significant_impacts])
            
            # ç§»é™¤å•ç‹¬å¼ºè°ƒæŸä¸ªæ¸ é“çš„æ¶ˆæ¯
            main_contributor_msg = ""
                
            print(f"âœ… è®¤è¯ç‡æ¸ é“å¼‚å¸¸åˆ†æå®Œæˆï¼Œè¯†åˆ«{len(significant_impacts)}ä¸ªå¼‚å¸¸æ¸ é“")
            
            return {
                'metric': 'è®¤è¯ç‡å¼‚å¸¸',
                'anomaly_direction': 'æ¸ é“åˆ†æ',
                'anomaly_value': f'{total_current_verified_rate:.2f}%',
                'channel_impacts': significant_impacts,
                'analysis_type': 'verified_rate_analysis',
                'analysis_summary': f'è¯†åˆ«{len(significant_impacts)}ä¸ªè®¤è¯ç‡å¼‚å¸¸æ¸ é“',
                'total_contribution': negative_contribution,
                'main_contributor_msg': main_contributor_msg,
                'overall_change': overall_verified_rate_change
            }
            
        except Exception as e:
            print(f"ERROR: è®¤è¯ç‡å¼‚å¸¸åˆ†æå¤±è´¥: {e}")
            return {
                'metric': 'è®¤è¯ç‡å¼‚å¸¸',
                'channel_impacts': [],
                'analysis_type': 'verified_rate_analysis',
                'analysis_summary': f'è®¤è¯ç‡å¼‚å¸¸åˆ†æå¤±è´¥: {str(e)}'
            }
    
    def _analyze_quality_rate_by_channel(self, channel_df: pd.DataFrame, prev_channel_df: pd.DataFrame, 
                                       anomaly: dict, date_str: str, conn: sqlite3.Connection) -> dict:
        """åˆ†æGoodä¸”è®¤è¯ç‡å¼‚å¸¸çš„æ¸ é“å½±å“"""
        
        # è®¡ç®—æ•´ä½“Goodä¸”è®¤è¯ç‡ä½œä¸ºåŸºå‡†
        total_all_users = channel_df['total_users'].sum()
        total_quality = channel_df['quality_users'].sum()
        overall_quality_rate = (total_quality / total_all_users * 100) if total_all_users > 0 else 0
        
        # è®¡ç®—å„æ¸ é“çš„Goodä¸”è®¤è¯ç‡å’Œæƒé‡å½±å“
        channel_quality_rate_analysis = []
        
        for _, row in channel_df.iterrows():
            total_users = row.get('total_users', 0)
            quality_users = row.get('quality_users', 0)
            
            if total_users >= 50:  # æ ·æœ¬é‡è¦æ±‚
                quality_rate = (quality_users / total_users * 100) if total_users > 0 else 0
                
                # åªä¿ç•™Goodä¸”è®¤è¯ç‡ä½äºæ•´ä½“æ°´å¹³çš„æ¸ é“ï¼ˆè´Ÿå‘å½±å“ï¼‰
                if quality_rate < overall_quality_rate:
                    # è®¡ç®—æƒé‡å½±å“ï¼šè¯¥æ¸ é“å¯¹æ•´ä½“Goodä¸”è®¤è¯ç‡ä¸‹é™çš„è´¡çŒ®
                    weight = total_users / total_all_users if total_all_users > 0 else 0
                    quality_rate_impact = (quality_rate - overall_quality_rate) * weight
                    
                    # è®¡ç®—ä¸¥é‡ç¨‹åº¦åˆ†æ•°
                    severity_score = abs(quality_rate_impact) * (1 + weight * 10)
                    
                    channel_quality_rate_analysis.append({
                        'channel': row['ad_channel'],
                        'total_users': total_users,
                        'quality_users': quality_users,
                        'quality_rate': quality_rate,
                        'vs_overall': quality_rate - overall_quality_rate,
                        'weight': weight,
                        'weighted_impact': quality_rate_impact,
                        'severity_score': severity_score
                    })
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†æ•°é™åºæ’åº
        channel_quality_rate_analysis.sort(key=lambda x: x['severity_score'], reverse=True)
        
        return {
            'metric': 'Goodä¸”è®¤è¯ç‡',
            'anomaly_direction': anomaly.get('direction', ''),
            'anomaly_value': anomaly.get('current_value', ''),
            'channel_impacts': channel_quality_rate_analysis[:10],
            'analysis_type': 'quality_rate_analysis'
        }
    
    def _generate_html_report(self, date_str: str, core_data: dict, anomaly_data: dict, creative_data: dict, account_data: dict = None, conn=None) -> str:
        """ç”Ÿæˆæ ‡å‡†åŒ–HTMLæŠ¥å‘Š - ä¸¥æ ¼æŒ‰ç…§è§„èŒƒç»“æ„"""
        
        # ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„æ•°æ®éƒ½ä¸ä¸ºNone
        date_str = date_str or "Unknown Date"
        core_data = core_data or {}
        anomaly_data = anomaly_data or {}
        creative_data = creative_data or {}
        account_data = account_data or {}
        
        # ä¿®å¤Noneå€¼ï¼Œç¡®ä¿æ¨¡æ¿æ¸²æŸ“æ­£å¸¸
        if core_data.get('retention_rate') is None:
            core_data['retention_rate'] = 0
        if core_data.get('prev_retention_rate') is None:
            core_data['prev_retention_rate'] = 0
        
        # ä¿®å¤raw_dataä¸­çš„Noneå€¼
        raw_data = core_data.get('raw_data', {})
        for key in raw_data:
            if raw_data[key] is None:
                raw_data[key] = 0
        
        html = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>æ¯æ—¥ä¸šåŠ¡æ•°æ®æŠ¥å‘Š - {date_str}</title>
    <style>
        body {{ 
            font-family: 'PingFang SC', 'Microsoft YaHei', Arial, sans-serif; 
            margin: 0; padding: 20px; background-color: #f5f7fa; line-height: 1.4; font-size: 13px;
        }}
        .container {{ 
            max-width: 1200px; margin: 0 auto; background: white; 
            padding: 30px; border-radius: 10px; box-shadow: 0 4px 20px rgba(0,0,0,0.1);
        }}
        .header {{ 
            text-align: center; border-bottom: 3px solid #3498db; 
            padding-bottom: 20px; margin-bottom: 30px;
        }}
        .module {{ 
            margin: 40px 0; background: #f8f9fc; padding: 25px; 
            border-radius: 8px; border-left: 4px solid #3498db;
        }}
        .module h2 {{ 
            color: #2c3e50; margin-top: 0; font-size: 18px;
        }}
        .metrics-grid {{ 
            display: grid; grid-template-columns: repeat(auto-fit, minmax(180px, 1fr)); 
            gap: 15px; margin: 20px 0;
        }}
        .metric-card {{ 
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
            color: white; padding: 15px; border-radius: 6px; text-align: center;
        }}
        .metric-card .icon {{ font-size: 16px; margin-bottom: 5px; }}
        .metric-card .value {{ font-size: 16px; font-weight: bold; margin: 5px 0; }}
        .metric-card .label {{ font-size: 11px; opacity: 0.9; }}
        .anomaly-item {{ 
            background: #fff3cd; border: 1px solid #ffeaa7; 
            padding: 15px; margin: 10px 0; border-radius: 5px;
        }}
        .anomaly-high {{ background: #f8d7da; border-color: #f5c6cb; }}
        table {{ 
            width: 100%; border-collapse: collapse; margin-top: 20px; 
            background: white; border-radius: 8px; overflow: hidden;
        }}
        th {{ 
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
            color: white; padding: 15px; text-align: left;
        }}
        .rules-note {{
            background: #e8f4fd; border: 1px solid #bee5eb; 
            padding: 15px; margin: 20px 0; border-radius: 5px; font-size: 12px;
            color: #0c5460;
        }}
        td {{ padding: 12px; border-bottom: 1px solid #ecf0f1; }}
        tr:nth-child(even) {{ background-color: #f8f9fa; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“Š æ¯æ—¥ä¸šåŠ¡æ•°æ®æŠ¥å‘Š</h1>
            <p>ğŸ“… {date_str} | â° {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
        
        <div class="rules-note">
            âœ… <strong>æŠ¥å‘Šè§„èŒƒç¡®è®¤</strong>: è®¤è¯ç‡={core_data.get('quality_users', 0):,}Ã·{core_data.get('raw_data', {}).get('good_users', 0):,}={core_data.get('verified_rate', 0):.2f}% | 
            æ¬¡ç•™ç‡åŸºäºGoodä¸”è®¤è¯ç”¨æˆ· | ç”¨æˆ·è´¨é‡åŸºäºGoodä¸”è®¤è¯ç”¨æˆ·
        </div>
        
        <!-- ç¬¬ä¸€éƒ¨åˆ†ï¼šæ ¸å¿ƒæŒ‡æ ‡ -->
        <div class="module">
            <h2>ğŸ“Š æ¨¡å—ä¸€ï¼šæ ¸å¿ƒæŒ‡æ ‡</h2>
            
            <h3 style="font-size: 14px; margin: 15px 0 10px 0;">ğŸ¯ æ ¸å¿ƒä¸šåŠ¡</h3>
            <div class="metrics-grid">
                <div class="metric-card">
                    <div class="icon">â­</div>
                    <div class="value">{core_data.get('quality_users', 0):,}</div>
                    <div class="label">Goodä¸”è®¤è¯ç”¨æˆ·æ•°</div>
                </div>
                <div class="metric-card">
                    <div class="icon">ğŸ’°</div>
                    <div class="value">Â¥{core_data.get('cpa', 0):.2f}</div>
                    <div class="label">CPA</div>
                </div>
                <div class="metric-card">
                    <div class="icon">ğŸ“ˆ</div>
                    <div class="value">Â¥{core_data.get('arpu', 0):.2f}</div>
                    <div class="label">ARPU</div>
                </div>
                <div class="metric-card">
                    <div class="icon">ğŸ”„</div>
                    <div class="value">{f"{core_data.get('prev_retention_rate', 0):.2f}%" if core_data.get('prev_retention_rate') is not None else f"{core_data.get('retention_rate', 0):.2f}%"}</div>
                    <div class="label">æ¬¡ç•™ç‡{f"ï¼ˆ{core_data.get('prev_retention_date', '')}ï¼‰" if core_data.get('prev_retention_rate') is not None else ""}</div>
                </div>
            </div>
            
            <h3 style="font-size: 14px; margin: 15px 0 10px 0;">ğŸ‘¥ ç”¨æˆ·è´¨é‡</h3>
            <div class="metrics-grid">
                <div class="metric-card">
                    <div class="icon">ğŸ‘©</div>
                    <div class="value">{core_data.get('female_ratio', 0):.2f}%</div>
                    <div class="label">å¥³æ€§å æ¯”</div>
                </div>
                <div class="metric-card">
                    <div class="icon">ğŸ§‘â€ğŸ’¼</div>
                    <div class="value">{core_data.get('young_ratio', 0):.2f}%</div>
                    <div class="label">å¹´è½»å æ¯”ï¼ˆ20-23å²ï¼‰</div>
                </div>
                <div class="metric-card">
                    <div class="icon">ğŸ™ï¸</div>
                    <div class="value">{core_data.get('high_tier_ratio', 0):.2f}%</div>
                    <div class="label">é«˜çº¿åŸå¸‚å æ¯”</div>
                </div>
            </div>
            
            <h3 style="font-size: 14px; margin: 15px 0 10px 0;">ğŸ¯ æ³¨å†Œè½¬åŒ–</h3>
            <div class="metrics-grid">
                <div class="metric-card">
                    <div class="icon">âœ…</div>
                    <div class="value">{core_data['good_rate']:.2f}%</div>
                    <div class="label">Goodç‡</div>
                </div>
                <div class="metric-card">
                    <div class="icon">ğŸ”</div>
                    <div class="value">{core_data['verified_rate']:.2f}%</div>
                    <div class="label">è®¤è¯ç‡</div>
                </div>
                <div class="metric-card">
                    <div class="icon">ğŸ¯</div>
                    <div class="value">{core_data['quality_rate']:.2f}%</div>
                    <div class="label">Goodä¸”è®¤è¯ç‡</div>
                </div>
            </div>
        </div>
        
        <!-- ç¬¬äºŒéƒ¨åˆ†ï¼šå¼‚å¸¸åˆ†æ -->
        <div class="module">
            <h2>ğŸš¨ æ¨¡å—äºŒï¼šå¼‚å¸¸åˆ†æ</h2>
            
            <!-- å¼‚å¸¸æ£€æµ‹ç»“æœéƒ¨åˆ†å·²æ•´åˆåˆ°æ™ºèƒ½é—®é¢˜å®šä½ -->"""
        
        # å¼‚å¸¸æ£€æµ‹ç»“æœå·²æ•´åˆåˆ°æ™ºèƒ½é—®é¢˜å®šä½ï¼Œè¿™é‡Œå®Œå…¨éšè—
        if False and anomaly_data['anomalies']:
            # åˆ†ç±»ç»Ÿè®¡
            serious_anomalies = [a for a in anomaly_data['anomalies'] if a.get('type') == 'serious']
            sensitive_anomalies = [a for a in anomaly_data['anomalies'] if a.get('type') == 'sensitive']
            
            
            # æ˜¾ç¤ºä¸¥é‡å¼‚å¸¸ - ä¼˜åŒ–æ ·å¼
            if serious_anomalies:
                html += """<h4 style="
                    color: #dc2626;
                    margin: 20px 0 12px 0;
                    font-size: 16px;
                    font-weight: bold;
                ">ğŸš¨ ä¸¥é‡å¼‚å¸¸ (è¶…å‡ºIQRèŒƒå›´)</h4>"""
                for anomaly in serious_anomalies:
                    html += f"""
                    <div style="
                        background: linear-gradient(135deg, #fef2f2 0%, #fecaca 100%);
                        border: none;
                        border-left: 4px solid #dc2626;
                        border-radius: 8px;
                        padding: 16px;
                        margin: 12px 0;
                        box-shadow: 0 2px 4px rgba(220, 38, 38, 0.1);
                    ">
                        <div style="display: flex; align-items: center; margin-bottom: 8px;">
                            <span style="
                                background: #dc2626;
                                color: white;
                                padding: 4px 8px;
                                border-radius: 12px;
                                font-size: 12px;
                                font-weight: bold;
                                margin-right: 8px;
                            ">ğŸš¨</span>
                            <strong style="color: #991b1b; font-size: 16px;">{anomaly['metric']}</strong>
                            <span style="
                                background: #fca5a5;
                                color: #7f1d1d;
                                padding: 2px 8px;
                                border-radius: 10px;
                                font-size: 12px;
                                margin-left: 8px;
                            ">{anomaly['direction']}</span>
                        </div>
                        <div style="
                            background: rgba(255, 255, 255, 0.7);
                            border-radius: 6px;
                            padding: 12px;
                            margin: 8px 0;
                        ">
                            <div style="color: #1f2937; font-weight: 600; margin-bottom: 4px;">
                                å½“å‰å€¼: <span style="color: #dc2626;">{anomaly['current_value']}</span>
                            </div>
                            <div style="color: #6b7280; font-size: 14px; margin-bottom: 4px;">
                                æ­£å¸¸èŒƒå›´: <span style="color: #059669;">{anomaly['normal_range']}</span>
                            </div>
                            <div style="color: #6b7280; font-size: 14px; line-height: 1.4;">
                                <span style="color: #991b1b;">ğŸ“Š æ£€æµ‹åŸå› :</span> {anomaly['reason']}
                            </div>
                        </div>
                    </div>"""
            
            # æ˜¾ç¤ºæ•æ„Ÿå¼‚å¸¸ - ä¼˜åŒ–æ ·å¼
            if sensitive_anomalies:
                for anomaly in sensitive_anomalies:
                    html += f"""
                    <div style="
                        background: linear-gradient(135deg, #fffbeb 0%, #fef3c7 100%);
                        border: none;
                        border-left: 4px solid #f59e0b;
                        border-radius: 8px;
                        padding: 16px;
                        margin: 12px 0;
                        box-shadow: 0 2px 4px rgba(245, 158, 11, 0.1);
                        transition: all 0.2s ease;
                    ">
                        <div style="display: flex; align-items: center; margin-bottom: 8px;">
                            <span style="
                                background: #f59e0b;
                                color: white;
                                padding: 4px 8px;
                                border-radius: 12px;
                                font-size: 12px;
                                font-weight: bold;
                                margin-right: 8px;
                            ">âš ï¸</span>
                            <strong style="color: #92400e; font-size: 16px;">{anomaly['metric']}</strong>
                        </div>
                        <div style="
                            background: rgba(255, 255, 255, 0.6);
                            border-radius: 6px;
                            padding: 12px;
                            margin: 8px 0;
                        ">
                            <div style="color: #1f2937; font-weight: 600; margin-bottom: 4px;">
                                å½“å‰å€¼: <span style="color: #d97706;">{anomaly['current_value']}</span>
                            </div>
                            <div style="color: #6b7280; font-size: 14px; line-height: 1.4;">
                                <span style="color: #92400e;">ğŸ“Š æ£€æµ‹åŸå› :</span> {anomaly['reason']}
                            </div>
                        </div>
                    </div>"""
        # ä¸å†æ˜¾ç¤º"æœªæ£€æµ‹åˆ°å¼‚å¸¸"ï¼Œå› ä¸ºå¼‚å¸¸ä¿¡æ¯å·²æ•´åˆåˆ°é—®é¢˜å®šä½éƒ¨åˆ†
        
        # ç®€åŒ–å¼‚å¸¸æŒ‡æ ‡æ˜¾ç¤º - åªæ˜¾ç¤ºé—®é¢˜å’Œæ•°æ®
        smart_analysis = anomaly_data.get('smart_channel_analysis', {})
        if smart_analysis.get('has_analysis'):
            html += f"""
            <h3>ğŸ¯ é—®é¢˜å®šä½</h3>"""
            
            for analysis in smart_analysis.get('channel_impact_analysis', []):
                metric = analysis.get('metric', '')
                analysis_type = analysis.get('analysis_type', '')
                
                if analysis_type == 'intelligent_root_cause':
                    # ç®€æ´æ–‡æœ¬æ ¼å¼
                    html += f"""
                    <div style="margin: 15px 0;">
                        <h4 style="color: #dc2626; margin: 0 0 8px 0; font-size: 14px;">
                            âš ï¸ {metric} å¼‚å¸¸
                        </h4>"""
                    
                    # è·å–å¹¶æ˜¾ç¤ºå¼‚å¸¸æ£€æµ‹åŸå› 
                    anomaly_reason = ""
                    for anomaly in anomaly_data.get('anomalies', []):
                        if anomaly.get('metric') == metric:
                            anomaly_reason = anomaly.get('reason', '')
                            break
                    
                    if anomaly_reason:
                        html += f"""<p style="margin: 5px 0; color: #666; font-size: 13px;">{anomaly_reason}</p>"""
                    
                    # æŸ¥æ‰¾å¼‚å¸¸æ¸ é“æ•°æ®
                    channel_data_found = False
                    if analysis.get('root_causes'):
                        for cause in analysis['root_causes']:
                            # CPAæ¸ é“åˆ†æ
                            if cause.get('dimension') == 'CPAæ¸ é“åˆ†æ' and cause.get('cpa_channel_detail'):
                                cpa_detail = cause['cpa_channel_detail']
                                channel_data = cpa_detail.get('channel_data', [])[:3]
                                if channel_data:
                                    for i, channel in enumerate(channel_data, 1):
                                        channel_name = channel.get('channel', 'æœªçŸ¥æ¸ é“')
                                        channel_cpa = channel.get('channel_cpa', 0)
                                        excess_cpa = channel.get('excess_cpa', 0)
                                        users = channel.get('quality_users', 0)
                                        severity_score = channel.get('severity_score', 0)
                                        if users >= 10000:
                                            users_display = f"{users/1000:.1f}K"
                                        else:
                                            users_display = f"{users:,}"
                                        html += f"â€¢ <strong>{channel_name}</strong>: CPAÂ¥{channel_cpa:.1f} (é«˜{excess_cpa:.1f}å…ƒ), {users_display}ç”¨æˆ·<br>"
                                    channel_data_found = True
                                break
                            
                            # ARPUæ¸ é“åˆ†æ
                            elif cause.get('dimension') == 'ARPUæ¸ é“åˆ†æ' and cause.get('arpu_channel_detail'):
                                arpu_detail = cause['arpu_channel_detail']
                                channel_impacts = arpu_detail.get('channel_impacts', [])[:3]
                                
                                # æ·»åŠ ä¸»è¦è´¡çŒ®æ¸ é“è¯´æ˜
                                main_contributor_msg = arpu_detail.get('main_contributor_msg', '')
                                overall_arpu_change = arpu_detail.get('overall_arpu_change', 0)
                                total_contribution = arpu_detail.get('total_contribution', 0)
                                
                                if main_contributor_msg:
                                    html += f"<strong style='color: #d32f2f;'>âš ï¸ {main_contributor_msg}</strong><br><br>"
                                
                                if overall_arpu_change < 0 and total_contribution > 50:
                                    html += f"<strong>æ•´ä½“ARPUä¸‹é™{abs(overall_arpu_change):.2f}å…ƒï¼Œä»¥ä¸‹æ¸ é“è´¡çŒ®äº†{total_contribution:.1f}%çš„ä¸‹é™:</strong><br>"
                                
                                if channel_impacts:
                                    for i, impact in enumerate(channel_impacts, 1):
                                        channel_name = impact.get('channel', 'æœªçŸ¥æ¸ é“')
                                        arpu_current = impact.get('arpu_current', 0)
                                        arpu_change = impact.get('arpu_change', 0)
                                        users = impact.get('quality_users', 0)
                                        weight = impact.get('weight', 0) * 100
                                        contribution = impact.get('contribution_to_overall', 0)
                                        severity_score = impact.get('severity_score', 0)
                                        if users >= 10000:
                                            users_display = f"{users/1000:.1f}K"
                                        else:
                                            users_display = f"{users:,}"
                                        change_direction = "ä¸‹é™" if arpu_change < 0 else "ä¸Šå‡"
                                        
                                        # ç®€æ´æ–‡æœ¬æ ¼å¼
                                        html += f"â€¢ <strong>{channel_name}</strong>: Â¥{arpu_current:.2f} ({change_direction}{abs(arpu_change):.2f}å…ƒ), {users_display}ç”¨æˆ·, è´¡çŒ®{abs(contribution):.1f}%<br>"
                                    channel_data_found = True
                                break
                            
                            # Goodç‡æ¸ é“åˆ†æ
                            elif cause.get('dimension') == 'Goodç‡æ¸ é“åˆ†æ' and cause.get('good_rate_channel_detail'):
                                good_rate_detail = cause['good_rate_channel_detail']
                                channel_impacts = good_rate_detail.get('channel_impacts', [])[:3]
                                
                                # æ·»åŠ ä¸»è¦è´¡çŒ®æ¸ é“è¯´æ˜
                                main_contributor_msg = good_rate_detail.get('main_contributor_msg', '')
                                overall_change = good_rate_detail.get('overall_change', 0)
                                total_contribution = good_rate_detail.get('total_contribution', 0)
                                
                                if main_contributor_msg:
                                    html += f"<strong style='color: #d32f2f;'>âš ï¸ {main_contributor_msg}</strong><br><br>"
                                
                                if channel_impacts:
                                    for i, impact in enumerate(channel_impacts, 1):
                                        channel_name = impact.get('channel', 'æœªçŸ¥æ¸ é“')
                                        good_rate_current = impact.get('good_rate_current', 0)
                                        good_rate_change = impact.get('good_rate_change', 0)
                                        users = impact.get('total_users', 0)
                                        weight = impact.get('weight', 0) * 100
                                        contribution = impact.get('contribution_to_overall', 0)
                                        if users >= 10000:
                                            users_display = f"{users/1000:.1f}K"
                                        else:
                                            users_display = f"{users:,}"
                                        change_direction = "ä¸‹é™" if good_rate_change < 0 else "ä¸Šå‡"
                                        
                                        # ç®€æ´æ–‡æœ¬æ ¼å¼
                                        html += f"â€¢ <strong>{channel_name}</strong>: {good_rate_current:.2f}% ({change_direction}{abs(good_rate_change):.2f}pp), {users_display}ç”¨æˆ·, è´¡çŒ®{abs(contribution):.1f}%<br>"
                                    channel_data_found = True
                                break
                            
                            # è®¤è¯ç‡æ¸ é“åˆ†æ
                            elif cause.get('dimension') == 'è®¤è¯ç‡æ¸ é“åˆ†æ' and cause.get('verified_rate_channel_detail'):
                                verified_rate_detail = cause['verified_rate_channel_detail']
                                channel_impacts = verified_rate_detail.get('channel_impacts', [])[:3]
                                
                                # æ·»åŠ ä¸»è¦è´¡çŒ®æ¸ é“è¯´æ˜
                                main_contributor_msg = verified_rate_detail.get('main_contributor_msg', '')
                                overall_change = verified_rate_detail.get('overall_change', 0)
                                total_contribution = verified_rate_detail.get('total_contribution', 0)
                                
                                if main_contributor_msg:
                                    html += f"<strong style='color: #d32f2f;'>âš ï¸ {main_contributor_msg}</strong><br><br>"
                                
                                if channel_impacts:
                                    for i, impact in enumerate(channel_impacts, 1):
                                        channel_name = impact.get('channel', 'æœªçŸ¥æ¸ é“')
                                        verified_rate_current = impact.get('verified_rate_current', 0)
                                        verified_rate_change = impact.get('verified_rate_change', 0)
                                        users = impact.get('good_users', 0)
                                        weight = impact.get('weight', 0) * 100
                                        contribution = impact.get('contribution_to_overall', 0)
                                        if users >= 10000:
                                            users_display = f"{users/1000:.1f}K"
                                        else:
                                            users_display = f"{users:,}"
                                        change_direction = "ä¸‹é™" if verified_rate_change < 0 else "ä¸Šå‡"
                                        
                                        # ç®€æ´æ–‡æœ¬æ ¼å¼
                                        html += f"â€¢ <strong>{channel_name}</strong>: {verified_rate_current:.2f}% ({change_direction}{abs(verified_rate_change):.2f}pp), {users_display}Goodç”¨æˆ·, è´¡çŒ®{abs(contribution):.1f}%<br>"
                                    channel_data_found = True
                                break
                            
                            # Goodä¸”è®¤è¯ç”¨æˆ·æ•°æ¸ é“åˆ†æ
                            elif cause.get('dimension') == 'Goodä¸”è®¤è¯ç”¨æˆ·æ•°æ¸ é“åˆ†æ' and cause.get('quality_users_channel_detail'):
                                quality_users_detail = cause['quality_users_channel_detail']
                                channel_impacts = quality_users_detail.get('channel_impacts', [])[:3]
                                main_contributor_msg = quality_users_detail.get('main_contributor_msg', "")
                                
                                if channel_impacts:
                                    for i, impact in enumerate(channel_impacts, 1):
                                        channel_name = impact.get('channel', 'æœªçŸ¥æ¸ é“')
                                        quality_users_current = impact.get('quality_users_current', 0)
                                        quality_users_change = impact.get('quality_users_change', 0)
                                        weight = impact.get('weight', 0) * 100
                                        contribution = impact.get('contribution_to_overall', 0)
                                        if quality_users_current >= 10000:
                                            users_display = f"{quality_users_current/1000:.1f}K"
                                        else:
                                            users_display = f"{quality_users_current:,}"
                                        change_direction = "ä¸‹é™" if quality_users_change < 0 else "ä¸Šå‡"
                                        
                                        # ç®€æ´æ–‡æœ¬æ ¼å¼
                                        html += f"â€¢ <strong>{channel_name}</strong>: {users_display} ({change_direction}{abs(quality_users_change):,}ç”¨æˆ·), è´¡çŒ®{abs(contribution):.1f}%<br>"
                                    channel_data_found = True
                                break
                            
                            # Goodä¸”è®¤è¯ç‡æ¸ é“åˆ†æ
                            elif cause.get('dimension') == 'Goodä¸”è®¤è¯ç‡æ¸ é“åˆ†æ' and cause.get('quality_rate_channel_detail'):
                                quality_rate_detail = cause['quality_rate_channel_detail']
                                channel_impacts = quality_rate_detail.get('channel_impacts', [])[:3]
                                main_contributor_msg = quality_rate_detail.get('main_contributor_msg', "")
                                
                                if channel_impacts:
                                    for i, impact in enumerate(channel_impacts, 1):
                                        channel_name = impact.get('channel', 'æœªçŸ¥æ¸ é“')
                                        quality_rate_current = impact.get('quality_rate_current', 0)
                                        quality_rate_change = impact.get('quality_rate_change', 0)
                                        total_users_current = impact.get('total_users_current', 0)
                                        weight = impact.get('weight', 0) * 100
                                        contribution = impact.get('contribution_to_overall', 0)
                                        if total_users_current >= 10000:
                                            users_display = f"{total_users_current/1000:.1f}K"
                                        else:
                                            users_display = f"{total_users_current:,}"
                                        change_direction = "ä¸‹é™" if quality_rate_change < 0 else "ä¸Šå‡"
                                        
                                        # ç®€æ´æ–‡æœ¬æ ¼å¼
                                        html += f"â€¢ <strong>{channel_name}</strong>: {quality_rate_current:.2f}% ({change_direction}{abs(quality_rate_change):.2f}pp), {users_display}ç”¨æˆ·, è´¡çŒ®{abs(contribution):.1f}%<br>"
                                    channel_data_found = True
                                break
                            
                            # é€šç”¨æ¸ é“åˆ†æ
                            elif cause.get('dimension', '').endswith('æ¸ é“åˆ†æ') and cause.get('generic_channel_detail'):
                                generic_detail = cause['generic_channel_detail']
                                channel_impacts = generic_detail.get('channel_impacts', [])[:3]
                                if channel_impacts:
                                    html += "<strong>é—®é¢˜æ¸ é“:</strong><br>"
                                    for i, impact in enumerate(channel_impacts, 1):
                                        channel_name = impact.get('channel', 'æœªçŸ¥æ¸ é“')
                                        metric_current = impact.get('metric_current', 0)
                                        metric_change = impact.get('metric_change', 0)
                                        users = impact.get('users', 0)
                                        weight = impact.get('weight', 0) * 100
                                        severity_score = impact.get('severity_score', 0)
                                        if users >= 10000:
                                            users_display = f"{users/1000:.1f}K"
                                        else:
                                            users_display = f"{users:,}"
                                        change_direction = "ä¸‹é™" if metric_change < 0 else "ä¸Šå‡"
                                        
                                        # æ ¹æ®æŒ‡æ ‡ç±»å‹æ˜¾ç¤ºå•ä½
                                        metric_display = f"{metric_current:.1f}%"
                                        change_display = f"{change_direction}{abs(metric_change):.1f}pp"
                                        
                                        html += f"â€¢ {channel_name}: {metric_display} ({change_display}, {users_display}ç”¨æˆ·, ä¸¥é‡ç¨‹åº¦{severity_score:.1f})<br>"
                                    channel_data_found = True
                                break
                    
                    if not channel_data_found:
                        html += """<p style="color: #666; font-style: italic;">ğŸ” å¼‚å¸¸åŸå› åˆ†æä¸­...</p>"""
                    
                    html += "</div>"  # å…³é—­å¼‚å¸¸å®¹å™¨
                
                else:
                    # å…¶ä»–å¼‚å¸¸ç±»å‹æ˜¾ç¤º
                    html += f"""
                    <div class="anomaly-item" style="border-left: 4px solid #fd7e14;">
                        <strong>ğŸ¯ {metric} å¼‚å¸¸</strong><br>"""
                    
                    # æ˜¾ç¤ºå‰3ä¸ªæœ€ä¸¥é‡çš„æ¸ é“
                    top_impacts = analysis.get('channel_impacts', [])[:3]
                    if top_impacts:
                        html += "<strong>é—®é¢˜æ¸ é“:</strong><br>"
                        for impact in top_impacts:
                            if analysis_type == 'cpa_distribution':
                                summary = f"â€¢ {impact['channel']}: CPAÂ¥{impact['channel_cpa']:.0f}"
                            elif analysis_type == 'arpu_impact':
                                change = impact.get('arpu_change', 0)
                                direction = "ä¸‹é™" if change < 0 else "ä¸Šå‡"
                                summary = f"â€¢ {impact['channel']}: ARPU{direction}{abs(change):.1f}å…ƒ"
                            elif analysis_type == 'generic_channel_analysis':
                                # é€šç”¨æ¸ é“åˆ†æï¼ˆåŒ…æ‹¬è®¤è¯ç‡ç­‰ï¼‰
                                metric_current = impact.get('metric_current', 0)
                                metric_change = impact.get('metric_change', 0)
                                users = impact.get('users', 0)
                                weight = impact.get('weight', 0) * 100
                                
                                if users >= 10000:
                                    users_display = f"{users/1000:.1f}K"
                                else:
                                    users_display = f"{users:,}"
                                
                                change_direction = "ä¸‹é™" if metric_change < 0 else "ä¸Šå‡"
                                summary = f"â€¢ {impact['channel']}: {metric_current:.1f}% ({change_direction}{abs(metric_change):.1f}pp, {users_display}ç”¨æˆ·, {weight:.1f}%æƒé‡)"
                            else:
                                summary = f"â€¢ {impact.get('channel', 'æœªçŸ¥æ¸ é“')}"
                            html += summary + "<br>"
                    
                    html += "</div>"
        else:
            html += """
            <h3>ğŸ¯ é—®é¢˜å®šä½</h3>
            <div class="anomaly-item">
                <strong>âœ… æœªæ£€æµ‹åˆ°å¼‚å¸¸</strong><br>
                æ‰€æœ‰æ ¸å¿ƒæŒ‡æ ‡éƒ½åœ¨æ­£å¸¸èŒƒå›´å†…
            </div>"""
        
        html += """
        </div>"""
        
        # ç¬¬ä¸‰éƒ¨åˆ†ï¼šè´¦æˆ·åˆ†æï¼ˆåŸç¬¬å››éƒ¨åˆ†ï¼‰
        if account_data:
            html += self._generate_account_analysis_html(account_data, date_str, conn)
        
        # ç¬¬å››éƒ¨åˆ†ï¼šç´ æåˆ†æï¼ˆåŸç¬¬ä¸‰éƒ¨åˆ†ï¼‰
        html += self._generate_creative_analysis_html(creative_data)
        
        html += """
        <div style="margin-top: 40px; padding-top: 20px; border-top: 1px solid #ecf0f1; color: #7f8c8d; font-size: 14px;">
            <p>ğŸ“Š æŠ¥å‘Šè§„èŒƒè¯´æ˜ï¼š</p>
            <ul>
                <li><strong>è®¤è¯ç‡</strong> = Goodä¸”è®¤è¯ç”¨æˆ·æ•° Ã· Goodç”¨æˆ·æ•°ï¼ˆä¸æ˜¯æ€»ç”¨æˆ·æ•°ï¼‰</li>
                <li><strong>æ¬¡ç•™ç‡</strong> = Goodä¸”è®¤è¯ç”¨æˆ·ç•™å­˜æ•° Ã· Goodä¸”è®¤è¯ç”¨æˆ·æ•°</li>
                <li><strong>ç”¨æˆ·è´¨é‡æŒ‡æ ‡</strong> = åŸºäºGoodä¸”è®¤è¯ç”¨æˆ·è®¡ç®—</li>
                <li><strong>å¼‚å¸¸æ£€æµ‹</strong> = ä½¿ç”¨IQRï¼ˆå››åˆ†ä½è·ï¼‰æ–¹æ³•ï¼Œæ£€æµ‹æ¨¡å—1ä¸­æ‰€æœ‰æ ¸å¿ƒæŒ‡æ ‡ï¼ˆ14å¤©å†å²æ•°æ®ï¼‰</li>
                <li><strong>æ™ºèƒ½æ¸ é“åˆ†æ</strong> = åŸºäºå¼‚å¸¸æ£€æµ‹ç»“æœï¼Œè‡ªåŠ¨åˆ†æARPU/CPA/ç”¨æˆ·æ•°å¼‚å¸¸çš„æ¸ é“å½±å“ï¼ŒæŒ‰å½±å“ç¨‹åº¦é™åºæ’åº</li>
                <li><strong>æ¸ é“åˆ†æèŒƒå›´</strong> = ä»…åŒ…å«ç”¨æˆ·æ•°â‰¥50çš„æ¸ é“ï¼ŒæŒ‰åŠ æƒå½±å“/ç»å¯¹å˜åŒ–æ’åº</li>
            </ul>
            <p><small>âš ï¸ æœ¬æŠ¥å‘Šä¸¥æ ¼æŒ‰ç…§å†…ç½®è§„èŒƒç”Ÿæˆï¼Œç¡®ä¿æ•°æ®å‡†ç¡®æ€§å’Œä¸€è‡´æ€§</small></p>
        </div>
    </div>

<script>
function filterAccounts() {
    const osFilter = document.getElementById('osFilter').value;
    const genderFilter = document.getElementById('genderFilter').value;
    const accountCards = document.querySelectorAll('.account-card');
    
    accountCards.forEach(card => {
        const cardOs = card.getAttribute('data-os');
        const cardGender = card.getAttribute('data-gender');
        
        const osMatch = (osFilter === 'all' || osFilter === cardOs);
        const genderMatch = (genderFilter === 'all' || genderFilter === cardGender);
        
        if (osMatch && genderMatch) {
            card.style.display = 'block';
        } else {
            card.style.display = 'none';
        }
    });
    
    // æ›´æ–°æ¯ä¸ªåˆ†ç»„çš„æ˜¾ç¤ºè®¡æ•°
    updateGroupCounts();
}

function resetFilters() {
    document.getElementById('osFilter').value = 'all';
    document.getElementById('genderFilter').value = 'all';
    
    const accountCards = document.querySelectorAll('.account-card');
    accountCards.forEach(card => {
        card.style.display = 'block';
    });
    
    // æ›´æ–°æ¯ä¸ªåˆ†ç»„çš„æ˜¾ç¤ºè®¡æ•°
    updateGroupCounts();
}

function updateGroupCounts() {
    // æ›´æ–°ç™½é¢†å æ¯”é™ä½è´¦æˆ·è®¡æ•°
    const whiteCollarContainer = document.querySelector('h4[style*="color: #e74c3c"]');
    if (whiteCollarContainer) {
        const visibleCards = whiteCollarContainer.parentElement.querySelectorAll('.account-card:not([style*="display: none"])');
        const totalCards = whiteCollarContainer.parentElement.querySelectorAll('.account-card');
        if (visibleCards.length === totalCards.length) {
            whiteCollarContainer.innerHTML = `ğŸ¢ ç™½é¢†å æ¯”é™ä½è´¦æˆ· (${totalCards.length}ä¸ª)`;
        } else {
            whiteCollarContainer.innerHTML = `ğŸ¢ ç™½é¢†å æ¯”é™ä½è´¦æˆ· (æ˜¾ç¤º${visibleCards.length}/${totalCards.length}ä¸ª)`;
        }
    }
    
    // æ›´æ–°å¹´è½»å æ¯”å‡é«˜è´¦æˆ·è®¡æ•°
    const youngContainer = document.querySelector('h4[style*="color: #f39c12"]');
    if (youngContainer) {
        const visibleCards = youngContainer.parentElement.querySelectorAll('.account-card:not([style*="display: none"])');
        const totalCards = youngContainer.parentElement.querySelectorAll('.account-card');
        if (visibleCards.length === totalCards.length) {
            youngContainer.innerHTML = `ğŸ‘¶ å¹´è½»å æ¯”å‡é«˜è´¦æˆ· (${totalCards.length}ä¸ª)`;
        } else {
            youngContainer.innerHTML = `ğŸ‘¶ å¹´è½»å æ¯”å‡é«˜è´¦æˆ· (æ˜¾ç¤º${visibleCards.length}/${totalCards.length}ä¸ª)`;
        }
    }
    
    // æ›´æ–°ä¸‰çº¿åŸå¸‚å æ¯”å‡é«˜è´¦æˆ·è®¡æ•°
    const thirdTierContainer = document.querySelector('h4[style*="color: #9b59b6"]');
    if (thirdTierContainer) {
        const visibleCards = thirdTierContainer.parentElement.querySelectorAll('.account-card:not([style*="display: none"])');
        const totalCards = thirdTierContainer.parentElement.querySelectorAll('.account-card');
        if (visibleCards.length === totalCards.length) {
            thirdTierContainer.innerHTML = `ğŸ™ï¸ ä¸‰çº¿åŸå¸‚å æ¯”å‡é«˜è´¦æˆ· (${totalCards.length}ä¸ª)`;
        } else {
            thirdTierContainer.innerHTML = `ğŸ™ï¸ ä¸‰çº¿åŸå¸‚å æ¯”å‡é«˜è´¦æˆ· (æ˜¾ç¤º${visibleCards.length}/${totalCards.length}ä¸ª)`;
        }
    }
}

// é¡µé¢åŠ è½½å®Œæˆååˆå§‹åŒ–
document.addEventListener('DOMContentLoaded', function() {
    updateGroupCounts();
});
</script>

</body>
</html>"""
        
        return html
    
    def _save_report(self, html_content: str, date_str: str) -> str:
        """ä¿å­˜æŠ¥å‘Šæ–‡ä»¶ - æŒ‰ç…§è§„èŒƒå‘½å"""
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs('./output/reports', exist_ok=True)
        
        # æŒ‰è§„èŒƒç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"./output/reports/daily_report_{date_str.replace('-', '')}_{timestamp}.html"
        
        # ä¿å­˜æ–‡ä»¶
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"âœ… æŠ¥å‘Šç”ŸæˆæˆåŠŸ: {filename}")
        print("ğŸŒ æ­£åœ¨è‡ªåŠ¨æ‰“å¼€æŠ¥å‘Š...")
        os.system(f"open '{filename}'")
        
        return filename
    
    def _validate_data_consistency(self, conn: sqlite3.Connection, date_str: str, core_data: dict) -> bool:
        """éªŒè¯æ•°æ®ä¸€è‡´æ€§ - é˜²æ­¢æ•°æ®é”™è¯¯"""
        if not self.data_validation_enabled:
            return True
            
        try:
            print("ğŸ” æ‰§è¡Œæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥...")
            
            # 1. éªŒè¯CPAè®¡ç®—ä¸€è‡´æ€§
            # æ–¹æ³•1ï¼šä¸»æŠ¥å‘Šçš„è®¡ç®—æ–¹æ³•ï¼ˆåˆ†ç¦»æŸ¥è¯¢ï¼‰
            user_query = "SELECT SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END) as quality_users FROM cpz_qs_newuser_channel_i_d WHERE dt = ?"
            cost_query = "SELECT COALESCE(SUM(cash_cost), 0) as total_cost FROM dwd_ttx_market_cash_cost_i_d WHERE dt = ?"
            
            user_result = pd.read_sql_query(user_query, conn, params=[date_str])
            cost_result = pd.read_sql_query(cost_query, conn, params=[date_str])
            
            main_quality_users = user_result.iloc[0]['quality_users']
            main_total_cost = cost_result.iloc[0]['total_cost']
            main_cpa = main_total_cost / main_quality_users if main_quality_users > 0 else 0
            
            # 2. éªŒè¯ä¸æ ¸å¿ƒæ•°æ®çš„ä¸€è‡´æ€§
            if abs(main_quality_users - core_data['quality_users']) > 1:
                print(f"âŒ æ•°æ®ä¸ä¸€è‡´è­¦å‘Š: ä¼˜è´¨ç”¨æˆ·æ•°ä¸åŒ¹é…")
                print(f"   ä¸»æŸ¥è¯¢: {main_quality_users:,} vs æ ¸å¿ƒæ•°æ®: {core_data['quality_users']:,}")
                return False
                
            if abs(main_cpa - core_data['cpa']) > 0.01:
                print(f"âŒ æ•°æ®ä¸ä¸€è‡´è­¦å‘Š: CPAè®¡ç®—ä¸åŒ¹é…")
                print(f"   ä¸»æŸ¥è¯¢: Â¥{main_cpa:.2f} vs æ ¸å¿ƒæ•°æ®: Â¥{core_data['cpa']:.2f}")
                return False
            
            # 3. éªŒè¯æ¸ é“æ•°æ®æ±‡æ€»ä¸€è‡´æ€§
            channel_summary_query = """
                WITH user_data AS (
                    SELECT SUM(CASE WHEN status='good' AND verification_status='verified' THEN newuser ELSE 0 END) as total_quality_users
                    FROM cpz_qs_newuser_channel_i_d WHERE dt = ?
                ),
                cost_data AS (
                    SELECT SUM(cash_cost) as total_cost
                    FROM dwd_ttx_market_cash_cost_i_d WHERE dt = ?  
                )
                SELECT u.total_quality_users, c.total_cost
                FROM user_data u, cost_data c
            """
            
            channel_result = pd.read_sql_query(channel_summary_query, conn, params=[date_str, date_str])
            if not channel_result.empty:
                channel_users = channel_result.iloc[0]['total_quality_users']
                channel_cost = channel_result.iloc[0]['total_cost']
                
                # å…è®¸10%çš„è¯¯å·®èŒƒå›´ï¼ˆè€ƒè™‘æ¸ é“æ˜ å°„å·®å¼‚ï¼‰
                user_diff_pct = abs(channel_users - main_quality_users) / main_quality_users * 100 if main_quality_users > 0 else 0
                cost_diff_pct = abs(channel_cost - main_total_cost) / main_total_cost * 100 if main_total_cost > 0 else 0
                
                if user_diff_pct > 10:
                    print(f"âš ï¸  æ•°æ®å·®å¼‚è­¦å‘Š: æ¸ é“æ±‡æ€»ç”¨æˆ·æ•°å·®å¼‚{user_diff_pct:.1f}%")
                    print(f"   ç›´æ¥æŸ¥è¯¢: {main_quality_users:,} vs æ¸ é“æ±‡æ€»: {channel_users:,}")
                
                if cost_diff_pct > 10:
                    print(f"âš ï¸  æ•°æ®å·®å¼‚è­¦å‘Š: æ¸ é“æ±‡æ€»æˆæœ¬å·®å¼‚{cost_diff_pct:.1f}%")
                    print(f"   ç›´æ¥æŸ¥è¯¢: Â¥{main_total_cost:,.2f} vs æ¸ é“æ±‡æ€»: Â¥{channel_cost:,.2f}")
            
            print("âœ… æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def _analyze_age_group_impact(self, metric_name: str, date_str: str, prev_date: str, conn: sqlite3.Connection) -> dict:
        """åˆ†æå¹´é¾„æ®µç»´åº¦å¯¹å¼‚å¸¸çš„å½±å“"""
        try:
            # è·å–å¹´é¾„æ®µæ•°æ®å¯¹æ¯”
            if 'å¥³æ€§å æ¯”' in metric_name:
                query = f"""
                WITH curr_data AS (
                    SELECT 
                        age_group,
                        SUM(CASE WHEN gender = 'female' AND status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END) as female_users,
                        SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END) as total_users
                    FROM cpz_qs_newuser_channel_i_d 
                    WHERE dt = '{date_str}' AND age_group <> ''
                    GROUP BY age_group
                ),
                prev_data AS (
                    SELECT 
                        age_group,
                        SUM(CASE WHEN gender = 'female' AND status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END) * 100.0 / 
                        SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END) as female_pct_prev
                    FROM cpz_qs_newuser_channel_i_d 
                    WHERE dt = '{prev_date}' AND age_group <> ''
                    GROUP BY age_group
                )
                SELECT 
                    curr_data.age_group,
                    curr_data.female_users,
                    curr_data.total_users,
                    curr_data.female_users * 100.0 / curr_data.total_users as female_pct_curr,
                    prev_data.female_pct_prev,
                    (curr_data.female_users * 100.0 / curr_data.total_users) - prev_data.female_pct_prev as pct_change
                FROM curr_data
                LEFT JOIN prev_data ON curr_data.age_group = prev_data.age_group
                WHERE curr_data.total_users >= 50
                ORDER BY ABS((curr_data.female_users * 100.0 / curr_data.total_users) - prev_data.female_pct_prev) DESC
                """
            
            # æ‰§è¡ŒæŸ¥è¯¢å¹¶åˆ†æç»“æœ - è¿™é‡Œç®€åŒ–å¤„ç†
            return {'has_impact': False, 'analysis': 'å¹´é¾„æ®µåˆ†æå·²ç®€åŒ–'}
            
        except Exception as e:
            print(f"å¹´é¾„æ®µåˆ†æé”™è¯¯: {e}")
            return {'has_impact': False, 'error': str(e)}

    def _has_continuous_trend(self, values: list, threshold: float = 0.1) -> bool:
        """æ£€æµ‹æ˜¯å¦å­˜åœ¨è¿ç»­è¶‹åŠ¿"""
        if len(values) < 3:
            return False
        
        # æ£€æŸ¥æœ€å3ä¸ªå€¼æ˜¯å¦å‘ˆç°è¿ç»­å˜åŒ–
        last_3 = values[-3:]
        if len(last_3) < 3:
            return False
        
        # å¦‚æœè¿ç»­ä¸Šå‡æˆ–è¿ç»­ä¸‹é™ï¼Œè®¤ä¸ºæ˜¯è¿ç»­è¶‹åŠ¿
        increasing = all(last_3[i] <= last_3[i+1] for i in range(len(last_3)-1))
        decreasing = all(last_3[i] >= last_3[i+1] for i in range(len(last_3)-1))
        
        return increasing or decreasing

    def _analyze_quality_users_by_channel_enhanced(self, channel_df: pd.DataFrame, prev_channel_df: pd.DataFrame, 
                                                 anomaly: dict, date_str: str, conn: sqlite3.Connection) -> dict:
        """åˆ†æGoodä¸”è®¤è¯ç”¨æˆ·æ•°å¼‚å¸¸çš„æ¸ é“å½±å“ï¼Œæ”¯æŒè´¡çŒ®åº¦è®¡ç®—"""
        try:
            print(f"DEBUG: Goodä¸”è®¤è¯ç”¨æˆ·æ•°æ¸ é“åˆ†æå¼€å§‹ - {date_str}")
            
            # è·å–å½“å‰å’Œå‰ä¸€å¤©çš„æ•°æ®
            current_query = f"""
                SELECT ad_channel,
                       SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END) as quality_users_current
                FROM cpz_qs_newuser_channel_i_d 
                WHERE dt = '{date_str}' AND ad_channel != '(unknown)'
                GROUP BY ad_channel
                HAVING quality_users_current >= 50
                ORDER BY quality_users_current DESC
            """
            
            prev_date = (datetime.strptime(date_str, '%Y-%m-%d') - timedelta(days=1)).strftime('%Y-%m-%d')
            prev_query = f"""
                SELECT ad_channel,
                       SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END) as quality_users_prev
                FROM cpz_qs_newuser_channel_i_d 
                WHERE dt = '{prev_date}' AND ad_channel != '(unknown)'
                GROUP BY ad_channel
                HAVING quality_users_prev >= 50
                ORDER BY quality_users_prev DESC
            """
            
            current_df = pd.read_sql_query(current_query, conn)
            prev_df = pd.read_sql_query(prev_query, conn)
            
            # è®¡ç®—æ•´ä½“å˜åŒ–
            total_current_quality_users = current_df['quality_users_current'].sum() if not current_df.empty else 0
            total_prev_quality_users = prev_df['quality_users_prev'].sum() if not prev_df.empty else 0
            overall_quality_users_change = total_current_quality_users - total_prev_quality_users
            
            print(f"DEBUG: æ•´ä½“Goodä¸”è®¤è¯ç”¨æˆ·æ•° - å½“å‰:{total_current_quality_users}, å‰æ—¥:{total_prev_quality_users}, å˜åŒ–:{overall_quality_users_change}")
            
            # åˆå¹¶æ•°æ®
            merged_df = pd.merge(current_df, prev_df, on='ad_channel', how='outer').fillna(0)
            
            # åˆ†ææ¯ä¸ªæ¸ é“çš„å½±å“
            channel_impacts = []
            for _, row in merged_df.iterrows():
                quality_users_current = row['quality_users_current']
                quality_users_prev = row['quality_users_prev'] 
                quality_users_change = quality_users_current - quality_users_prev
                
                # è®¡ç®—æƒé‡
                weight = quality_users_current / total_current_quality_users if total_current_quality_users > 0 else 0
                
                # è®¡ç®—åŠ æƒå½±å“ = ç”¨æˆ·æ•°å˜åŒ– * æƒé‡
                weighted_impact = quality_users_change * weight
                
                if abs(quality_users_change) >= 10:  # åªå…³æ³¨å˜åŒ–è¾ƒå¤§çš„æ¸ é“
                    channel_impacts.append({
                        'channel': row['ad_channel'],
                        'quality_users_current': quality_users_current,
                        'quality_users_prev': quality_users_prev,
                        'quality_users_change': quality_users_change,
                        'weight': weight,
                        'weighted_impact': weighted_impact,
                        'severity_score': abs(quality_users_change) * (1 + weight * 10)
                    })
            
            # è®¡ç®—è´¡çŒ®åº¦
            for impact in channel_impacts:
                if overall_quality_users_change != 0:
                    impact['contribution_to_overall'] = (impact['weighted_impact'] / abs(overall_quality_users_change)) * 100
                else:
                    impact['contribution_to_overall'] = 0
            
            # ç­›é€‰å’Œæ’åºæ˜¾è‘—å½±å“
            if overall_quality_users_change < 0:
                # æ•´ä½“ä¸‹é™æ—¶ï¼Œåªæ˜¾ç¤ºå¯¹ä¸‹é™æœ‰è´Ÿé¢è´¡çŒ®çš„æ¸ é“
                negative_impacts = [impact for impact in channel_impacts if impact.get('contribution_to_overall', 0) < 0]
                negative_impacts.sort(key=lambda x: x.get('contribution_to_overall', 0))
                significant_impacts = negative_impacts[:3]
            else:
                # æ•´ä½“ä¸Šå‡æ—¶ï¼Œæ˜¾ç¤ºæ‰€æœ‰å¼‚å¸¸æ¸ é“
                channel_impacts.sort(key=lambda x: abs(x.get('contribution_to_overall', 0)), reverse=True)
                significant_impacts = channel_impacts[:3]
            
            print(f"âœ… Goodä¸”è®¤è¯ç”¨æˆ·æ•°æ¸ é“å¼‚å¸¸åˆ†æå®Œæˆï¼Œè¯†åˆ«{len(significant_impacts)}ä¸ªå¼‚å¸¸æ¸ é“")
            
            return {
                'metric': 'Goodä¸”è®¤è¯ç”¨æˆ·æ•°å¼‚å¸¸',
                'anomaly_direction': 'æ¸ é“åˆ†æ',
                'channel_impacts': significant_impacts,
                'analysis_type': 'quality_users_analysis',
                'analysis_summary': f'è¯†åˆ«{len(significant_impacts)}ä¸ªGoodä¸”è®¤è¯ç”¨æˆ·æ•°å¼‚å¸¸æ¸ é“',
                'overall_change': overall_quality_users_change,
                'main_contributor_msg': ""
            }
            
        except Exception as e:
            print(f"ERROR: Goodä¸”è®¤è¯ç”¨æˆ·æ•°å¼‚å¸¸åˆ†æå¤±è´¥: {e}")
            return {
                'metric': 'Goodä¸”è®¤è¯ç”¨æˆ·æ•°å¼‚å¸¸',
                'anomaly_direction': 'åˆ†æå¤±è´¥',
                'channel_impacts': [],
                'analysis_type': 'quality_users_analysis',
                'analysis_summary': f'Goodä¸”è®¤è¯ç”¨æˆ·æ•°å¼‚å¸¸åˆ†æå¤±è´¥: {str(e)}'
            }
    
    def _analyze_quality_rate_by_channel(self, channel_df: pd.DataFrame, prev_channel_df: pd.DataFrame, 
                                       anomaly: dict, date_str: str, conn: sqlite3.Connection) -> dict:
        """åˆ†æGoodä¸”è®¤è¯ç‡å¼‚å¸¸çš„æ¸ é“å½±å“ï¼Œæ”¯æŒè´¡çŒ®åº¦è®¡ç®—"""
        try:
            print(f"DEBUG: Goodä¸”è®¤è¯ç‡æ¸ é“åˆ†æå¼€å§‹ - {date_str}")
            
            # è·å–å½“å‰å’Œå‰ä¸€å¤©çš„æ•°æ®
            current_query = f"""
                SELECT ad_channel,
                       SUM(newuser) as total_users_current,
                       SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END) as quality_users_current,
                       ROUND(100.0 * SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END) / SUM(newuser), 2) as quality_rate_current
                FROM cpz_qs_newuser_channel_i_d 
                WHERE dt = '{date_str}' AND ad_channel != '(unknown)'
                GROUP BY ad_channel
                HAVING total_users_current >= 50
                ORDER BY quality_users_current DESC
            """
            
            prev_date = (datetime.strptime(date_str, '%Y-%m-%d') - timedelta(days=1)).strftime('%Y-%m-%d')
            prev_query = f"""
                SELECT ad_channel,
                       SUM(newuser) as total_users_prev,
                       SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END) as quality_users_prev,
                       ROUND(100.0 * SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END) / SUM(newuser), 2) as quality_rate_prev
                FROM cpz_qs_newuser_channel_i_d 
                WHERE dt = '{prev_date}' AND ad_channel != '(unknown)'
                GROUP BY ad_channel
                HAVING total_users_prev >= 50
                ORDER BY quality_users_prev DESC
            """
            
            current_df = pd.read_sql_query(current_query, conn)
            prev_df = pd.read_sql_query(prev_query, conn)
            
            # è®¡ç®—æ•´ä½“å˜åŒ–
            total_current_total_users = current_df['total_users_current'].sum() if not current_df.empty else 0
            total_current_quality_users = current_df['quality_users_current'].sum() if not current_df.empty else 0
            total_prev_total_users = prev_df['total_users_prev'].sum() if not prev_df.empty else 0
            total_prev_quality_users = prev_df['quality_users_prev'].sum() if not prev_df.empty else 0
            
            total_current_quality_rate = (total_current_quality_users / total_current_total_users * 100) if total_current_total_users > 0 else 0
            total_prev_quality_rate = (total_prev_quality_users / total_prev_total_users * 100) if total_prev_total_users > 0 else 0
            overall_quality_rate_change = total_current_quality_rate - total_prev_quality_rate
            
            print(f"DEBUG: æ•´ä½“Goodä¸”è®¤è¯ç‡ - å½“å‰:{total_current_quality_rate:.2f}%, å‰æ—¥:{total_prev_quality_rate:.2f}%, å˜åŒ–:{overall_quality_rate_change:.2f}pp")
            
            # åˆå¹¶æ•°æ®
            merged_df = pd.merge(current_df, prev_df, on='ad_channel', how='outer').fillna(0)
            
            # åˆ†ææ¯ä¸ªæ¸ é“çš„å½±å“
            channel_impacts = []
            for _, row in merged_df.iterrows():
                quality_rate_current = row['quality_rate_current']
                quality_rate_prev = row['quality_rate_prev'] 
                quality_rate_change = quality_rate_current - quality_rate_prev
                total_users_current = row['total_users_current']
                
                # è®¡ç®—æƒé‡ (åŸºäºå½“å‰ç”¨æˆ·æ•°)
                weight = total_users_current / total_current_total_users if total_current_total_users > 0 else 0
                
                # è®¡ç®—åŠ æƒå½±å“ = è´¨é‡ç‡å˜åŒ– * æƒé‡
                weighted_impact = quality_rate_change * weight
                
                if abs(quality_rate_change) >= 1:  # åªå…³æ³¨å˜åŒ–>=1ppçš„æ¸ é“
                    channel_impacts.append({
                        'channel': row['ad_channel'],
                        'quality_rate_current': quality_rate_current,
                        'quality_rate_prev': quality_rate_prev,
                        'quality_rate_change': quality_rate_change,
                        'total_users_current': total_users_current,
                        'weight': weight,
                        'weighted_impact': weighted_impact,
                        'severity_score': abs(quality_rate_change) * weight * 100
                    })
            
            # è®¡ç®—è´¡çŒ®åº¦
            for impact in channel_impacts:
                if overall_quality_rate_change != 0:
                    impact['contribution_to_overall'] = (impact['weighted_impact'] / abs(overall_quality_rate_change)) * 100
                else:
                    impact['contribution_to_overall'] = 0
            
            # ç­›é€‰å’Œæ’åºæ˜¾è‘—å½±å“
            if overall_quality_rate_change < 0:
                # æ•´ä½“ä¸‹é™æ—¶ï¼Œåªæ˜¾ç¤ºå¯¹ä¸‹é™æœ‰è´Ÿé¢è´¡çŒ®çš„æ¸ é“
                negative_impacts = [impact for impact in channel_impacts if impact.get('contribution_to_overall', 0) < 0]
                negative_impacts.sort(key=lambda x: x.get('contribution_to_overall', 0))
                significant_impacts = negative_impacts[:3]
            else:
                # æ•´ä½“ä¸Šå‡æ—¶ï¼Œæ˜¾ç¤ºæ‰€æœ‰å¼‚å¸¸æ¸ é“
                channel_impacts.sort(key=lambda x: abs(x.get('contribution_to_overall', 0)), reverse=True)
                significant_impacts = channel_impacts[:3]
            
            print(f"âœ… Goodä¸”è®¤è¯ç‡æ¸ é“å¼‚å¸¸åˆ†æå®Œæˆï¼Œè¯†åˆ«{len(significant_impacts)}ä¸ªå¼‚å¸¸æ¸ é“")
            
            return {
                'metric': 'Goodä¸”è®¤è¯ç‡å¼‚å¸¸',
                'anomaly_direction': 'æ¸ é“åˆ†æ',
                'channel_impacts': significant_impacts,
                'analysis_type': 'quality_rate_analysis',
                'analysis_summary': f'è¯†åˆ«{len(significant_impacts)}ä¸ªGoodä¸”è®¤è¯ç‡å¼‚å¸¸æ¸ é“',
                'overall_change': overall_quality_rate_change,
                'main_contributor_msg': ""
            }
            
        except Exception as e:
            print(f"ERROR: Goodä¸”è®¤è¯ç‡å¼‚å¸¸åˆ†æå¤±è´¥: {e}")
            return {
                'metric': 'Goodä¸”è®¤è¯ç‡å¼‚å¸¸',
                'anomaly_direction': 'åˆ†æå¤±è´¥',
                'channel_impacts': [],
                'analysis_type': 'quality_rate_analysis',
                'analysis_summary': f'Goodä¸”è®¤è¯ç‡å¼‚å¸¸åˆ†æå¤±è´¥: {str(e)}'
            }
    
    def _collect_creative_analysis(self, conn: sqlite3.Connection, date_str: str) -> dict:
        """æ”¶é›†ç´ æåˆ†ææ•°æ®"""
        try:
            print("ğŸ“Š å¼€å§‹ç´ æåˆ†ææ•°æ®æ”¶é›†...")
            
            # è®¡ç®—æ˜¨å¤©çš„æ—¥æœŸ
            from datetime import datetime, timedelta
            date_obj = datetime.strptime(date_str, '%Y-%m-%d')
            yesterday_obj = date_obj - timedelta(days=1)
            yesterday_str = yesterday_obj.strftime('%Y-%m-%d')
            
            # 1. è·å–ä»Šæ—¥ç´ ææ’å
            today_query = self.QUERIES['creative_ranking_today'].format(date=date_str)
            today_df = pd.read_sql_query(today_query, conn)
            
            # 2. è·å–æ˜¨æ—¥ç´ ææ’å
            yesterday_query = self.QUERIES['creative_ranking_yesterday'].format(yesterday_date=yesterday_str)
            yesterday_df = pd.read_sql_query(yesterday_query, conn)
            
            # 3. åˆå¹¶æ•°æ®ï¼Œè®¡ç®—æ’åå˜åŒ–
            creative_rankings = []
            
            # åˆ›å»ºæ˜¨æ—¥æ’åå­—å…¸ï¼ˆç°åœ¨æŒ‰ç´ æIDï¼Œä¸åˆ†æ¸ é“ï¼‰
            yesterday_ranks = {}
            for idx, row in yesterday_df.iterrows():
                creative_id = row['media_id_str']
                yesterday_ranks[creative_id] = row['rank_yesterday']
            
            # å¤„ç†ä»Šæ—¥æ’åæ•°æ®
            for idx, row in today_df.iterrows():
                creative_id = row['media_id_str']
                today_rank = idx + 1
                yesterday_rank = yesterday_ranks.get(creative_id, None)
                
                # è®¡ç®—æ’åå˜åŒ–
                rank_change = ""
                rank_change_num = 0
                if yesterday_rank is not None:
                    rank_change_num = yesterday_rank - today_rank  # æ­£æ•°è¡¨ç¤ºä¸Šå‡
                    if rank_change_num > 0:
                        rank_change = f"â†‘{rank_change_num}"
                    elif rank_change_num < 0:
                        rank_change = f"â†“{abs(rank_change_num)}"
                    else:
                        rank_change = "="
                else:
                    rank_change = "â­æ–°è¿›æ¦œ"
                
                # å¤„ç†ç©ºå€¼
                arpu = row['arpu'] if pd.notna(row['arpu']) else 0
                retention_rate = row['retention_rate'] if pd.notna(row['retention_rate']) else 0
                female_rate = row['female_rate'] if pd.notna(row['female_rate']) else 0
                white_collar_rate = row['white_collar_rate'] if pd.notna(row['white_collar_rate']) else 0
                ios_rate = row['ios_rate'] if pd.notna(row['ios_rate']) else 0
                
                # å¤„ç†æ¸ é“ä¿¡æ¯
                channels = row['channels'] if pd.notna(row['channels']) else ''
                channel_count = int(row['channel_count']) if pd.notna(row['channel_count']) else 0
                
                creative_rankings.append({
                    'rank': today_rank,
                    'creative_id': row['media_id_str'],
                    'channels': channels,
                    'channel_count': channel_count,
                    'good_verified': int(row['good_verified']),
                    'cost': float(row['cost']) if pd.notna(row['cost']) else 0,
                    'cpa': float(row['cpa']) if pd.notna(row['cpa']) else 0,
                    'arpu': float(arpu),
                    'retention_rate': float(retention_rate),
                    'female_rate': float(female_rate),
                    'white_collar_rate': float(white_collar_rate),
                    'ios_rate': float(ios_rate),
                    'yesterday_rank': yesterday_rank,
                    'rank_change': rank_change,
                    'rank_change_num': rank_change_num
                })
            
            # 4. ç”Ÿæˆåˆ†ææ´å¯Ÿ
            insights = self._generate_creative_insights(creative_rankings)
            
            print(f"âœ… ç´ æåˆ†æå®Œæˆï¼Œè·å¾—{len(creative_rankings)}ä¸ªç´ ææ’å")
            
            return {
                'rankings': creative_rankings,
                'insights': insights,
                'total_creatives': len(creative_rankings),
                'analysis_date': date_str,
                'yesterday_date': yesterday_str
            }
            
        except Exception as e:
            print(f"âŒ ç´ æåˆ†æå¤±è´¥: {e}")
            return {
                'rankings': [],
                'insights': {
                    'top_rising': None,
                    'top_falling': None,
                    'new_entries': [],
                    'cpa_anomalies': [],
                    'high_female_rate': None,
                    'high_ios_rate': None
                },
                'total_creatives': 0,
                'analysis_date': date_str,
                'error': str(e)
            }
    
    def _generate_creative_insights(self, rankings: list) -> dict:
        """ç”Ÿæˆç´ æåˆ†ææ´å¯Ÿ"""
        insights = {
            'top_rising': None,
            'top_falling': None,
            'new_entries': [],
            'cpa_anomalies': [],
            'high_female_rate': None,
            'high_ios_rate': None
        }
        
        if not rankings:
            return insights
        
        # æ‰¾åˆ°æ’åå˜åŒ–æœ€å¤§çš„ç´ æ
        rising_creatives = [c for c in rankings if c['rank_change_num'] > 0]
        falling_creatives = [c for c in rankings if c['rank_change_num'] < 0]
        new_entries = [c for c in rankings if c['yesterday_rank'] is None]
        
        if rising_creatives:
            insights['top_rising'] = max(rising_creatives, key=lambda x: x['rank_change_num'])
        
        if falling_creatives:
            insights['top_falling'] = min(falling_creatives, key=lambda x: x['rank_change_num'])
        
        insights['new_entries'] = new_entries[:3]  # åªæ˜¾ç¤ºå‰3ä¸ªæ–°è¿›æ¦œ
        
        # CPAå¼‚å¸¸åˆ†æï¼ˆè¶…è¿‡å¹³å‡å€¼2å€çš„ï¼‰
        valid_cpas = [c['cpa'] for c in rankings if c['cpa'] > 0]
        if valid_cpas:
            avg_cpa = sum(valid_cpas) / len(valid_cpas)
            cpa_threshold = avg_cpa * 2
            insights['cpa_anomalies'] = [c for c in rankings if c['cpa'] > cpa_threshold][:3]
        
        # å¥³æ€§å æ¯”æœ€é«˜çš„ç´ æ
        if rankings:
            insights['high_female_rate'] = max(rankings, key=lambda x: x['female_rate'])
            insights['high_ios_rate'] = max(rankings, key=lambda x: x['ios_rate'])
        
        return insights
    
    def _collect_account_analysis(self, conn: sqlite3.Connection, date_str: str, channel: str = 'Douyin') -> dict:
        """æ”¶é›†è´¦æˆ·åˆ†ææ•°æ®"""
        try:
            print(f"ğŸ“Š å¼€å§‹è´¦æˆ·åˆ†ææ•°æ®æ”¶é›†...")
            
            # è®¡ç®—æ˜¨æ—¥å’Œ7æ—¥å‰æ—¥æœŸ
            from datetime import datetime, timedelta
            current_date = datetime.strptime(date_str, '%Y-%m-%d')
            yesterday_str = (current_date - timedelta(days=1)).strftime('%Y-%m-%d')
            week_ago_str = (current_date - timedelta(days=7)).strftime('%Y-%m-%d')
            
            # 1. è·å–å½“æ—¥è´¦æˆ·æ•°æ®
            today_query = self.QUERIES['account_metrics_today'].format(date=date_str, channel=channel)
            today_df = pd.read_sql_query(today_query, conn)
            
            # 2. è·å–æ˜¨æ—¥è´¦æˆ·æ•°æ®
            yesterday_query = self.QUERIES['account_metrics_yesterday'].format(yesterday_date=yesterday_str, channel=channel)
            yesterday_df = pd.read_sql_query(yesterday_query, conn)
            
            # 3. è·å–7æ—¥å‡å€¼æ•°æ®
            avg_7d_query = self.QUERIES['account_metrics_7d_avg'].format(
                start_date=week_ago_str, end_date=yesterday_str, channel=channel
            )
            avg_7d_df = pd.read_sql_query(avg_7d_query, conn)
            
            # 4. æ£€æµ‹å¼‚å¸¸è´¦æˆ·
            anomaly_accounts = self._detect_account_anomalies(today_df, yesterday_df, avg_7d_df)
            
            # 5. è®¡ç®—ç›¸å…³æ€§
            correlation_analysis = self._calculate_age_arpu_correlation(today_df, yesterday_df)
            
            print(f"âœ… è´¦æˆ·åˆ†æå®Œæˆï¼Œè·å¾—{len(today_df)}ä¸ªè´¦æˆ·ï¼Œæ£€æµ‹åˆ°{len(anomaly_accounts)}ä¸ªå¼‚å¸¸è´¦æˆ·")
            
            return {
                'today_accounts': today_df,
                'yesterday_accounts': yesterday_df,
                'avg_7d_accounts': avg_7d_df,
                'anomaly_accounts': anomaly_accounts,
                'correlation_analysis': correlation_analysis,
                'channel': channel,
                'analysis_date': date_str,
                'yesterday_date': yesterday_str
            }
            
        except Exception as e:
            print(f"âŒ è´¦æˆ·åˆ†æå¤±è´¥: {e}")
            return {
                'today_accounts': pd.DataFrame(),
                'yesterday_accounts': pd.DataFrame(),
                'avg_7d_accounts': pd.DataFrame(),
                'anomaly_accounts': [],
                'correlation_analysis': {},
                'channel': channel,
                'analysis_date': date_str,
                'error': str(e)
            }
    
    def _detect_account_anomalies(self, today_df: pd.DataFrame, yesterday_df: pd.DataFrame, avg_7d_df: pd.DataFrame) -> dict:
        """æ£€æµ‹å¼‚å¸¸è´¦æˆ·ï¼šARPUé™ä½ä¸”ç”¨æˆ·è´¨é‡æŒ‡æ ‡è´Ÿå‘å˜åŒ–è¶…è¿‡1%"""
        # æŒ‰æŒ‡æ ‡åˆ†ç±»çš„å¼‚å¸¸è´¦æˆ·
        anomaly_by_metric = {
            'white_collar': [],
            'young': [],
            'third_tier_city': [],
            'overall_changes': {},
            'metrics_triggered': []
        }
        
        if today_df.empty or avg_7d_df.empty:
            return anomaly_by_metric
        
        # 1. é¦–å…ˆè®¡ç®—æ•´ä½“æŒ‡æ ‡å˜åŒ–ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦æ£€æµ‹å¯¹åº”ç±»å‹çš„å¼‚å¸¸
        overall_changes = self._calculate_overall_quality_changes(today_df, avg_7d_df)
        anomaly_by_metric['overall_changes'] = overall_changes
        
        # åªæœ‰å½“æ•´ä½“æŒ‡æ ‡è´Ÿå‘å˜åŒ–>1%æ—¶ï¼Œæ‰æ£€æµ‹å¯¹åº”çš„è´¦æˆ·å¼‚å¸¸
        quality_metrics_to_check = []
        arpu_change = overall_changes.get('arpu_change', 0) or 0
        white_collar_change = overall_changes.get('white_collar_change', 0) or 0
        young_change = overall_changes.get('young_change', 0) or 0
        third_tier_city_change = overall_changes.get('third_tier_city_change', 0) or 0
        
        if arpu_change < -1:  # ARPUä¸‹é™>1%
            quality_metrics_to_check.append('arpu')
        if white_collar_change < -1:  # ç™½é¢†å æ¯”ä¸‹é™>1%
            quality_metrics_to_check.append('white_collar')
            anomaly_by_metric['metrics_triggered'].append('white_collar')
        if young_change > 1:  # å¹´è½»å æ¯”ä¸Šå‡>1%
            quality_metrics_to_check.append('young')
            anomaly_by_metric['metrics_triggered'].append('young')
        if third_tier_city_change > 1:  # ä¸‰çº¿åŸå¸‚å æ¯”ä¸Šå‡>1%
            quality_metrics_to_check.append('third_tier_city')
            anomaly_by_metric['metrics_triggered'].append('third_tier_city')
        
        # å¦‚æœæ²¡æœ‰æ•´ä½“å¼‚å¸¸ï¼Œåˆ™ä¸æ£€æµ‹è´¦æˆ·å¼‚å¸¸
        if not quality_metrics_to_check:
            return anomaly_by_metric
        
        # 2. åˆ›å»º7æ—¥å‡å€¼å­—å…¸è¿›è¡Œè´¦æˆ·çº§åˆ«å¼‚å¸¸æ£€æµ‹
        avg_7d_dict = {}
        for _, row in avg_7d_df.iterrows():
            key = (row['account'], row['os'], row['gender'])
            avg_7d_dict[key] = row
        
        # 3. æ£€æµ‹æ¯ä¸ªè´¦æˆ·çš„å¼‚å¸¸
        for _, today_row in today_df.iterrows():
            account_key = (today_row['account'], today_row['os'], today_row['gender'])
            
            # å¿…é¡»æœ‰7æ—¥å‡å€¼æ•°æ®æ‰èƒ½æ¯”è¾ƒ
            if account_key not in avg_7d_dict:
                continue
                
            avg_7d_row = avg_7d_dict[account_key]
            
            # æ£€æµ‹ARPUé™ä½æ¡ä»¶
            arpu_today = today_row['arpu'] if pd.notna(today_row['arpu']) else 0
            arpu_7d_avg = avg_7d_row['arpu_avg'] if pd.notna(avg_7d_row['arpu_avg']) else 0
            arpu_decreased = arpu_today < arpu_7d_avg and arpu_7d_avg > 0
            
            # å¿…é¡»æ»¡è¶³ARPUé™ä½æ¡ä»¶
            if not arpu_decreased:
                continue
            
            # åˆ›å»ºåŸºç¡€è´¦æˆ·ä¿¡æ¯
            base_account = {
                'account': today_row['account'],
                'account_name': today_row['account_name'],
                'os': today_row['os'],
                'gender': today_row['gender'],
                'good_verified': int(today_row['good_verified']),
                'arpu_today': arpu_today,
                'arpu_7d_avg': arpu_7d_avg,
                'white_collar_rate_today': today_row.get('white_collar_rate', 0),
                'under_23_rate_today': today_row.get('under_23_rate', 0),
                'third_tier_city_rate_today': today_row.get('third_tier_city_rate', 0),
                'white_collar_rate_7d_avg': avg_7d_row.get('white_collar_rate_avg', 0),
                'under_23_rate_7d_avg': avg_7d_row.get('under_23_rate_avg', 0),
                'third_tier_city_rate_7d_avg': avg_7d_row.get('third_tier_city_rate_avg', 0),
            }
            
            # æ£€æµ‹å„é¡¹è´¨é‡æŒ‡æ ‡å¹¶åˆ†ç±»åˆ°å¯¹åº”åˆ—è¡¨
            
            # ç™½é¢†å æ¯”æ£€æµ‹
            if 'white_collar' in quality_metrics_to_check:
                white_collar_today = today_row.get('white_collar_rate', 0) if pd.notna(today_row.get('white_collar_rate', 0)) else 0
                white_collar_7d_avg = avg_7d_row.get('white_collar_rate_avg', 0) if pd.notna(avg_7d_row.get('white_collar_rate_avg', 0)) else 0
                white_collar_change = white_collar_today - white_collar_7d_avg
                if white_collar_change < -1:  # ç™½é¢†å æ¯”ä¸‹é™>1%
                    account_copy = base_account.copy()
                    account_copy.update({
                        'primary_anomaly': 'white_collar',
                        'primary_anomaly_desc': f'ç™½é¢†å æ¯”ä¸‹é™{abs(white_collar_change):.1f}pp',
                        'anomaly_change': white_collar_change
                    })
                    anomaly_by_metric['white_collar'].append(account_copy)
            
            # å¹´è½»å æ¯”æ£€æµ‹
            if 'young' in quality_metrics_to_check:
                young_today = today_row.get('under_23_rate', 0) if pd.notna(today_row.get('under_23_rate', 0)) else 0
                young_7d_avg = avg_7d_row.get('under_23_rate_avg', 0) if pd.notna(avg_7d_row.get('under_23_rate_avg', 0)) else 0
                young_change = young_today - young_7d_avg
                if young_change > 1:  # å¹´è½»å æ¯”ä¸Šå‡>1%
                    account_copy = base_account.copy()
                    account_copy.update({
                        'primary_anomaly': 'young',
                        'primary_anomaly_desc': f'å¹´è½»å æ¯”ä¸Šå‡{young_change:.1f}pp',
                        'anomaly_change': young_change
                    })
                    anomaly_by_metric['young'].append(account_copy)
            
            # ä¸‰çº¿åŸå¸‚å æ¯”æ£€æµ‹
            if 'third_tier_city' in quality_metrics_to_check:
                third_tier_today = today_row.get('third_tier_city_rate', 0) if pd.notna(today_row.get('third_tier_city_rate', 0)) else 0
                third_tier_7d_avg = avg_7d_row.get('third_tier_city_rate_avg', 0) if pd.notna(avg_7d_row.get('third_tier_city_rate_avg', 0)) else 0
                third_tier_change = third_tier_today - third_tier_7d_avg
                if third_tier_change > 1:  # ä¸‰çº¿åŸå¸‚å æ¯”ä¸Šå‡>1%
                    account_copy = base_account.copy()
                    account_copy.update({
                        'primary_anomaly': 'third_tier_city',
                        'primary_anomaly_desc': f'ä¸‰çº¿åŸå¸‚å æ¯”ä¸Šå‡{third_tier_change:.1f}pp',
                        'anomaly_change': third_tier_change
                    })
                    anomaly_by_metric['third_tier_city'].append(account_copy)
        
        # æŒ‰Goodä¸”è®¤è¯ç”¨æˆ·æ•°é™åºæ’åºæ¯ä¸ªç±»åˆ«
        for metric in ['white_collar', 'young', 'third_tier_city']:
            anomaly_by_metric[metric].sort(key=lambda x: x['good_verified'], reverse=True)
        
        return anomaly_by_metric
    
    def _calculate_overall_quality_changes(self, today_df: pd.DataFrame, avg_7d_df: pd.DataFrame) -> dict:
        """è®¡ç®—æ•´ä½“è´¨é‡æŒ‡æ ‡å˜åŒ–"""
        # è®¡ç®—å½“æ—¥æ•´ä½“æŒ‡æ ‡
        today_total_users = today_df['good_verified'].sum()
        today_total_revenue = (today_df['arpu'] * today_df['good_verified']).sum()
        today_total_white_collar = (today_df.get('white_collar_rate', 0) * today_df['good_verified'] / 100).sum()
        today_total_young = (today_df.get('under_23_rate', 0) * today_df['good_verified'] / 100).sum()
        today_total_third_tier = (today_df.get('third_tier_city_rate', 0) * today_df['good_verified'] / 100).sum()
        
        # è®¡ç®—7æ—¥å‡å€¼æ•´ä½“æŒ‡æ ‡
        avg_7d_total_users = avg_7d_df['good_verified_avg'].sum()
        avg_7d_total_revenue = (avg_7d_df['arpu_avg'] * avg_7d_df['good_verified_avg']).sum()
        avg_7d_total_white_collar = (avg_7d_df.get('white_collar_rate_avg', 0) * avg_7d_df['good_verified_avg'] / 100).sum()
        avg_7d_total_young = (avg_7d_df.get('under_23_rate_avg', 0) * avg_7d_df['good_verified_avg'] / 100).sum()
        avg_7d_total_third_tier = (avg_7d_df.get('third_tier_city_rate_avg', 0) * avg_7d_df['good_verified_avg'] / 100).sum()
        
        # è®¡ç®—å˜åŒ–ç™¾åˆ†æ¯”
        arpu_today = today_total_revenue / today_total_users if today_total_users > 0 else 0
        arpu_7d_avg = avg_7d_total_revenue / avg_7d_total_users if avg_7d_total_users > 0 else 0
        arpu_change = arpu_today - arpu_7d_avg
        
        white_collar_today = today_total_white_collar / today_total_users * 100 if today_total_users > 0 else 0
        white_collar_7d_avg = avg_7d_total_white_collar / avg_7d_total_users * 100 if avg_7d_total_users > 0 else 0
        white_collar_change = white_collar_today - white_collar_7d_avg
        
        young_today = today_total_young / today_total_users * 100 if today_total_users > 0 else 0
        young_7d_avg = avg_7d_total_young / avg_7d_total_users * 100 if avg_7d_total_users > 0 else 0
        young_change = young_today - young_7d_avg
        
        third_tier_today = today_total_third_tier / today_total_users * 100 if today_total_users > 0 else 0
        third_tier_7d_avg = avg_7d_total_third_tier / avg_7d_total_users * 100 if avg_7d_total_users > 0 else 0
        third_tier_city_change = third_tier_today - third_tier_7d_avg
        
        return {
            'arpu_change': arpu_change,
            'white_collar_change': white_collar_change,
            'young_change': young_change,
            'third_tier_city_change': third_tier_city_change
        }
    
    def _calculate_age_arpu_correlation(self, today_df: pd.DataFrame, yesterday_df: pd.DataFrame) -> dict:
        """è®¡ç®—å¹´é¾„å æ¯”ä¸ARPUå˜åŒ–çš„ç›¸å…³æ€§"""
        if today_df.empty or yesterday_df.empty:
            return {'under_20_correlation': 0, 'under_23_correlation': 0}
        
        # åˆ›å»ºæ˜¨æ—¥æ•°æ®å­—å…¸
        yesterday_dict = {}
        for _, row in yesterday_df.iterrows():
            yesterday_dict[row['account']] = row
        
        # æ”¶é›†å˜åŒ–æ•°æ®
        arpu_changes = []
        under_20_changes = []
        under_23_changes = []
        
        for _, today_row in today_df.iterrows():
            account = today_row['account']
            if account not in yesterday_dict:
                continue
                
            yesterday_row = yesterday_dict[account]
            
            # è®¡ç®—å˜åŒ–é‡
            arpu_change = (today_row['arpu'] or 0) - (yesterday_row['arpu'] or 0)
            under_20_change = (today_row['under_20_rate'] or 0) - (yesterday_row['under_20_rate'] or 0)
            under_23_change = (today_row['under_23_rate'] or 0) - (yesterday_row['under_23_rate'] or 0)
            
            # è¿‡æ»¤æ‰æ— æ•ˆæ•°æ®
            if abs(arpu_change) > 0.01:  # ARPUå˜åŒ–è‡³å°‘0.01å…ƒ
                arpu_changes.append(arpu_change)
                under_20_changes.append(under_20_change)
                under_23_changes.append(under_23_change)
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        correlation_result = {'under_20_correlation': 0, 'under_23_correlation': 0}
        
        if len(arpu_changes) >= 3:  # è‡³å°‘3ä¸ªæ ·æœ¬æ‰è®¡ç®—ç›¸å…³æ€§
            try:
                import numpy as np
                
                # è®¡ç®—20å²ä»¥ä¸‹å æ¯”ä¸ARPUå˜åŒ–çš„ç›¸å…³æ€§
                if len(set(under_20_changes)) > 1:  # ç¡®ä¿ä¸æ˜¯æ‰€æœ‰å€¼éƒ½ç›¸åŒ
                    correlation_result['under_20_correlation'] = np.corrcoef(arpu_changes, under_20_changes)[0, 1]
                    if np.isnan(correlation_result['under_20_correlation']):
                        correlation_result['under_20_correlation'] = 0
                
                # è®¡ç®—23å²ä»¥ä¸‹å æ¯”ä¸ARPUå˜åŒ–çš„ç›¸å…³æ€§
                if len(set(under_23_changes)) > 1:
                    correlation_result['under_23_correlation'] = np.corrcoef(arpu_changes, under_23_changes)[0, 1]
                    if np.isnan(correlation_result['under_23_correlation']):
                        correlation_result['under_23_correlation'] = 0
                        
            except Exception as e:
                print(f"âš ï¸ ç›¸å…³æ€§è®¡ç®—å¤±è´¥: {e}")
        
        return correlation_result
    
    def _generate_account_analysis_html(self, account_data: dict, date_str: str = None, conn=None) -> str:
        """ç”Ÿæˆè´¦æˆ·åˆ†æHTMLéƒ¨åˆ†"""
        if not account_data or account_data.get('error'):
            return """
        <!-- ç¬¬ä¸‰éƒ¨åˆ†ï¼šè´¦æˆ·åˆ†æ -->
        <div class="module">
            <h2>ğŸ“Š æ¨¡å—ä¸‰ï¼šè´¦æˆ·æ•°æ®ç›‘æµ‹</h2>
            <div class="anomaly-item">
                <strong>âš ï¸ è´¦æˆ·æ•°æ®ä¸å¯ç”¨</strong><br>
                """ + account_data.get('error', 'æ²¡æœ‰è´¦æˆ·æ•°æ®') + """
            </div>
        </div>
        """
        
        anomaly_accounts = account_data.get('anomaly_accounts', [])
        correlation_analysis = account_data.get('correlation_analysis', {})
        channel = account_data.get('channel', 'Douyin')
        analysis_date = account_data.get('analysis_date', '')
        
        # è®¡ç®—å æ¯”æ•°æ®
        channel_total = 0
        attribute_totals = {}
        
        if anomaly_accounts and conn is not None and date_str:
            try:
                # æŸ¥è¯¢å½“æ—¥æ‰€æœ‰è´¦æˆ·æ•°æ® - ä½¿ç”¨ä¸è´¦æˆ·åˆ†æç›¸åŒçš„è¡¨å’Œé€»è¾‘
                sql = f"""
                SELECT 
                    account,
                    CASE WHEN account_name LIKE '%å®‰å“%' THEN 'Android'
                         WHEN account_name LIKE '%iOS%' OR account_name LIKE '%ios%' OR account_name LIKE '%IOS%' THEN 'iOS'
                         ELSE 'other' END AS os,
                    CASE WHEN account_name LIKE '%ç”·%' AND account_name NOT LIKE '%ç”·å¥³%' THEN 'ç”·'
                         WHEN account_name LIKE '%å¥³%' AND account_name NOT LIKE '%ç”·å¥³%' THEN 'å¥³'
                         ELSE 'other' END AS gender,
                    SUM(total_good_verified) as good_verified
                FROM dwd_ttx_market_cash_cost_i_d_test 
                WHERE dt = '{date_str}' AND channel = '{channel}' AND account IS NOT NULL AND account != ''
                GROUP BY account, account_name
                """
                
                all_accounts_df = pd.read_sql_query(sql, conn)
                channel_total = all_accounts_df['good_verified'].sum()
                
                # è®¡ç®—å„å±æ€§ç»„åˆçš„æ€»æ•°
                for _, row in all_accounts_df.iterrows():
                    os_type = row['os']
                    gender = row['gender']
                    key = f"{os_type}_{gender}"
                    if key not in attribute_totals:
                        attribute_totals[key] = 0
                    attribute_totals[key] += row['good_verified']
                    
            except Exception as e:
                print(f"å æ¯”è®¡ç®—å¤±è´¥: {e}")
        
        html = f"""
        <!-- ç¬¬ä¸‰éƒ¨åˆ†ï¼šè´¦æˆ·åˆ†æ -->
        <div class="module">
            <h2>ğŸ“Š æ¨¡å—ä¸‰ï¼šè´¦æˆ·æ•°æ®ç›‘æµ‹ - {channel}</h2>
            
            <div style="display: flex; justify-content: space-between; align-items: flex-end; margin-bottom: 15px;">
                <h3 style="margin: 0;">ğŸ” ARPUé™ä½ä¸”ç”¨æˆ·è´¨é‡è´Ÿå‘å˜åŒ–çš„è´¦æˆ·</h3>
                
                <!-- è´¦æˆ·å±æ€§ç­›é€‰å™¨ -->
                <div style="background: #f8f9fa; padding: 10px; border-radius: 6px; border: 1px solid #dee2e6; font-size: 12px;">
                    <span style="font-weight: bold; margin-right: 10px; color: #495057;">è´¦æˆ·å±æ€§:</span>
                    <label style="margin-right: 5px;">OS:</label>
                    <select id="osFilter" style="padding: 3px 5px; border: 1px solid #ced4da; border-radius: 3px; font-size: 11px; margin-right: 10px;">
                        <option value="all">å…¨éƒ¨</option>
                        <option value="Android">Android</option>
                        <option value="iOS">iOS</option>
                        <option value="other">å…¶ä»–</option>
                    </select>
                    <label style="margin-right: 5px;">SEX:</label>
                    <select id="genderFilter" style="padding: 3px 5px; border: 1px solid #ced4da; border-radius: 3px; font-size: 11px; margin-right: 8px;">
                        <option value="all">å…¨éƒ¨</option>
                        <option value="ç”·">ç”·</option>
                        <option value="å¥³">å¥³</option>
                        <option value="other">å…¶ä»–</option>
                    </select>
                    <button onclick="filterAccounts()" style="padding: 4px 8px; background: #007bff; color: white; border: none; border-radius: 3px; cursor: pointer; font-size: 11px; margin-right: 4px;">ç­›é€‰</button>
                    <button onclick="resetFilters()" style="padding: 4px 8px; background: #6c757d; color: white; border: none; border-radius: 3px; cursor: pointer; font-size: 11px;">é‡ç½®</button>
                </div>
            </div>"""
            
        # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸æ•°æ®
        has_anomalies = (
            anomaly_accounts.get('white_collar', []) or 
            anomaly_accounts.get('young', []) or 
            anomaly_accounts.get('third_tier_city', [])
        ) if isinstance(anomaly_accounts, dict) else bool(anomaly_accounts)
            
        if has_anomalies:
            # æ·»åŠ å¼‚å¸¸è´¦æˆ·æ€»ç»“
            if isinstance(anomaly_accounts, dict):
                white_collar_accounts = anomaly_accounts.get('white_collar', [])
                young_accounts = anomaly_accounts.get('young', [])
                third_tier_accounts = anomaly_accounts.get('third_tier_city', [])
                
                white_collar_count = len(white_collar_accounts)
                young_count = len(young_accounts)
                third_tier_count = len(third_tier_accounts)
                total_anomaly_count = white_collar_count + young_count + third_tier_count
                
                # æ£€æµ‹é‡å¤è´¦æˆ·å’Œå¤šæŒ‡æ ‡å¼‚å¸¸
                all_accounts = {}
                
                # ç»Ÿè®¡æ¯ä¸ªè´¦æˆ·å‡ºç°åœ¨å“ªäº›å¼‚å¸¸ç±»å‹ä¸­
                for account in white_collar_accounts:
                    account_id = account['account']
                    if account_id not in all_accounts:
                        all_accounts[account_id] = {'account': account, 'anomaly_types': []}
                    all_accounts[account_id]['anomaly_types'].append('ç™½é¢†å æ¯”é™ä½')
                
                for account in young_accounts:
                    account_id = account['account']
                    if account_id not in all_accounts:
                        all_accounts[account_id] = {'account': account, 'anomaly_types': []}
                    all_accounts[account_id]['anomaly_types'].append('å¹´è½»å æ¯”å‡é«˜')
                
                for account in third_tier_accounts:
                    account_id = account['account']
                    if account_id not in all_accounts:
                        all_accounts[account_id] = {'account': account, 'anomaly_types': []}
                    all_accounts[account_id]['anomaly_types'].append('ä¸‰çº¿åŸå¸‚å æ¯”å‡é«˜')
                
                # è®¡ç®—å®é™…å”¯ä¸€è´¦æˆ·æ•°å’Œå¤šæŒ‡æ ‡å¼‚å¸¸è´¦æˆ·
                unique_account_count = len(all_accounts)
                multi_anomaly_accounts = [acc for acc in all_accounts.values() if len(acc['anomaly_types']) > 1]
                multi_anomaly_count = len(multi_anomaly_accounts)
                
                html += f"""
            <div style="background: #fff8e1; padding: 15px; border-radius: 8px; margin: 15px 0; border-left: 4px solid #ff9800;">
                <h4 style="margin: 0 0 10px 0; color: #e65100;">ğŸ“Š å¼‚å¸¸è´¦æˆ·æ¦‚è§ˆ</h4>
                <div style="display: flex; gap: 30px; align-items: center; flex-wrap: wrap; font-size: 14px; margin-bottom: 10px;">
                    <div><strong>å¼‚å¸¸è´¦æˆ·æ•°:</strong> {total_anomaly_count}ä¸ª</div>
                    <div style="color: #d32f2f;"><strong>å¤šæŒ‡æ ‡å¼‚å¸¸:</strong> {multi_anomaly_count}ä¸ª</div>
                </div>
                <div style="display: flex; gap: 30px; align-items: center; flex-wrap: wrap; font-size: 14px; margin-bottom: 10px;">
                    <div style="color: #e74c3c;"><strong>ç™½é¢†å æ¯”é™ä½:</strong> {white_collar_count}ä¸ª</div>
                    <div style="color: #f39c12;"><strong>å¹´è½»å æ¯”å‡é«˜:</strong> {young_count}ä¸ª</div>
                    <div style="color: #9b59b6;"><strong>ä¸‰çº¿åŸå¸‚å æ¯”å‡é«˜:</strong> {third_tier_count}ä¸ª</div>
                </div>"""
                
                # å¦‚æœæœ‰å¤šæŒ‡æ ‡å¼‚å¸¸è´¦æˆ·ï¼Œæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                if multi_anomaly_count > 0:
                    html += """
                <div style="background: #ffebee; padding: 10px; border-radius: 6px; margin-top: 10px; border-left: 3px solid #d32f2f;">
                    <strong style="color: #d32f2f;">ğŸš¨ å¤šæŒ‡æ ‡å¼‚å¸¸è´¦æˆ·è¯¦æƒ…:</strong><br>"""
                    
                    for acc_data in multi_anomaly_accounts:  # æ˜¾ç¤ºæ‰€æœ‰å¤šæŒ‡æ ‡å¼‚å¸¸è´¦æˆ·
                        account_id = acc_data['account']['account']
                        anomaly_count = len(acc_data['anomaly_types'])
                        
                        # è´¦æˆ·IDç®€åŒ–æ˜¾ç¤º
                        display_account_id = str(account_id)[:16] + ('...' if len(str(account_id)) > 16 else '')
                        
                        html += f"""
                    <span style="color: #666; font-size: 13px;">â€¢ <strong>{display_account_id}</strong> ({anomaly_count}ä¸ªæŒ‡æ ‡å¼‚å¸¸)</span><br>"""
                    
                    html += "</div>"
                
                html += "</div>"
                
                # ä½¿ç”¨æ–°çš„åˆ†ç±»æ˜¾ç¤ºæ ¼å¼
                # ä¸‰åˆ—å¸ƒå±€ï¼šæŒ‰æŒ‡æ ‡åˆ†ç±»æ˜¾ç¤º
                html += """
            <div style="margin: 20px 0; display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 20px;">"""
                
                # ç¬¬ä¸€åˆ—ï¼šç™½é¢†å æ¯”å¼‚å¸¸
                white_collar_accounts = anomaly_accounts.get('white_collar', [])
                html += f"""
                <div style="background: #fff; border: 1px solid #e74c3c; border-radius: 8px; padding: 16px;">
                    <h4 style="margin: 0 0 15px 0; color: #e74c3c;">ğŸ¢ ç™½é¢†å æ¯”é™ä½è´¦æˆ· ({len(white_collar_accounts)}ä¸ª)</h4>"""
                
                for account in white_collar_accounts:  # æ˜¾ç¤ºæ‰€æœ‰å¼‚å¸¸è´¦æˆ·
                    html += self._format_account_card(account, attribute_totals, channel_total, 'white_collar')
                
                html += """
                </div>"""
                
                # ç¬¬äºŒåˆ—ï¼šå¹´è½»å æ¯”å¼‚å¸¸
                young_accounts = anomaly_accounts.get('young', [])
                html += f"""
                <div style="background: #fff; border: 1px solid #f39c12; border-radius: 8px; padding: 16px;">
                    <h4 style="margin: 0 0 15px 0; color: #f39c12;">ğŸ‘¶ å¹´è½»å æ¯”å‡é«˜è´¦æˆ· ({len(young_accounts)}ä¸ª)</h4>"""
                
                for account in young_accounts:  # æ˜¾ç¤ºæ‰€æœ‰å¼‚å¸¸è´¦æˆ·
                    html += self._format_account_card(account, attribute_totals, channel_total, 'young')
                
                html += """
                </div>"""
                
                # ç¬¬ä¸‰åˆ—ï¼šä¸‰çº¿åŸå¸‚å æ¯”å¼‚å¸¸
                third_tier_accounts = anomaly_accounts.get('third_tier_city', [])
                html += f"""
                <div style="background: #fff; border: 1px solid #9b59b6; border-radius: 8px; padding: 16px;">
                    <h4 style="margin: 0 0 15px 0; color: #9b59b6;">ğŸ™ï¸ ä¸‰çº¿åŸå¸‚å æ¯”å‡é«˜è´¦æˆ· ({len(third_tier_accounts)}ä¸ª)</h4>"""
                
                for account in third_tier_accounts:  # æ˜¾ç¤ºæ‰€æœ‰å¼‚å¸¸è´¦æˆ·
                    html += self._format_account_card(account, attribute_totals, channel_total, 'third_tier_city')
                
                html += """
                </div>
            </div>"""
                
            else:
                # æ—§æ ¼å¼å…¼å®¹ - æ·»åŠ æ€»ç»“
                html += f"""
            <div style="background: #fff8e1; padding: 15px; border-radius: 8px; margin: 15px 0; border-left: 4px solid #ff9800;">
                <h4 style="margin: 0 0 10px 0; color: #e65100;">ğŸ“Š å¼‚å¸¸è´¦æˆ·æ¦‚è§ˆ</h4>
                <div style="font-size: 14px;">
                    <div><strong>æ€»è®¡:</strong> {len(anomaly_accounts)}ä¸ªå¼‚å¸¸è´¦æˆ·</div>
                </div>
                <p style="margin: 10px 0 0 0; color: #666; font-size: 13px; font-style: italic;">
                    ğŸ’¡ ä»¥ä¸‹è´¦æˆ·åœ¨ARPUé™ä½çš„åŒæ—¶ä¼´éšç”¨æˆ·è´¨é‡è´Ÿå‘å˜åŒ–ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨æŠ•æ”¾ç­–ç•¥å’Œç´ æä¼˜åŒ–
                </p>
            </div>"""
                
                for account in anomaly_accounts:  # æ˜¾ç¤ºæ‰€æœ‰å¼‚å¸¸è´¦æˆ·
                    html += self._format_account_card(account, attribute_totals, channel_total, 'legacy')
        
        else:
            html += """
            <div class="anomaly-item">
                <strong>âœ… æœªæ£€æµ‹åˆ°å¼‚å¸¸è´¦æˆ·</strong><br>
                æ•´ä½“æŒ‡æ ‡å˜åŒ–æœªè¶…è¿‡1%é˜ˆå€¼ï¼Œæˆ–æ²¡æœ‰å‘ç°ARPUé™ä½ä¸”ç”¨æˆ·è´¨é‡æŒ‡æ ‡è´Ÿå‘å˜åŒ–çš„è´¦æˆ·
            </div>"""
        
        html += """
        </div>"""
        
        return html
    
    def _calculate_overall_quality_changes(self, today_df: pd.DataFrame, avg_7d_df: pd.DataFrame) -> dict:
        """è®¡ç®—æ•´ä½“è´¨é‡æŒ‡æ ‡å˜åŒ–"""
        # è®¡ç®—å½“æ—¥æ•´ä½“æŒ‡æ ‡
        today_total_users = today_df['good_verified'].sum()
        today_total_revenue = (today_df['arpu'] * today_df['good_verified']).sum()
        
        # å®‰å…¨è·å–æŒ‡æ ‡åˆ—ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å¡«å……0
        white_collar_col = today_df['white_collar_rate'] if 'white_collar_rate' in today_df.columns else 0
        under_23_col = today_df['under_23_rate'] if 'under_23_rate' in today_df.columns else 0  
        third_tier_col = today_df['third_tier_city_rate'] if 'third_tier_city_rate' in today_df.columns else 0
        
        today_total_white_collar = (white_collar_col * today_df['good_verified'] / 100).sum() if hasattr(white_collar_col, 'sum') else 0
        today_total_young = (under_23_col * today_df['good_verified'] / 100).sum() if hasattr(under_23_col, 'sum') else 0
        today_total_third_tier = (third_tier_col * today_df['good_verified'] / 100).sum() if hasattr(third_tier_col, 'sum') else 0
        
        # è®¡ç®—7æ—¥å‡å€¼æ•´ä½“æŒ‡æ ‡
        avg_7d_total_users = avg_7d_df['good_verified_avg'].sum()
        avg_7d_total_revenue = (avg_7d_df['arpu_avg'] * avg_7d_df['good_verified_avg']).sum()
        
        # å®‰å…¨è·å–7æ—¥å‡å€¼æŒ‡æ ‡åˆ—
        white_collar_avg_col = avg_7d_df['white_collar_rate_avg'] if 'white_collar_rate_avg' in avg_7d_df.columns else 0
        under_23_avg_col = avg_7d_df['under_23_rate_avg'] if 'under_23_rate_avg' in avg_7d_df.columns else 0
        third_tier_avg_col = avg_7d_df['third_tier_city_rate_avg'] if 'third_tier_city_rate_avg' in avg_7d_df.columns else 0
        
        avg_7d_total_white_collar = (white_collar_avg_col * avg_7d_df['good_verified_avg'] / 100).sum() if hasattr(white_collar_avg_col, 'sum') else 0
        avg_7d_total_young = (under_23_avg_col * avg_7d_df['good_verified_avg'] / 100).sum() if hasattr(under_23_avg_col, 'sum') else 0
        avg_7d_total_third_tier = (third_tier_avg_col * avg_7d_df['good_verified_avg'] / 100).sum() if hasattr(third_tier_avg_col, 'sum') else 0
        
        # è®¡ç®—å˜åŒ–ç™¾åˆ†æ¯”
        arpu_today = today_total_revenue / today_total_users if today_total_users > 0 else 0
        arpu_7d_avg = avg_7d_total_revenue / avg_7d_total_users if avg_7d_total_users > 0 else 0
        arpu_change = arpu_today - arpu_7d_avg
        
        white_collar_today = today_total_white_collar / today_total_users * 100 if today_total_users > 0 else 0
        white_collar_7d_avg = avg_7d_total_white_collar / avg_7d_total_users * 100 if avg_7d_total_users > 0 else 0
        white_collar_change = white_collar_today - white_collar_7d_avg
        
        young_today = today_total_young / today_total_users * 100 if today_total_users > 0 else 0
        young_7d_avg = avg_7d_total_young / avg_7d_total_users * 100 if avg_7d_total_users > 0 else 0
        young_change = young_today - young_7d_avg
        
        third_tier_today = today_total_third_tier / today_total_users * 100 if today_total_users > 0 else 0
        third_tier_7d_avg = avg_7d_total_third_tier / avg_7d_total_users * 100 if avg_7d_total_users > 0 else 0
        third_tier_city_change = third_tier_today - third_tier_7d_avg
        
        return {
            'arpu_change': arpu_change,
            'white_collar_change': white_collar_change,
            'young_change': young_change,
            'third_tier_city_change': third_tier_city_change
        }
    
    def _format_account_card(self, account: dict, attribute_totals: dict, channel_total: int, metric_type: str) -> str:
        """æ ¼å¼åŒ–å•ä¸ªè´¦æˆ·å¡ç‰‡"""
        account_id = account['account']
        account_name = account.get('account_name', 'Unknown')
        os_type = account.get('os', 'other')
        gender = account.get('gender', 'other')
        good_verified = account['good_verified']
        
        # å½“æ—¥æ•°æ®
        arpu_today = account['arpu_today']
        white_collar_today = account.get('white_collar_rate_today', 0)
        under_23_today = account.get('under_23_rate_today', 0)
        third_tier_today = account.get('third_tier_city_rate_today', 0)
        
        # 7æ—¥å‡å€¼æ•°æ®
        arpu_7d_avg = account['arpu_7d_avg']
        white_collar_7d_avg = account.get('white_collar_rate_7d_avg', 0)
        under_23_7d_avg = account.get('under_23_rate_7d_avg', 0)
        third_tier_7d_avg = account.get('third_tier_city_rate_7d_avg', 0)
        
        # æ ¼å¼åŒ–å±æ€§æ˜¾ç¤º
        if os_type == 'other' and gender == 'other':
            attributes = "æœªçŸ¥å±æ€§"
        elif os_type == 'other':
            attributes = gender
        elif gender == 'other':
            attributes = os_type
        else:
            attributes = f"{os_type} {gender}"
        
        # è®¡ç®—å æ¯”
        attribute_key = f"{os_type}_{gender}"
        same_attribute_total = attribute_totals.get(attribute_key, 0)
        attribute_percentage = (good_verified / same_attribute_total * 100) if same_attribute_total > 0 else 0
        channel_percentage = (good_verified / channel_total * 100) if channel_total > 0 else 0
        
        # æ ¹æ®æŒ‡æ ‡ç±»å‹æ˜¾ç¤ºä¸åŒçš„é‡ç‚¹ä¿¡æ¯
        if metric_type == 'white_collar':
            focus_metric = f"ç™½é¢†å æ¯”: {white_collar_today:.1f}% â†’ {white_collar_7d_avg:.1f}% (â†“{abs(white_collar_today - white_collar_7d_avg):.1f}pp)"
        elif metric_type == 'young':
            focus_metric = f"å¹´è½»å æ¯”: {under_23_today:.1f}% â†’ {under_23_7d_avg:.1f}% (â†‘{abs(under_23_today - under_23_7d_avg):.1f}pp)"
        elif metric_type == 'third_tier_city':
            focus_metric = f"ä¸‰çº¿åŸå¸‚: {third_tier_today:.1f}% â†’ {third_tier_7d_avg:.1f}% (â†‘{abs(third_tier_today - third_tier_7d_avg):.1f}pp)"
        else:
            # legacyæ ¼å¼æˆ–å…¶ä»–
            focus_metric = f"ARPU: Â¥{arpu_today:.2f} â†’ Â¥{arpu_7d_avg:.2f} (Â¥{arpu_today - arpu_7d_avg:.2f})"
        
        html = f"""
        <div class="account-card" data-os="{os_type}" data-gender="{gender}" 
             style="background: #f8f9fa; border: 1px solid #dee2e6; border-radius: 6px; 
                    padding: 12px; margin: 10px 0; font-size: 12px;">
            <div style="margin-bottom: 6px;">
                <strong>{account_id}</strong><br>
                <span style="color: #666;">è´¦æˆ·å±æ€§: {attributes}</span>
            </div>
            
            <div style="margin-bottom: 6px;">
                <strong>Goodä¸”è®¤è¯:</strong> {good_verified:,}äºº<br>
                <span style="color: #666; font-size: 11px;">æ¸ é“å†…å æ¯”: {channel_percentage:.1f}% | åŒå±æ€§å æ¯”: {attribute_percentage:.1f}%</span>
            </div>
            
            <div style="background: #e9ecef; padding: 8px; border-radius: 4px; font-size: 11px;">
                {focus_metric}<br><br>
                
                <strong>ARPU:</strong> Â¥{arpu_today:.2f} â†’ Â¥{arpu_7d_avg:.2f} (Â¥{arpu_today - arpu_7d_avg:.2f})
            </div>
        </div>"""
        
        return html
    
    def _generate_creative_analysis_html(self, creative_data: dict) -> str:
        """ç”Ÿæˆç´ æåˆ†æHTMLéƒ¨åˆ†"""
        if not creative_data or creative_data.get('error'):
            return """
        <!-- ç¬¬å››éƒ¨åˆ†ï¼šç´ æåˆ†æ -->
        <div class="module">
            <h2>ğŸ“Š æ¨¡å—å››ï¼šç´ æåˆ†æ</h2>
            <div class="anomaly-item">
                <strong>âš ï¸ ç´ ææ•°æ®ä¸å¯ç”¨</strong><br>
                """ + creative_data.get('error', 'æ²¡æœ‰ç´ ææ•°æ®') + """
            </div>
        </div>
        """
        
        rankings = creative_data.get('rankings', [])
        insights = creative_data.get('insights', {})
        
        html = """
        <!-- ç¬¬å››éƒ¨åˆ†ï¼šç´ æåˆ†æ -->
        <div class="module">
            <h2>ğŸ“Š æ¨¡å—å››ï¼šç´ æåˆ†æ</h2>
            
            <h3>ğŸ† Top 20ç´ ææ’è¡Œæ¦œ</h3>"""
        
        if rankings:
            html += """
            <div style="overflow-x: auto;">
                <table style="width: 100%; border-collapse: collapse; margin: 20px 0; font-size: 12px;">
                    <thead>
                        <tr style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white;">
                            <th style="padding: 10px 8px; text-align: center; border: 1px solid #ddd;">æ’å</th>
                            <th style="padding: 10px 8px; text-align: left; border: 1px solid #ddd;">ç´ æID</th>
                            <th style="padding: 10px 8px; text-align: right; border: 1px solid #ddd;">Goodè®¤è¯</th>
                            <th style="padding: 10px 8px; text-align: right; border: 1px solid #ddd;">æ¶ˆè€—</th>
                            <th style="padding: 10px 8px; text-align: right; border: 1px solid #ddd;">CPA</th>
                            <th style="padding: 10px 8px; text-align: right; border: 1px solid #ddd;">ARPU</th>
                            <th style="padding: 10px 8px; text-align: right; border: 1px solid #ddd;">æ¬¡ç•™ç‡</th>
                            <th style="padding: 10px 8px; text-align: right; border: 1px solid #ddd;">å¥³æ¯”</th>
                            <th style="padding: 10px 8px; text-align: right; border: 1px solid #ddd;">ç™½é¢†æ¯”</th>
                            <th style="padding: 10px 8px; text-align: right; border: 1px solid #ddd;">iOSæ¯”</th>
                            <th style="padding: 10px 8px; text-align: center; border: 1px solid #ddd;">æ˜¨æ—¥æ’å</th>
                            <th style="padding: 10px 8px; text-align: center; border: 1px solid #ddd;">å˜åŒ–</th>
                        </tr>
                    </thead>
                    <tbody>"""
            
            for creative in rankings:
                rank = creative['rank']
                creative_id = creative['creative_id']  # å®Œæ•´æ˜¾ç¤ºç´ æIDï¼Œä¸æˆªæ–­
                
                # å¤„ç†æ¸ é“ä¿¡æ¯
                channels = creative.get('channels', '') or ''
                channel_count = creative.get('channel_count', 0) or 0
                if channel_count > 1:
                    channel_display = f"{channel_count}ä¸ªæ¸ é“"
                    channel_tooltip = channels
                else:
                    channel_display = channels
                    channel_tooltip = channels
                
                good_verified = creative['good_verified']
                cost = creative['cost']
                cpa = creative['cpa']
                arpu = creative['arpu']
                retention_rate = creative['retention_rate']
                female_rate = creative['female_rate']
                white_collar_rate = creative['white_collar_rate']
                ios_rate = creative['ios_rate']
                yesterday_rank = creative['yesterday_rank'] if creative['yesterday_rank'] else '-'
                rank_change = creative['rank_change']
                
                # æ ¹æ®æ’åå˜åŒ–è®¾ç½®é¢œè‰²
                change_color = "#28a745" if "â†‘" in rank_change else "#dc3545" if "â†“" in rank_change else "#6c757d"
                if "â­" in rank_change:
                    change_color = "#ffc107"
                
                row_color = "#f8f9fa" if rank % 2 == 0 else "#ffffff"
                
                html += f"""
                        <tr style="background-color: {row_color};">
                            <td style="padding: 8px; text-align: center; border: 1px solid #ddd; font-weight: bold;">{rank}</td>
                            <td style="padding: 8px; text-align: left; border: 1px solid #ddd; font-family: monospace; font-size: 10px; word-break: break-all; min-width: 150px;" title="{creative['creative_id']}">{creative_id}</td>
                            <td style="padding: 8px; text-align: right; border: 1px solid #ddd; font-weight: bold;">{good_verified:,}</td>
                            <td style="padding: 8px; text-align: right; border: 1px solid #ddd;">Â¥{cost:,.0f}</td>
                            <td style="padding: 8px; text-align: right; border: 1px solid #ddd;">Â¥{cpa:.2f}</td>
                            <td style="padding: 8px; text-align: right; border: 1px solid #ddd;">Â¥{arpu:.2f}</td>
                            <td style="padding: 8px; text-align: right; border: 1px solid #ddd;">{retention_rate:.1f}%</td>
                            <td style="padding: 8px; text-align: right; border: 1px solid #ddd;">{female_rate:.1f}%</td>
                            <td style="padding: 8px; text-align: right; border: 1px solid #ddd;">{white_collar_rate:.1f}%</td>
                            <td style="padding: 8px; text-align: right; border: 1px solid #ddd;">{ios_rate:.1f}%</td>
                            <td style="padding: 8px; text-align: center; border: 1px solid #ddd;">{yesterday_rank}</td>
                            <td style="padding: 8px; text-align: center; border: 1px solid #ddd; color: {change_color}; font-weight: bold;">{rank_change}</td>
                        </tr>"""
            
            html += """
                    </tbody>
                </table>
            </div>"""
        else:
            html += """
            <div class="anomaly-item">
                <strong>âš ï¸ æš‚æ— ç´ ææ’åæ•°æ®</strong><br>
                è¯·æ£€æŸ¥ç´ ææ•°æ®æ˜¯å¦å·²åŒæ­¥
            </div>"""
        
        # æ·»åŠ æ´å¯Ÿåˆ†æ
        html += """
            <h3>ğŸ” æ’åå˜åŒ–åˆ†æ</h3>"""
        
        if insights:
            html += '<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">'
            
            # ä¸Šå‡æœ€å¿«
            if insights.get('top_rising'):
                rising = insights['top_rising']
                html += f'''
                <div style="background: linear-gradient(135deg, #d4edda 0%, #c3e6cb 100%); border: none; border-left: 4px solid #28a745; border-radius: 8px; padding: 16px;">
                    <h4 style="color: #155724; margin: 0 0 12px 0;">ğŸ”¥ ä¸Šå‡æœ€å¿«</h4>
                    <div><strong>{rising['creative_id'][:15]}...</strong></div>
                    <div>å˜åŒ–: {rising['rank_change']} ({rising['good_verified']:,}ç”¨æˆ·)</div>
                    <div>CPA: Â¥{rising['cpa']:.2f}</div>
                </div>'''
            
            # ä¸‹é™æœ€å¿«
            if insights.get('top_falling'):
                falling = insights['top_falling']
                html += f'''
                <div style="background: linear-gradient(135deg, #f8d7da 0%, #f5c6cb 100%); border: none; border-left: 4px solid #dc3545; border-radius: 8px; padding: 16px;">
                    <h4 style="color: #721c24; margin: 0 0 12px 0;">ğŸ“‰ ä¸‹é™æœ€å¿«</h4>
                    <div><strong>{falling['creative_id'][:15]}...</strong></div>
                    <div>å˜åŒ–: {falling['rank_change']} ({falling['good_verified']:,}ç”¨æˆ·)</div>
                    <div>CPA: Â¥{falling['cpa']:.2f}</div>
                </div>'''
            
            html += '</div>'
            
            # æ–°è¿›æ¦œç´ æ
            new_entries = insights.get('new_entries', [])
            if new_entries:
                html += '''
                <div style="background: linear-gradient(135deg, #fff3cd 0%, #ffeaa7 100%); border: none; border-left: 4px solid #ffc107; border-radius: 8px; padding: 16px; margin: 20px 0;">
                    <h4 style="color: #856404; margin: 0 0 12px 0;">â­ æ–°è¿›æ¦œç´ æ</h4>'''
                for new_entry in new_entries[:3]:
                    html += f'''
                    <div style="margin-bottom: 8px;">
                        â€¢ <strong>{new_entry['creative_id'][:15]}...</strong> ({new_entry['channels'][:20]}{'...' if len(new_entry['channels']) > 20 else ''}) 
                        - {new_entry['good_verified']:,}ç”¨æˆ·, CPAÂ¥{new_entry['cpa']:.2f}
                    </div>'''
                html += '</div>'
            
            # CPAå¼‚å¸¸ç´ æ
            cpa_anomalies = insights.get('cpa_anomalies', [])
            if cpa_anomalies:
                html += '''
                <div style="background: linear-gradient(135deg, #f8d7da 0%, #f5c6cb 100%); border: none; border-left: 4px solid #dc3545; border-radius: 8px; padding: 16px; margin: 20px 0;">
                    <h4 style="color: #721c24; margin: 0 0 12px 0;">ğŸ’¸ CPAå¼‚å¸¸ç´ æ</h4>'''
                for anomaly in cpa_anomalies[:3]:
                    html += f'''
                    <div style="margin-bottom: 8px;">
                        â€¢ <strong>{anomaly['creative_id'][:15]}...</strong> ({anomaly['channels'][:20]}{'...' if len(anomaly['channels']) > 20 else ''}) 
                        - CPAÂ¥{anomaly['cpa']:.2f} (å»ºè®®å…³æ³¨)
                    </div>'''
                html += '</div>'
            
            # ç”¨æˆ·ç”»åƒä¼˜åŠ¿
            if insights.get('high_female_rate') and insights['high_female_rate']['female_rate'] > 70:
                high_female = insights['high_female_rate']
                html += f'''
                <div style="background: linear-gradient(135deg, #e2e3ff 0%, #c5ceff 100%); border: none; border-left: 4px solid #6f42c1; border-radius: 8px; padding: 16px; margin: 20px 0;">
                    <h4 style="color: #4c1d95; margin: 0 0 12px 0;">ğŸ‘© å¥³æ€§ç”¨æˆ·é›†ä¸­</h4>
                    <div><strong>{high_female['creative_id'][:15]}...</strong></div>
                    <div>å¥³æ¯”: {high_female['female_rate']:.1f}% (é€‚åˆå¥³æ€§å‘äº§å“)</div>
                </div>'''
            
            if insights.get('high_ios_rate') and insights['high_ios_rate']['ios_rate'] > 50:
                high_ios = insights['high_ios_rate']
                html += f'''
                <div style="background: linear-gradient(135deg, #e2e3ff 0%, #c5ceff 100%); border: none; border-left: 4px solid #6f42c1; border-radius: 8px; padding: 16px; margin: 20px 0;">
                    <h4 style="color: #4c1d95; margin: 0 0 12px 0;">ğŸ“± iOSç”¨æˆ·é›†ä¸­</h4>
                    <div><strong>{high_ios['creative_id'][:15]}...</strong></div>
                    <div>iOSæ¯”: {high_ios['ios_rate']:.1f}% (é«˜ç«¯ç”¨æˆ·èšé›†)</div>
                </div>'''
        
        else:
            html += """
            <div class="anomaly-item">
                <strong>âš ï¸ æš‚æ— æ’åå˜åŒ–åˆ†æ</strong><br>
                éœ€è¦è‡³å°‘ä¸¤å¤©çš„æ•°æ®è¿›è¡Œå¯¹æ¯”åˆ†æ
            </div>"""
        
        html += """
        </div>
        """
        
        return html

def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¥å£"""
    import sys
    
    if len(sys.argv) > 1:
        date = sys.argv[1]
    else:
        date = '2025-07-25'
    
    print("ğŸ“Š æ ‡å‡†åŒ–æŠ¥å‘Šç”Ÿæˆå™¨")
    print("=" * 50)
    print("âœ… å†…ç½®å®Œæ•´è§„èŒƒï¼Œç¡®ä¿æ•°æ®å‡†ç¡®æ€§")
    print("âœ… å›ºå®šæŠ¥å‘Šç»“æ„ï¼Œé¿å…éšæ„å‘æŒ¥")
    print("âœ… æ ‡å‡†åŒ–è®¡ç®—å…¬å¼ï¼Œé˜²æ­¢è®¡ç®—é”™è¯¯")
    print("=" * 50)
    
    generator = StandardReportGenerator()
    try:
        generator.generate_report(date)
        print("\nğŸ‰ æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼æ‰€æœ‰æ•°æ®å‡æŒ‰è§„èŒƒè®¡ç®—ï¼Œè¯·æ£€æŸ¥å‡†ç¡®æ€§ã€‚")
    except Exception as e:
        import traceback
        print(f"\nâŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        print("\nå®Œæ•´é”™è¯¯ä¿¡æ¯ï¼š")
        traceback.print_exc()
        sys.exit(1)

if __name__ == '__main__':
    main()
