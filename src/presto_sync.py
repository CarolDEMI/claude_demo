#!/usr/bin/env python3
"""
Prestoæ•°æ®åº“åŒæ­¥è„šæœ¬
ä»çº¿ä¸ŠPrestoæ•°æ®åº“å¯¼å…¥æ•°æ®åˆ°æœ¬åœ°SQLite
"""
import sqlite3
import pandas as pd
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import logging
import argparse
import sys

# æ·»åŠ è¿›åº¦æ˜¾ç¤ºæ”¯æŒ
try:
    from progress_bar import RealProgressBar, MultiStepProgress, progress_iterator
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„æ›¿ä»£
    class RealProgressBar:
        def __init__(self, *args, **kwargs): pass
        def update(self, *args, **kwargs): pass
        def finish(self, *args, **kwargs): pass
    
    class MultiStepProgress:
        def __init__(self, *args, **kwargs): pass
        def __enter__(self): return self
        def __exit__(self, *args): pass
        def start_step(self, *args): pass
        def complete_step(self, *args): pass
    
    def progress_iterator(iterable, description=""):
        return iterable

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å¯¼å…¥æ™ºèƒ½è¿æ¥ç®¡ç†å™¨
try:
    from .presto_connection import PrestoConnectionManager, create_smart_connection
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    import sys
    import os
    sys.path.insert(0, os.path.dirname(__file__))
    from presto_connection import PrestoConnectionManager, create_smart_connection

class PrestoSync:
    def __init__(self, local_db_path: str = "./data/data.db"):
        self.local_db_path = local_db_path
        self.presto_config = self._load_presto_config()
        self.connection_manager = create_smart_connection(self.presto_config)
        
    def _load_presto_config(self) -> Dict:
        """åŠ è½½Prestoè¿æ¥é…ç½®"""
        try:
            # ä»é…ç½®æ–‡ä»¶è¯»å–è¿æ¥ä¿¡æ¯
            import sys
            import os
            # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
            project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            sys.path.insert(0, project_root)
            from config import PRESTO_CONFIG
            
            logger.info("âœ… ä»presto_config.pyåŠ è½½é…ç½®")
            logger.info(f"ğŸ“ è¿æ¥åˆ°: {PRESTO_CONFIG['host']}:{PRESTO_CONFIG['port']}")
            return PRESTO_CONFIG
            
        except ImportError:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°presto_config.pyï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._get_default_config()
        except Exception as e:
            logger.warning(f"âš ï¸ é…ç½®åŠ è½½å¤±è´¥: {e}")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "host": "localhost",
            "port": 8080,
            "catalog": "hive",
            "schema": "da",
            "user": "admin"
        }
    
    def connect_presto(self):
        """è¿æ¥åˆ°Prestoæ•°æ®åº“ï¼ˆä½¿ç”¨æ™ºèƒ½è¿æ¥ç®¡ç†å™¨ï¼‰"""
        return self.connection_manager.connect()
    
    def execute_user_data_query(self, start_date: str = None, end_date: str = None) -> Optional[pd.DataFrame]:
        """æ‰§è¡Œç”¨æˆ·æ•°æ®æŸ¥è¯¢"""
        
        # ä»æ–‡ä»¶è¯»å–çš„åŸå§‹SQL
        base_sql = """
        select
            k1.dt,
            k1.ad_channel,
            k1.agent,
            k1.ad_account,
            k1.subchannel,
            k1.status,
            k1.verification_status,
            k1.os_type,
            k1.gender,
            k1.tag,
            k1.age_group,
            k1.dengji,
            count(distinct k1.user_id) as newuser,
            sum(k1.is_returned_1_day) is_returned_1_day,
            sum(k1.zizhu_revenue_1) as zizhu_revenue_1,
            sum(k1.zizhu_revenue_1_aftertax) as zizhu_revenue_1_aftertax
        from
            (SELECT
                t1.dt,
                t1.user_id,
                t1.ad_channel,
                t1.agent,
                t1.ad_account,
                t1.subchannel,
                t1.tag,
                t1.verification_status,
                t1.gender,
                t1.age_group,
                t1.dengji,
                t1.os_type,
                t1.status,
                t1.is_returned_1_day,
                t1.new_user_zizhu_24h_revenue,
                t1.new_user_zizhu_24h_revenue_aftertax,
                t1.cash_cost,
                sum(if(date(t1.dt)=t2.buy_dt,t2.zizhu_revenue,null)) zizhu_revenue_1,
                sum(
                    case when date(t1.dt)=t2.buy_dt and t1.os_type = 'android' then t2.zizhu_revenue*(1-0.006)
                    when date(t1.dt)=t2.buy_dt and t1.os_type = 'iOS' then t2.zizhu_revenue*(1-0.3144) else null end
                ) zizhu_revenue_1_aftertax
            FROM
                (select
                    t0.dt,
                    t0.user_id,
                    t0.ad_channel,
                    t0.agent,
                    t0.ad_account,
                    t0.subchannel,
                    t0.tag,
                    t0.verification_status,
                    t0.gender,
                    t0.age_group,
                    t0.dengji,
                    t0.os_type,
                    t0.status,
                    t0.returned_1d as is_returned_1_day,
                    t5.new_user_zizhu_24h_revenue,
                    case when t0.os_type = 'android' then t5.new_user_zizhu_24h_revenue*(1-0.006)
                        when t0.os_type = 'iOS' then t5.new_user_zizhu_24h_revenue*(1-0.3144) else null end new_user_zizhu_24h_revenue_aftertax,
                    t0.cash_cost
                from
                    (SELECT
                        dt,
                        ad_channel,
                        agent,
                        ad_account,
                        subchannel,
                        tag,
                        user_id,
                        verification_status,
                        gender,
                        CASE
                            WHEN age < 20 THEN '20-'
                            WHEN age BETWEEN 20 AND 23 THEN '20~23'
                            WHEN age BETWEEN 24 AND 30 THEN '24~30'
                            WHEN age BETWEEN 31 AND 35 THEN '31~35'
                            WHEN age BETWEEN 36 AND 40 THEN '36~40'
                            WHEN age > 40 THEN '40+'
                        END as age_group,
                        dengji,
                        os_type,
                        status,
                        returned_1d,
                        cash_cost
                    FROM da.cpz_qs_newuser_channel_i_d
                    WHERE date(dt) BETWEEN date('{start_date}') AND date('{end_date}')
                    ) t0
                    left join
                    (select
                        dt,
                        user_id,
                        COALESCE(new_user_qs_24h_revenue, 0) + COALESCE(new_user_see_24h_revenue, 0) + COALESCE(new_user_vip_24h_revenue, 0) AS new_user_zizhu_24h_revenue
                    from dwd.dwd_qs_daily_active_users_info_wide_table_a_d
                    where dt > '2023-01-01' and is_new_user = 1
                    ) t5 on t0.dt = t5.dt and t0.user_id = t5.user_id
                ) t1
                LEFT JOIN(
                    SELECT
                        date(order_created_time) as buy_dt,
                        user_id,
                        sum(case when product_type in (1,2,3,4,12,14) then price/100 else 0 end) as zizhu_revenue
                    FROM dwd.detail_ttx_order_wide_a_d
                    WHERE date(dt) BETWEEN date('{start_date}') AND date('{end_date}')
                    GROUP BY 1,2
                ) t2 ON t1.user_id=t2.user_id
            GROUP BY 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17
            ) k1
        group by 1,2,3,4,5,6,7,8,9,10,11,12
        """
        
        # è®¾ç½®é»˜è®¤æ—¥æœŸèŒƒå›´
        if not start_date:
            start_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
        if not end_date:
            end_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
        
        sql = base_sql.format(start_date=start_date, end_date=end_date)
        
        logger.info(f"ğŸ“Š æ‰§è¡ŒæŸ¥è¯¢: {start_date} åˆ° {end_date}")
        
        try:
            with self.connection_manager.get_connection() as conn:
                df = pd.read_sql(sql, conn)
                logger.info(f"âœ… æŸ¥è¯¢æˆåŠŸï¼Œè·å¾— {len(df)} æ¡è®°å½•")
                return df
                
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
            return None
    
    def import_to_local_db(self, df: pd.DataFrame, table_name: str = "cpz_qs_newuser_channel_i_d") -> bool:
        """å°†æ•°æ®å¯¼å…¥æœ¬åœ°SQLiteæ•°æ®åº“"""
        if df is None or df.empty:
            logger.warning("âš ï¸ æ²¡æœ‰æ•°æ®éœ€è¦å¯¼å…¥")
            return False
        
        # ç™½åå•éªŒè¯ - åªå…è®¸å¯¼å…¥åˆ°å·²çŸ¥çš„å®‰å…¨è¡¨
        ALLOWED_TABLES = [
            'cpz_qs_newuser_channel_i_d',
            'dwd_ttx_market_cash_cost_i_d',
            'dws_ttx_market_media_reports_i_d',
            'dwd_ttx_market_cash_cost_i_d_test'
        ]
        
        if table_name not in ALLOWED_TABLES:
            logger.error(f"âŒ ä¸å…è®¸å¯¼å…¥åˆ°è¡¨: {table_name}")
            raise ValueError(f"ä¸å…è®¸å¯¼å…¥åˆ°è¡¨: {table_name}")
        
        try:
            # æ•°æ®æ¸…ç†å’Œæ ¼å¼åŒ–
            logger.info("ğŸ§¹ æ¸…ç†å’Œæ ¼å¼åŒ–æ•°æ®...")
            
            # æ ¹æ®è¡¨ç±»å‹å¤„ç†NULLå€¼
            if table_name == "cpz_qs_newuser_channel_i_d":
                df = df.fillna({
                    'dt': '',
                    'ad_channel': '',
                    'agent': '',
                    'ad_account': '',
                    'subchannel': '',
                    'status': '',
                    'verification_status': '',
                    'os_type': '',
                    'gender': '',
                    'tag': '',
                    'age_group': '',
                    'dengji': '',
                    'newuser': 0,
                    'is_returned_1_day': 0.0,
                    'zizhu_revenue_1': 0.0,
                    'zizhu_revenue_1_aftertax': 0.0
                })
                
                # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
                df['newuser'] = df['newuser'].astype(int)
                df['is_returned_1_day'] = pd.to_numeric(df['is_returned_1_day'], errors='coerce').fillna(0.0)
                df['zizhu_revenue_1'] = pd.to_numeric(df['zizhu_revenue_1'], errors='coerce').fillna(0.0)
                df['zizhu_revenue_1_aftertax'] = pd.to_numeric(df['zizhu_revenue_1_aftertax'], errors='coerce').fillna(0.0)
                
                # ç”¨æˆ·æ•°æ®æŒ‰å®Œæ•´ä¸»é”®å»é‡
                key_cols = ['dt', 'ad_channel', 'agent', 'ad_account', 'subchannel', 'status', 'verification_status', 'os_type', 'gender', 'tag', 'age_group', 'dengji']
                df = df.drop_duplicates(subset=key_cols, keep='last')
                logger.info(f"ğŸ”§ ç”¨æˆ·æ•°æ®å»é‡åå‰©ä½™ {len(df)} æ¡è®°å½•")
                
            elif table_name == "dwd_ttx_market_cash_cost_i_d":
                df = df.fillna({
                    'dt': '',
                    'channel': '',
                    'agent': '',
                    'account': '',
                    'ad_plan_id_str': '',
                    'cash_cost': 0.0
                })
                
                # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®ï¼Œå¤„ç†è´Ÿå€¼
                df['cash_cost'] = pd.to_numeric(df['cash_cost'], errors='coerce').fillna(0.0)
                # å°†è´Ÿå€¼è½¬ä¸º0ï¼ˆç¬¦åˆCHECKçº¦æŸï¼‰
                df.loc[df['cash_cost'] < 0, 'cash_cost'] = 0.0
                
                # æˆæœ¬æ•°æ®æŒ‰å®Œæ•´ä¸»é”®å»é‡
                key_cols = ['dt', 'channel', 'agent', 'account', 'ad_plan_id_str']
                df = df.drop_duplicates(subset=key_cols, keep='last')
                logger.info(f"ğŸ”§ æˆæœ¬æ•°æ®å»é‡åå‰©ä½™ {len(df)} æ¡è®°å½•")
                
            elif table_name == "dws_ttx_market_media_reports_i_d":
                # ç´ ææ•°æ®å¤„ç†
                df = df.fillna({
                    'dt': '',
                    'channel': '',
                    'agent': '',
                    'account': '',
                    'ad_plan_id_str': '',
                    'ad_plan_name': '',
                    'ad_creative_id_str': '',
                    'media_id_str': '',
                    'total_good_verified': 0,
                    'cash_cost': 0.0,
                    'show': 0,
                    'click': 0,
                    'total': 0,
                    'total_good_verified_female': 0,
                    'total_good_verified_male': 0,
                    'total_good_verified_white': 0,
                    'total_good_verified_ios': 0,
                    'total_good_verified_return_1d': 0,
                    'total_payed_amount_good_verified': 0.0
                })
                
                # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
                for col in ['total_good_verified', 'show', 'click', 'total', 
                           'total_good_verified_female', 'total_good_verified_male',
                           'total_good_verified_white', 'total_good_verified_ios', 
                           'total_good_verified_return_1d']:
                    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)
                
                for col in ['cash_cost', 'total_payed_amount_good_verified']:
                    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0.0)
                
                # å°†è´Ÿå€¼è½¬ä¸º0ï¼ˆç¬¦åˆCHECKçº¦æŸï¼‰
                numeric_cols = ['total_good_verified', 'cash_cost', 'show', 'click', 'total',
                               'total_good_verified_female', 'total_good_verified_male',
                               'total_good_verified_white', 'total_good_verified_ios',
                               'total_good_verified_return_1d', 'total_payed_amount_good_verified']
                for col in numeric_cols:
                    df.loc[df[col] < 0, col] = 0
                
                # ç´ ææ•°æ®ä½¿ç”¨å®Œæ•´ä¸»é”®å»é‡ï¼Œä¿ç•™æ¯ä¸ªè´¦æˆ·çš„ç‹¬ç«‹è®°å½•
                key_cols = ['dt', 'channel', 'account', 'ad_plan_id_str', 'ad_creative_id_str', 'media_id_str']
                df = df.drop_duplicates(subset=key_cols, keep='last')
                logger.info(f"ğŸ”§ ç´ ææ•°æ®å»é‡åå‰©ä½™ {len(df)} æ¡è®°å½•")
            
            logger.info(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {len(df)} æ¡è®°å½•ï¼Œæ—¶é—´èŒƒå›´: {df['dt'].min()} åˆ° {df['dt'].max()}")
            
            conn = sqlite3.connect(self.local_db_path)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºè¡¨ - ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢é˜²æ­¢SQLæ³¨å…¥
            cursor = conn.cursor()
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name=?", (table_name,))
            table_exists = cursor.fetchone()
            
            if not table_exists:
                logger.info(f"ğŸ“ åˆ›å»ºè¡¨ {table_name}")
                # æ ¹æ®è¡¨ç±»å‹åˆ›å»ºä¸åŒçš„è¡¨ç»“æ„
                if table_name == "cpz_qs_newuser_channel_i_d":
                    create_sql = f"""
                    CREATE TABLE {table_name} (
                        dt TEXT,
                        ad_channel TEXT,
                        agent TEXT,
                        ad_account TEXT,
                        subchannel TEXT,
                        status TEXT,
                        verification_status TEXT,
                        os_type TEXT,
                        gender TEXT,
                        tag TEXT,
                        age_group TEXT,
                        dengji TEXT,
                        newuser INTEGER DEFAULT 0,
                        is_returned_1_day REAL DEFAULT 0.0,
                        zizhu_revenue_1 REAL DEFAULT 0.0,
                        zizhu_revenue_1_aftertax REAL DEFAULT 0.0
                    )
                    """
                elif table_name == "dwd_ttx_market_cash_cost_i_d":
                    create_sql = f"""
                    CREATE TABLE {table_name} (
                        dt TEXT,
                        channel TEXT,
                        agent TEXT,
                        account TEXT,
                        ad_plan_id_str TEXT,
                        cash_cost REAL DEFAULT 0.0
                    )
                    """
                elif table_name == "dws_ttx_market_media_reports_i_d":
                    create_sql = f"""
                    CREATE TABLE {table_name} (
                        dt TEXT NOT NULL,
                        channel TEXT,
                        agent TEXT, 
                        account TEXT,
                        ad_plan_id_str TEXT,
                        ad_plan_name TEXT,
                        ad_creative_id_str TEXT NOT NULL,
                        media_id_str TEXT,
                        total_good_verified INTEGER DEFAULT 0,
                        cash_cost REAL DEFAULT 0,
                        show INTEGER DEFAULT 0,
                        click INTEGER DEFAULT 0,
                        total INTEGER DEFAULT 0,
                        total_good_verified_female INTEGER DEFAULT 0,
                        total_good_verified_male INTEGER DEFAULT 0,
                        total_good_verified_white INTEGER DEFAULT 0,
                        total_good_verified_ios INTEGER DEFAULT 0,
                        total_good_verified_return_1d INTEGER DEFAULT 0,
                        total_payed_amount_good_verified REAL DEFAULT 0,
                        PRIMARY KEY (dt, channel, account, ad_plan_id_str, ad_creative_id_str, media_id_str)
                    )
                    """
                elif table_name == "dwd_ttx_market_cash_cost_i_d_test":
                    create_sql = f"""
                    CREATE TABLE {table_name} (
                        stat_date TEXT,
                        stat_hour INTEGER,
                        channel TEXT,
                        agent TEXT,
                        account TEXT,
                        ad_plan_id_str TEXT,
                        ad_plan_name TEXT,
                        apk TEXT,
                        api TEXT,
                        provider TEXT,
                        show INTEGER DEFAULT 0,
                        click INTEGER DEFAULT 0,
                        cash_cost REAL DEFAULT 0,
                        cost REAL DEFAULT 0,
                        convert INTEGER DEFAULT 0,
                        download_start INTEGER DEFAULT 0,
                        download_finish INTEGER DEFAULT 0,
                        install INTEGER DEFAULT 0,
                        total INTEGER DEFAULT 0,
                        total_male INTEGER DEFAULT 0,
                        total_female INTEGER DEFAULT 0,
                        total_good INTEGER DEFAULT 0,
                        total_good_male INTEGER DEFAULT 0,
                        total_good_female INTEGER DEFAULT 0,
                        total_good_verified INTEGER DEFAULT 0,
                        total_good_verified_male INTEGER DEFAULT 0,
                        total_good_verified_female INTEGER DEFAULT 0,
                        total_good_white INTEGER DEFAULT 0,
                        total_good_verified_white INTEGER DEFAULT 0,
                        total_good_young INTEGER DEFAULT 0,
                        total_good_verified_young INTEGER DEFAULT 0,
                        total_good_old INTEGER DEFAULT 0,
                        total_good_verified_old INTEGER DEFAULT 0,
                        total_good_ios INTEGER DEFAULT 0,
                        total_good_verified_ios INTEGER DEFAULT 0,
                        total_banned INTEGER DEFAULT 0,
                        total_pending INTEGER DEFAULT 0,
                        total_fake INTEGER DEFAULT 0,
                        total_payed_amount REAL DEFAULT 0,
                        total_payed_amount_male REAL DEFAULT 0,
                        total_payed_amount_female REAL DEFAULT 0,
                        total_payed INTEGER DEFAULT 0,
                        audit_durtions INTEGER DEFAULT 0,
                        call_back_total_good_famle INTEGER DEFAULT 0,
                        call_back_total_good_male INTEGER DEFAULT 0,
                        call_back_total_good_total INTEGER DEFAULT 0,
                        call_back_total_signup_total INTEGER DEFAULT 0,
                        ecpm REAL DEFAULT 0,
                        active_plan INTEGER DEFAULT 0,
                        bad_plan INTEGER DEFAULT 0,
                        activation INTEGER DEFAULT 0,
                        total_payed_amount_good REAL DEFAULT 0,
                        total_payed_amount_good_verified REAL DEFAULT 0,
                        total_payed_good INTEGER DEFAULT 0,
                        total_payed_good_verified INTEGER DEFAULT 0,
                        total_good_return_1d INTEGER DEFAULT 0,
                        total_good_verified_return_1d INTEGER DEFAULT 0,
                        hq INTEGER DEFAULT 0,
                        greater_user INTEGER DEFAULT 0,
                        greater_user_male INTEGER DEFAULT 0,
                        greater_user_female INTEGER DEFAULT 0,
                        third_line_city_user INTEGER DEFAULT 0,
                        total_good_verified_twenty INTEGER DEFAULT 0,
                        call_back_total_good_verified_total INTEGER DEFAULT 0,
                        total_payed_amount_after_tax REAL DEFAULT 0,
                        total_payed_amount_after_tax_male REAL DEFAULT 0,
                        total_payed_amount_after_tax_female REAL DEFAULT 0,
                        total_payed_amount_after_tax_good REAL DEFAULT 0,
                        total_payed_amount_after_tax_good_verified REAL DEFAULT 0,
                        account_name TEXT,
                        account_note TEXT,
                        total_good_verified_22_40 INTEGER DEFAULT 0,
                        dt TEXT
                    )
                    """
                else:
                    raise ValueError(f"Unknown table type: {table_name}")
                
                cursor.execute(create_sql)
            
            # æ¸…ç†ç›®æ ‡æ—¥æœŸçš„ç°æœ‰æ•°æ® - è¡¨åå·²éªŒè¯ï¼Œå®‰å…¨ä½¿ç”¨
            dates = df['dt'].unique()
            delete_sql = f"DELETE FROM {table_name} WHERE dt = ?"
            for date in dates:
                cursor.execute(delete_sql, [str(date)])
                logger.info(f"ğŸ—‘ï¸ æ¸…ç† {date} çš„æ—§æ•°æ®")
            
            # å¯¼å…¥æ–°æ•°æ®
            df.to_sql(table_name, conn, if_exists='append', index=False)
            
            conn.commit()
            conn.close()
            
            logger.info(f"âœ… æˆåŠŸå¯¼å…¥ {len(df)} æ¡è®°å½•åˆ°æœ¬åœ°æ•°æ®åº“")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®å¯¼å…¥å¤±è´¥: {e}")
            return False
    
    def execute_cost_data_query(self, start_date: str = None, end_date: str = None) -> Optional[pd.DataFrame]:
        """æ‰§è¡Œæˆæœ¬æ•°æ®æŸ¥è¯¢"""
        
        # æˆæœ¬æ•°æ®SQL
        cost_sql = """
        select
            dt,
            channel,
            agent,
            account,
            ad_plan_id_str,
            sum(cash_cost) cash_cost
        from dwd.dwd_ttx_market_cash_cost_i_d
        where date(dt) BETWEEN date('{start_date}') AND date('{end_date}')
        group by 1,2,3,4,5
        """
        
        # è®¾ç½®é»˜è®¤æ—¥æœŸèŒƒå›´
        if not start_date:
            start_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
        if not end_date:
            end_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
        
        sql = cost_sql.format(start_date=start_date, end_date=end_date)
        
        logger.info(f"ğŸ“Š æ‰§è¡Œæˆæœ¬æ•°æ®æŸ¥è¯¢: {start_date} åˆ° {end_date}")
        
        try:
            with self.connection_manager.get_connection() as conn:
                df = pd.read_sql(sql, conn)
                logger.info(f"âœ… æˆæœ¬æ•°æ®æŸ¥è¯¢æˆåŠŸï¼Œè·å¾— {len(df)} æ¡è®°å½•")
                return df
                
        except Exception as e:
            logger.error(f"âŒ æˆæœ¬æ•°æ®æŸ¥è¯¢å¤±è´¥: {e}")
            return None

    def execute_creative_data_query(self, start_date: str = None, end_date: str = None) -> Optional[pd.DataFrame]:
        """æ‰§è¡Œç´ ææ•°æ®æŸ¥è¯¢"""
        
        # ç´ ææ•°æ®SQL
        creative_sql = """
        select
            dt,
            channel,
            agent,
            account,
            ad_plan_id_str,
            ad_plan_name,
            ad_creative_id_str,
            media_id_str,
            sum(total_good_verified) as total_good_verified,
            sum(cash_cost) as cash_cost,
            sum(show) as show,
            sum(click) as click,
            sum(total) as total,
            sum(total_good_verified_female) as total_good_verified_female,
            sum(total_good_verified_male) as total_good_verified_male,
            sum(total_good_verified_white) as total_good_verified_white,
            sum(total_good_verified_ios) as total_good_verified_ios,
            sum(total_good_verified_return_1d) as total_good_verified_return_1d,
            sum(total_payed_amount_good_verified) as total_payed_amount_good_verified
        from dws.dws_ttx_market_media_reports_i_d
        where date(dt) BETWEEN date('{start_date}') AND date('{end_date}')
        group by 1,2,3,4,5,6,7,8
        """
        
        # è®¾ç½®é»˜è®¤æ—¥æœŸèŒƒå›´
        if not start_date:
            start_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
        if not end_date:
            end_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
        
        sql = creative_sql.format(start_date=start_date, end_date=end_date)
        
        logger.info(f"ğŸ“Š æ‰§è¡Œç´ ææ•°æ®æŸ¥è¯¢: {start_date} åˆ° {end_date}")
        
        try:
            with self.connection_manager.get_connection() as conn:
                df = pd.read_sql(sql, conn)
                logger.info(f"âœ… ç´ ææ•°æ®æŸ¥è¯¢æˆåŠŸï¼Œè·å¾— {len(df)} æ¡è®°å½•")
                return df
                
        except Exception as e:
            logger.error(f"âŒ ç´ ææ•°æ®æŸ¥è¯¢å¤±è´¥: {e}")
            return None

    def execute_test_cost_data_query(self, start_date: str = None, end_date: str = None) -> Optional[pd.DataFrame]:
        """æ‰§è¡Œæµ‹è¯•æˆæœ¬æ•°æ®æŸ¥è¯¢"""
        
        # æµ‹è¯•æˆæœ¬æ•°æ®SQL - ä»8æœˆå¼€å§‹çš„æ•°æ®
        test_cost_sql = """
        select *
        from dwd.dwd_ttx_market_cash_cost_i_d
        where date(dt) BETWEEN date('{start_date}') AND date('{end_date}')
          and date(dt) >= date('2025-08-01')
        """
        
        # è®¾ç½®é»˜è®¤æ—¥æœŸèŒƒå›´ - å¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œä»2025-08-01å¼€å§‹
        if not start_date:
            start_date = '2025-08-01'
        if not end_date:
            end_date = datetime.now().strftime('%Y-%m-%d')
        
        # ç¡®ä¿ä¸æ—©äº2025-08-01
        if start_date < '2025-08-01':
            start_date = '2025-08-01'
        
        sql = test_cost_sql.format(start_date=start_date, end_date=end_date)
        
        logger.info(f"ğŸ“Š æ‰§è¡Œæµ‹è¯•æˆæœ¬æ•°æ®æŸ¥è¯¢: {start_date} åˆ° {end_date}")
        
        try:
            with self.connection_manager.get_connection() as conn:
                df = pd.read_sql(sql, conn)
                logger.info(f"âœ… æµ‹è¯•æˆæœ¬æ•°æ®æŸ¥è¯¢æˆåŠŸï¼Œè·å¾— {len(df)} æ¡è®°å½•")
                return df
                
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•æˆæœ¬æ•°æ®æŸ¥è¯¢å¤±è´¥: {e}")
            return None

    def sync_data(self, start_date: str = None, end_date: str = None) -> bool:
        """å®Œæ•´çš„æ•°æ®åŒæ­¥æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹æ•°æ®åŒæ­¥...")
        
        # 1. ä»PrestoæŸ¥è¯¢ç”¨æˆ·æ•°æ®
        user_df = self.execute_user_data_query(start_date, end_date)
        if user_df is None:
            return False
        
        # 2. æ•°æ®æ ¡éªŒå’Œæ¸…ç†
        if not self.validate_sync_data(user_df, start_date, end_date):
            logger.error("âŒ æ•°æ®æ ¡éªŒå¤±è´¥ï¼Œç»ˆæ­¢åŒæ­¥")
            return False
        
        # 3. ä»PrestoæŸ¥è¯¢æˆæœ¬æ•°æ®
        cost_df = self.execute_cost_data_query(start_date, end_date)
        if cost_df is None:
            logger.warning("âš ï¸ æˆæœ¬æ•°æ®æŸ¥è¯¢å¤±è´¥ï¼Œä»…åŒæ­¥ç”¨æˆ·æ•°æ®")
            cost_sync_success = False
        else:
            cost_sync_success = self.import_to_local_db(cost_df, "dwd_ttx_market_cash_cost_i_d")
        
        # 4. ä»PrestoæŸ¥è¯¢ç´ ææ•°æ®
        creative_df = self.execute_creative_data_query(start_date, end_date)
        if creative_df is None:
            logger.warning("âš ï¸ ç´ ææ•°æ®æŸ¥è¯¢å¤±è´¥ï¼Œè·³è¿‡ç´ ææ•°æ®åŒæ­¥")
            creative_sync_success = False
        else:
            creative_sync_success = self.import_to_local_db(creative_df, "dws_ttx_market_media_reports_i_d")
        
        # 5. å¯¼å…¥ç”¨æˆ·æ•°æ®åˆ°æœ¬åœ°æ•°æ®åº“
        user_sync_success = self.import_to_local_db(user_df, "cpz_qs_newuser_channel_i_d")
        
        # 6. åŒæ­¥åæ ¡éªŒ
        if user_sync_success:
            self.post_sync_validation(start_date, end_date)
        
        # 7. æŠ¥å‘ŠåŒæ­¥ç»“æœ
        sync_results = []
        if user_sync_success:
            sync_results.append("âœ… ç”¨æˆ·æ•°æ®")
        if cost_sync_success:
            sync_results.append("âœ… æˆæœ¬æ•°æ®")
        if creative_sync_success:
            sync_results.append("âœ… ç´ ææ•°æ®")
            
        failed_results = []
        if not cost_sync_success:
            failed_results.append("âŒ æˆæœ¬æ•°æ®")
        if not creative_sync_success:
            failed_results.append("âŒ ç´ ææ•°æ®")
        
        if user_sync_success:
            success_msg = "åŒæ­¥å®Œæˆ: " + ", ".join(sync_results)
            if failed_results:
                success_msg += " | å¤±è´¥: " + ", ".join(failed_results)
            logger.info("ğŸ‰ " + success_msg)
        else:
            logger.error("âŒ æ•°æ®åŒæ­¥å¤±è´¥")
        
        return user_sync_success
    
    def validate_sync_data(self, df: pd.DataFrame, start_date: str, end_date: str) -> bool:
        """æ•°æ®åŒæ­¥å‰çš„æ ¡éªŒ"""
        logger.info("ğŸ” å¼€å§‹æ•°æ®æ ¡éªŒ...")
        
        if df is None or df.empty:
            logger.error("âŒ æ•°æ®ä¸ºç©º")
            return False
        
        # 1. æ£€æŸ¥æ—¥æœŸèŒƒå›´
        df_dates = pd.to_datetime(df['dt']).dt.date
        start_dt = pd.to_datetime(start_date).date()
        end_dt = pd.to_datetime(end_date).date()
        
        invalid_dates = df_dates[(df_dates < start_dt) | (df_dates > end_dt)]
        if len(invalid_dates) > 0:
            logger.warning(f"âš ï¸ å‘ç° {len(invalid_dates)} æ¡è¶…å‡ºæ—¥æœŸèŒƒå›´çš„è®°å½•")
        
        # 2. æ£€æŸ¥ARPUåˆç†æ€§ï¼ˆé¿å…å¼‚å¸¸é«˜å€¼ï¼‰
        df_with_revenue = df[df['zizhu_revenue_1'] > 0].copy()
        if not df_with_revenue.empty:
            df_with_revenue['arpu'] = df_with_revenue['zizhu_revenue_1'] / df_with_revenue['newuser']
            high_arpu = df_with_revenue[df_with_revenue['arpu'] > 500]  # ARPU > 500è§†ä¸ºå¼‚å¸¸
            
            if len(high_arpu) > 0:
                logger.warning(f"âš ï¸ å‘ç° {len(high_arpu)} æ¡ARPUå¼‚å¸¸é«˜çš„è®°å½• (>500å…ƒ)")
                logger.warning(f"æœ€é«˜ARPU: {high_arpu['arpu'].max():.2f}å…ƒ")
                
                # è®°å½•å¼‚å¸¸æ•°æ®è¯¦æƒ…
                for _, row in high_arpu.head(3).iterrows():
                    logger.warning(f"å¼‚å¸¸è®°å½•: {row['dt']} {row['ad_channel']} ARPU={row['arpu']:.2f}")
        
        # 3. æ£€æŸ¥é‡å¤è®°å½•
        key_cols = ['dt', 'ad_channel', 'agent', 'ad_account', 'subchannel', 'status', 'verification_status', 'os_type', 'gender', 'age_group']
        duplicates = df[df.duplicated(subset=key_cols, keep=False)]
        if len(duplicates) > 0:
            logger.warning(f"âš ï¸ å‘ç° {len(duplicates)} æ¡é‡å¤è®°å½•")
        
        # 4. åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        total_users = df['newuser'].sum()
        total_revenue = df['zizhu_revenue_1'].sum()
        avg_arpu = total_revenue / total_users if total_users > 0 else 0
        
        logger.info(f"ğŸ“Š æ•°æ®æ¦‚å†µ: {len(df)}æ¡è®°å½•, {total_users}ç”¨æˆ·, {total_revenue:.2f}å…ƒæ”¶å…¥, å¹³å‡ARPU={avg_arpu:.2f}å…ƒ")
        
        return True
    
    def post_sync_validation(self, start_date: str, end_date: str):
        """åŒæ­¥åçš„æ•°æ®æ ¡éªŒ"""
        logger.info("ğŸ” åŒæ­¥åæ•°æ®æ ¡éªŒ...")
        
        conn = sqlite3.connect(self.local_db_path)
        cursor = conn.cursor()
        
        try:
            # æŒ‰æ—¥æœŸç»Ÿè®¡ARPUï¼Œæ£€æŸ¥å¼‚å¸¸å€¼
            cursor.execute('''
            SELECT 
                dt,
                SUM(zizhu_revenue_1) as total_revenue,
                SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END) as good_users,
                ROUND(SUM(zizhu_revenue_1) / NULLIF(SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END), 0), 2) as arpu
            FROM cpz_qs_newuser_channel_i_d
            WHERE dt BETWEEN ? AND ?
            GROUP BY dt
            ORDER BY dt
            ''', [start_date, end_date])
            
            results = cursor.fetchall()
            
            for dt, revenue, users, arpu in results:
                logger.info(f"ğŸ“Š {dt}: æ”¶å…¥={revenue:.2f}å…ƒ, Good+verified={users}äºº, ARPU={arpu}å…ƒ")
                
                # æ£€æŸ¥å¼‚å¸¸å€¼
                if arpu and arpu > 20:
                    logger.warning(f"âš ï¸ {dt} ARPUå¼‚å¸¸é«˜: {arpu}å…ƒï¼Œè¯·æ£€æŸ¥æ•°æ®")
                elif arpu and arpu < 3:
                    logger.warning(f"âš ï¸ {dt} ARPUå¼‚å¸¸ä½: {arpu}å…ƒï¼Œè¯·æ£€æŸ¥æ•°æ®")
        
        except Exception as e:
            logger.error(f"âŒ åŒæ­¥åæ ¡éªŒå¤±è´¥: {e}")
        finally:
            conn.close()
    
    def check_missing_dates(self, start_date: str = "2025-07-01", end_date: str = None, check_retention: bool = True) -> List[str]:
        """æ£€æŸ¥æœ¬åœ°æ•°æ®åº“ä¸­ç¼ºå¤±çš„æ—¥æœŸï¼ŒåŒ…æ‹¬æ¬¡ç•™ç‡æœªæ›´æ–°çš„æ—¥æœŸ"""
        if not end_date:
            end_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
        
        logger.info(f"ğŸ” æ£€æŸ¥ {start_date} åˆ° {end_date} çš„æ•°æ®ç¼ºå¤±æƒ…å†µ...")
        
        # ç”Ÿæˆæ—¥æœŸèŒƒå›´
        start_dt = datetime.strptime(start_date, '%Y-%m-%d')
        end_dt = datetime.strptime(end_date, '%Y-%m-%d')
        
        all_dates = []
        current_dt = start_dt
        while current_dt <= end_dt:
            all_dates.append(current_dt.strftime('%Y-%m-%d'))
            current_dt += timedelta(days=1)
        
        # æŸ¥è¯¢æœ¬åœ°æ•°æ®åº“å·²æœ‰çš„æ—¥æœŸ
        conn = sqlite3.connect(self.local_db_path)
        cursor = conn.cursor()
        
        try:
            # 1. æ£€æŸ¥å®Œå…¨ç¼ºå¤±çš„æ—¥æœŸ
            cursor.execute('''
            SELECT DISTINCT dt 
            FROM cpz_qs_newuser_channel_i_d 
            WHERE dt BETWEEN ? AND ?
            ORDER BY dt
            ''', [start_date, end_date])
            
            existing_dates = [row[0] for row in cursor.fetchall()]
            missing_dates = [date for date in all_dates if date not in existing_dates]
            
            # 2. æ£€æŸ¥æ¬¡ç•™ç‡æœªæ›´æ–°çš„æ—¥æœŸï¼ˆå¦‚æœå¯ç”¨æ£€æŸ¥ï¼‰
            retention_pending_dates = []
            if check_retention:
                # æ¬¡ç•™ç‡éœ€è¦æ¬¡æ—¥æ•°æ®æ‰èƒ½è®¡ç®—ï¼Œæ‰€ä»¥æ’é™¤æœ€åä¸€å¤©
                retention_check_dates = [date for date in existing_dates if date < end_date]
                
                for date in retention_check_dates:
                    # æ£€æŸ¥è¯¥æ—¥æœŸæ˜¯å¦æœ‰æ¬¡ç•™ç‡æ•°æ®
                    cursor.execute('''
                    SELECT 
                        COUNT(*) as total_good_verified,
                        SUM(CASE WHEN is_returned_1_day > 0 THEN 1 ELSE 0 END) as has_retention
                    FROM cpz_qs_newuser_channel_i_d 
                    WHERE dt = ? AND status = 'good' AND verification_status = 'verified'
                    ''', [date])
                    
                    result = cursor.fetchone()
                    if result:
                        total_users, users_with_retention = result
                        # å¦‚æœæœ‰good+verifiedç”¨æˆ·ä½†æ¬¡ç•™ç‡ä¸º0ï¼Œè¯´æ˜éœ€è¦æ›´æ–°
                        if total_users > 0 and users_with_retention == 0:
                            retention_pending_dates.append(date)
            
            # åˆå¹¶éœ€è¦å¤„ç†çš„æ—¥æœŸ
            all_missing_dates = list(set(missing_dates + retention_pending_dates))
            all_missing_dates.sort()
            
            logger.info(f"ğŸ“Š ç»Ÿè®¡ç»“æœ:")
            logger.info(f"   æ€»å¤©æ•°: {len(all_dates)} å¤©")
            logger.info(f"   å·²æœ‰æ•°æ®: {len(existing_dates)} å¤©")
            logger.info(f"   å®Œå…¨ç¼ºå¤±: {len(missing_dates)} å¤©")
            if check_retention:
                logger.info(f"   æ¬¡ç•™å¾…æ›´æ–°: {len(retention_pending_dates)} å¤©")
            logger.info(f"   éœ€è¦åŒæ­¥: {len(all_missing_dates)} å¤©")
            
            if missing_dates:
                logger.info(f"ğŸ“… ç¼ºå¤±æ—¥æœŸ: {', '.join(missing_dates[:5])}" + 
                           (f" ... (è¿˜æœ‰{len(missing_dates)-5}å¤©)" if len(missing_dates) > 5 else ""))
            
            if retention_pending_dates:
                logger.info(f"ğŸ”„ æ¬¡ç•™å¾…æ›´æ–°: {', '.join(retention_pending_dates[:5])}" + 
                           (f" ... (è¿˜æœ‰{len(retention_pending_dates)-5}å¤©)" if len(retention_pending_dates) > 5 else ""))
            
            if not all_missing_dates:
                logger.info("âœ… æ•°æ®å®Œæ•´ï¼Œæ— éœ€åŒæ­¥")
            
            return all_missing_dates
            
        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥ç¼ºå¤±æ—¥æœŸå¤±è´¥: {e}")
            return []
        finally:
            conn.close()
    
    def update_retention_only(self, target_date: str) -> bool:
        """ä»…æ›´æ–°æ¬¡ç•™ç‡åŠŸèƒ½å·²ç¦ç”¨ - åªèƒ½åŒæ­¥çœŸå®æ•°æ®"""
        logger.warning(f"âš ï¸ æ¬¡ç•™ç‡æ›´æ–°åŠŸèƒ½å·²ç¦ç”¨ï¼Œåªèƒ½åŒæ­¥çœŸå®æ•°æ®")
        logger.info(f"ğŸ’¡ è¯·ä½¿ç”¨å®Œæ•´æ•°æ®åŒæ­¥æ¥è·å–çœŸå®çš„æ¬¡ç•™ç‡æ•°æ®")
        return False
    
    def sync_missing_dates(self, start_date: str = "2025-07-01", end_date: str = None, retention_only: bool = False) -> bool:
        """åŒæ­¥ç¼ºå¤±æ—¥æœŸçš„æ•°æ®ï¼ˆå¸¦è¿›åº¦æ˜¾ç¤ºï¼‰"""
        missing_dates = self.check_missing_dates(start_date, end_date)
        
        if not missing_dates:
            logger.info("âœ… æ— éœ€åŒæ­¥ï¼Œæ•°æ®å·²å®Œæ•´")
            return True
        
        logger.info(f"ğŸš€ å¼€å§‹å¤„ç† {len(missing_dates)} ä¸ªæ—¥æœŸçš„æ•°æ®...")
        
        # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºåŒæ­¥è¿›åº¦
        progress_bar = RealProgressBar(len(missing_dates), "åŒæ­¥æ•°æ®")
        success_count = 0
        
        for i, date in enumerate(missing_dates):
            # æ›´æ–°è¿›åº¦æ¡çŠ¶æ€
            progress_bar.update(0, f"å¤„ç† {date}")
            
            # æ£€æŸ¥è¯¥æ—¥æœŸæ˜¯å¦åªæ˜¯éœ€è¦æ›´æ–°æ¬¡ç•™ç‡
            conn = sqlite3.connect(self.local_db_path)
            cursor = conn.cursor()
            
            try:
                cursor.execute('SELECT COUNT(*) FROM cpz_qs_newuser_channel_i_d WHERE dt = ?', [date])
                has_data = cursor.fetchone()[0] > 0
                
                if has_data and retention_only:
                    # åªæ›´æ–°æ¬¡ç•™ç‡
                    progress_bar.update(0, f"æ›´æ–°ç•™å­˜ç‡ {date}")
                    if self.update_retention_only(date):
                        success_count += 1
                        progress_bar.update(1, f"âœ… {date} ç•™å­˜ç‡å®Œæˆ")
                    else:
                        progress_bar.update(1, f"âŒ {date} ç•™å­˜ç‡å¤±è´¥")
                else:
                    # å®Œæ•´åŒæ­¥æ•°æ®
                    progress_bar.update(0, f"åŒæ­¥æ•°æ® {date}")
                    if self.sync_data(date, date):
                        success_count += 1
                        progress_bar.update(1, f"âœ… {date} åŒæ­¥å®Œæˆ")
                    else:
                        progress_bar.update(1, f"âŒ {date} åŒæ­¥å¤±è´¥")
                
            except Exception as e:
                logger.error(f"âŒ å¤„ç† {date} å¤±è´¥: {e}")
                progress_bar.update(1, f"âŒ {date} é”™è¯¯")
            finally:
                conn.close()
        
        # å®Œæˆè¿›åº¦æ¡
        if success_count == len(missing_dates):
            progress_bar.finish("ğŸ‰ å…¨éƒ¨æˆåŠŸ")
        else:
            progress_bar.finish(f"âš ï¸ {success_count}/{len(missing_dates)} æˆåŠŸ")
        
        logger.info(f"ğŸ“Š å¤„ç†å®Œæˆ: {success_count}/{len(missing_dates)} ä¸ªæ—¥æœŸå¤„ç†æˆåŠŸ")
        return success_count == len(missing_dates)
    
    def get_data_summary(self, start_date: str = "2025-07-01", end_date: str = None) -> pd.DataFrame:
        """è·å–æœ¬åœ°æ•°æ®åº“çš„æ•°æ®æ¦‚å†µ"""
        if not end_date:
            end_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
        
        conn = sqlite3.connect(self.local_db_path)
        
        try:
            query = '''
            SELECT 
                dt,
                COUNT(*) as record_count,
                SUM(newuser) as total_users,
                SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END) as good_verified_users,
                SUM(zizhu_revenue_1) as total_revenue,
                ROUND(SUM(zizhu_revenue_1) / NULLIF(SUM(CASE WHEN status = 'good' AND verification_status = 'verified' THEN newuser ELSE 0 END), 0), 2) as arpu
            FROM cpz_qs_newuser_channel_i_d
            WHERE dt BETWEEN ? AND ?
            GROUP BY dt
            ORDER BY dt
            '''
            
            df = pd.read_sql(query, conn, params=[start_date, end_date])
            return df
            
        except Exception as e:
            logger.error(f"âŒ è·å–æ•°æ®æ¦‚å†µå¤±è´¥: {e}")
            return pd.DataFrame()
        finally:
            conn.close()
    
    def test_connection(self) -> bool:
        """æµ‹è¯•è¿æ¥ï¼ˆä½¿ç”¨æ™ºèƒ½è¿æ¥ç®¡ç†å™¨ï¼‰"""
        logger.info("ğŸ” æµ‹è¯•Prestoè¿æ¥...")
        try:
            with self.connection_manager.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT 1")
                result = cursor.fetchone()
                logger.info("âœ… è¿æ¥æµ‹è¯•æˆåŠŸ")
                return True
        except Exception as e:
            logger.error(f"âŒ è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False

    def sync_test_cost_data(self, start_date: str = None, end_date: str = None) -> bool:
        """åŒæ­¥æ–°æµ‹è¯•æˆæœ¬è¡¨æ•°æ®"""
        logger.info("ğŸš€ å¼€å§‹åŒæ­¥æµ‹è¯•æˆæœ¬æ•°æ®...")
        
        # é»˜è®¤ä»2025å¹´8æœˆå¼€å§‹
        if not start_date:
            start_date = '2025-08-01'
        if not end_date:
            end_date = datetime.now().strftime('%Y-%m-%d')
        
        # ç¡®ä¿ä¸æ—©äº2025-08-01
        if start_date < '2025-08-01':
            start_date = '2025-08-01'
            logger.info(f"âš ï¸ èµ·å§‹æ—¥æœŸè°ƒæ•´ä¸º {start_date} (æµ‹è¯•è¡¨ä»…åŒ…å«8æœˆä»¥åæ•°æ®)")
        
        # 1. ä»PrestoæŸ¥è¯¢æµ‹è¯•æˆæœ¬æ•°æ®
        test_cost_df = self.execute_test_cost_data_query(start_date, end_date)
        if test_cost_df is None:
            logger.error("âŒ æµ‹è¯•æˆæœ¬æ•°æ®æŸ¥è¯¢å¤±è´¥")
            return False
        
        if test_cost_df.empty:
            logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•æˆæœ¬æ•°æ®")
            return False
        
        logger.info(f"ğŸ“Š è·å¾—æµ‹è¯•æˆæœ¬æ•°æ®: {len(test_cost_df)} æ¡è®°å½•")
        
        # 2. å¯¼å…¥åˆ°æœ¬åœ°æ•°æ®åº“
        success = self.import_to_local_db(test_cost_df, "dwd_ttx_market_cash_cost_i_d_test")
        
        if success:
            logger.info("ğŸ‰ æµ‹è¯•æˆæœ¬æ•°æ®åŒæ­¥æˆåŠŸï¼")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            logger.info(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
            logger.info(f"   è®°å½•æ•°: {len(test_cost_df):,}")
            logger.info(f"   æ—¥æœŸèŒƒå›´: {test_cost_df['dt'].min()} åˆ° {test_cost_df['dt'].max()}")
            logger.info(f"   æ¸ é“æ•°: {test_cost_df['channel'].nunique()}")
            logger.info(f"   æ€»æˆæœ¬: Â¥{test_cost_df['cash_cost'].sum():,.2f}")
        else:
            logger.error("âŒ æµ‹è¯•æˆæœ¬æ•°æ®åŒæ­¥å¤±è´¥")
        
        return success

def main():
    """ä¸»å‡½æ•°ï¼šæ”¯æŒå‘½ä»¤è¡Œå‚æ•°çš„æ•°æ®åŒæ­¥"""
    parser = argparse.ArgumentParser(description='Prestoæ•°æ®åŒæ­¥å·¥å…·')
    parser.add_argument('--check', action='store_true', help='æ£€æŸ¥ç¼ºå¤±çš„æ—¥æœŸ')
    parser.add_argument('--sync-missing', action='store_true', help='åŒæ­¥æ‰€æœ‰ç¼ºå¤±çš„æ—¥æœŸ')
    parser.add_argument('--update-retention', action='store_true', help='ä»…æ›´æ–°æ¬¡ç•™ç‡ï¼Œä¸é‡æ–°åŒæ­¥æ•°æ®')
    parser.add_argument('--date', type=str, help='åŒæ­¥æŒ‡å®šæ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)')
    parser.add_argument('--start-date', type=str, default='2025-07-01', help='å¼€å§‹æ—¥æœŸ (é»˜è®¤: 2025-07-01)')
    parser.add_argument('--end-date', type=str, help='ç»“æŸæ—¥æœŸ (é»˜è®¤: æ˜¨å¤©)')
    parser.add_argument('--summary', action='store_true', help='æ˜¾ç¤ºæ•°æ®æ¦‚å†µ')
    parser.add_argument('--test', action='store_true', help='æµ‹è¯•è¿æ¥')
    parser.add_argument('--sync-test-cost', action='store_true', help='åŒæ­¥æµ‹è¯•æˆæœ¬è¡¨ (ä»2025-08-01å¼€å§‹)')
    
    args = parser.parse_args()
    
    syncer = PrestoSync()
    
    # åŒæ­¥æµ‹è¯•æˆæœ¬è¡¨
    if args.sync_test_cost:
        logger.info("ğŸ“¥ å¼€å§‹åŒæ­¥æµ‹è¯•æˆæœ¬è¡¨æ•°æ®...")
        if syncer.sync_test_cost_data(args.start_date, args.end_date):
            logger.info("ğŸ‰ æµ‹è¯•æˆæœ¬æ•°æ®åŒæ­¥æˆåŠŸï¼")
        else:
            logger.error("âŒ æµ‹è¯•æˆæœ¬æ•°æ®åŒæ­¥å¤±è´¥")
        return
    
    # æµ‹è¯•è¿æ¥
    if args.test or not any([args.check, args.sync_missing, args.update_retention, args.date, args.summary, args.sync_test_cost]):
        if not syncer.test_connection():
            logger.error("âŒ æ— æ³•è¿æ¥åˆ°Prestoæ•°æ®åº“")
            logger.error("è¯·æ£€æŸ¥:")
            logger.error("1. PrestoæœåŠ¡æ˜¯å¦è¿è¡Œ")
            logger.error("2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸") 
            logger.error("3. è¿æ¥é…ç½®æ˜¯å¦æ­£ç¡®")
            logger.error("4. æ˜¯å¦å®‰è£…äº†presto-python-client: pip install presto-python-client")
            return
        
        if args.test:
            logger.info("âœ… è¿æ¥æµ‹è¯•æˆåŠŸ")
            return
    
    # æ£€æŸ¥ç¼ºå¤±æ—¥æœŸ
    if args.check:
        syncer.check_missing_dates(args.start_date, args.end_date)
        return
    
    # åŒæ­¥æŒ‡å®šæ—¥æœŸ
    if args.date:
        logger.info(f"ğŸ“¥ åŒæ­¥æŒ‡å®šæ—¥æœŸ: {args.date}")
        if syncer.sync_data(args.date, args.date):
            logger.info("ğŸ‰ æ•°æ®åŒæ­¥æˆåŠŸï¼")
        else:
            logger.error("âŒ æ•°æ®åŒæ­¥å¤±è´¥")
        return
    
    # ä»…æ›´æ–°æ¬¡ç•™ç‡
    if args.update_retention:
        missing_dates = syncer.check_missing_dates(args.start_date, args.end_date)
        if syncer.sync_missing_dates(args.start_date, args.end_date, retention_only=True):
            logger.info("ğŸ‰ æ¬¡ç•™ç‡æ›´æ–°å®Œæˆï¼")
        else:
            logger.error("âŒ éƒ¨åˆ†æ¬¡ç•™ç‡æ›´æ–°å¤±è´¥")
        return
    
    # åŒæ­¥ç¼ºå¤±æ—¥æœŸ
    if args.sync_missing:
        if syncer.sync_missing_dates(args.start_date, args.end_date):
            logger.info("ğŸ‰ ç¼ºå¤±æ•°æ®åŒæ­¥å®Œæˆï¼")
        else:
            logger.error("âŒ éƒ¨åˆ†æ•°æ®åŒæ­¥å¤±è´¥")
        return
    
    # æ˜¾ç¤ºæ•°æ®æ¦‚å†µ
    if args.summary:
        logger.info("ğŸ“Š ç”Ÿæˆæ•°æ®æ¦‚å†µ...")
        df = syncer.get_data_summary(args.start_date, args.end_date)
        if not df.empty:
            print("\n=== æ•°æ®æ¦‚å†µ ===")
            print(df.to_string(index=False))
            print(f"\nğŸ“ˆ ç»Ÿè®¡:")
            print(f"  æ€»å¤©æ•°: {len(df)} å¤©")
            print(f"  æ€»ç”¨æˆ·: {df['total_users'].sum():,} äºº")
            print(f"  æ€»æ”¶å…¥: {df['total_revenue'].sum():,.2f} å…ƒ")
            print(f"  å¹³å‡ARPU: {df['arpu'].mean():.2f} å…ƒ")
        return
    
    # é»˜è®¤è¡Œä¸ºï¼šåŒæ­¥æœ€è¿‘7å¤©æ•°æ®
    start_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
    end_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
    
    logger.info(f"ğŸ“… é»˜è®¤åŒæ­¥æœ€è¿‘7å¤©: {start_date} åˆ° {end_date}")
    
    if syncer.sync_data(start_date, end_date):
        logger.info("ğŸ‰ æ•°æ®åŒæ­¥æˆåŠŸï¼")
    else:
        logger.error("âŒ æ•°æ®åŒæ­¥å¤±è´¥")

if __name__ == '__main__':
    main()