#!/usr/bin/env python3
"""
åŸºäºæ¨¡æ¿çš„æ¯æ—¥æ•°æ®æŠ¥å‘Šç”Ÿæˆå™¨
ä½¿ç”¨æ ‡å‡†åŒ–é…ç½®ç”Ÿæˆæ ¼å¼ç»Ÿä¸€çš„æŠ¥å‘Š
"""
import sys
import os
import sqlite3
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, Any, List
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from config.daily_report_template import (
    REPORT_CONFIG, CORE_METRICS, CHANNEL_ANALYSIS, USER_QUALITY_SEGMENTS,
    PLATFORM_TAX_CONFIG, TREND_ANALYSIS, COST_ANALYSIS, FORMATTING,
    REPORT_SECTIONS, DATA_QUALITY_RULES, REPORT_ANNOTATIONS, EXPORT_CONFIG
)

class DailyReportGenerator:
    """æ ‡å‡†åŒ–æ¯æ—¥æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, db_path: str = "./data/data.db"):
        self.db_path = db_path
        self.report_data = {}
        
    def generate_report(self, date: str, output_format: str = 'console') -> str:
        """
        ç”ŸæˆæŒ‡å®šæ—¥æœŸçš„æ ‡å‡†åŒ–æŠ¥å‘Š
        
        Args:
            date: æŠ¥å‘Šæ—¥æœŸ (YYYY-MM-DD)
            output_format: è¾“å‡ºæ ¼å¼ ('console', 'json', 'csv')
        
        Returns:
            æŠ¥å‘Šå†…å®¹æˆ–æ–‡ä»¶è·¯å¾„
        """
        self.target_date = date
        self.report_data = {'date': date, 'sections': {}}
        
        # è¿æ¥æ•°æ®åº“
        conn = sqlite3.connect(self.db_path)
        
        try:
            # 1. æ ¸å¿ƒæŒ‡æ ‡åˆ†æ
            self._analyze_core_metrics(conn)
            
            # 2. æ¸ é“è´¨é‡åˆ†æ
            self._analyze_channels(conn)
            
            # 3. ç”¨æˆ·è´¨é‡åˆ†å¸ƒ
            self._analyze_user_quality(conn)
            
            # 4. æˆæœ¬ROIåˆ†æ
            self._analyze_cost_roi(conn)
            
            # 5. å¹³å°ç¨æ”¶åˆ†æ
            self._analyze_platform_tax(conn)
            
            # 6. è¶‹åŠ¿åˆ†æ
            self._analyze_trends(conn)
            
            # 7. æ•°æ®è´¨é‡éªŒè¯
            self._validate_data_quality()
            
        finally:
            conn.close()
        
        # æ ¹æ®æ ¼å¼è¾“å‡ºæŠ¥å‘Š
        if output_format == 'console':
            return self._format_console_report()
        elif output_format == 'json':
            return self._export_json_report()
        elif output_format == 'csv':
            return self._export_csv_report()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¾“å‡ºæ ¼å¼: {output_format}")
    
    def _analyze_core_metrics(self, conn: sqlite3.Connection):
        """åˆ†ææ ¸å¿ƒä¸šåŠ¡æŒ‡æ ‡"""
        query = f'''
        SELECT 
            COUNT(*) as record_count,
            SUM(newuser) as total_users,
            SUM(CASE WHEN status = "good" THEN newuser ELSE 0 END) as good_users,
            SUM(CASE WHEN verification_status = "verified" THEN newuser ELSE 0 END) as verified_users,
            SUM(CASE WHEN status = "good" AND verification_status = "verified" THEN newuser ELSE 0 END) as quality_users,
            SUM(zizhu_revenue_1) as revenue_pre_tax,
            SUM(zizhu_revenue_1_aftertax) as revenue_after_tax,
            SUM(CASE WHEN status = "good" AND verification_status = "verified" AND zizhu_revenue_1_aftertax > 0 THEN newuser ELSE 0 END) as paying_users
        FROM cpz_qs_newuser_channel_i_d
        WHERE dt = '{self.target_date}'
        '''
        
        df = pd.read_sql_query(query, conn)
        if df.empty:
            raise ValueError(f"æœªæ‰¾åˆ° {self.target_date} çš„æ•°æ®")
        
        data = df.iloc[0].to_dict()
        
        # è®¡ç®—è¡ç”ŸæŒ‡æ ‡
        data['arpu_after_tax'] = data['revenue_after_tax'] / data['quality_users'] if data['quality_users'] > 0 else 0
        data['conversion_rate'] = data['paying_users'] / data['quality_users'] * 100 if data['quality_users'] > 0 else 0
        data['paying_arpu'] = data['revenue_after_tax'] / data['paying_users'] if data['paying_users'] > 0 else 0
        
        # è®¡ç®—ç™¾åˆ†æ¯”
        data['good_rate'] = data['good_users'] / data['total_users'] * 100 if data['total_users'] > 0 else 0
        data['verified_rate'] = data['verified_users'] / data['total_users'] * 100 if data['total_users'] > 0 else 0
        data['quality_rate'] = data['quality_users'] / data['total_users'] * 100 if data['total_users'] > 0 else 0
        
        self.report_data['sections']['core_metrics'] = data
    
    def _analyze_channels(self, conn: sqlite3.Connection):
        """åˆ†ææ¸ é“è´¨é‡"""
        query = f'''
        SELECT 
            ad_channel,
            SUM(newuser) as user_count,
            SUM(CASE WHEN status = "good" AND verification_status = "verified" THEN newuser ELSE 0 END) as quality_users,
            ROUND(SUM(CASE WHEN status = "good" AND verification_status = "verified" THEN newuser ELSE 0 END) * 100.0 / SUM(newuser), 1) as quality_rate,
            SUM(zizhu_revenue_1_aftertax) as revenue_after_tax,
            ROUND(SUM(zizhu_revenue_1_aftertax) / NULLIF(SUM(CASE WHEN status = "good" AND verification_status = "verified" THEN newuser ELSE 0 END), 0), 2) as arpu_after_tax
        FROM cpz_qs_newuser_channel_i_d
        WHERE dt = '{self.target_date}'
        GROUP BY ad_channel
        ORDER BY user_count DESC
        LIMIT {CHANNEL_ANALYSIS['top_channels_limit']}
        '''
        
        df = pd.read_sql_query(query, conn)
        self.report_data['sections']['channels'] = df.to_dict('records')
    
    def _analyze_user_quality(self, conn: sqlite3.Connection):
        """åˆ†æç”¨æˆ·è´¨é‡åˆ†å¸ƒ"""
        quality_data = []
        total_users = self.report_data['sections']['core_metrics']['total_users']
        total_revenue = self.report_data['sections']['core_metrics']['revenue_after_tax']
        
        for segment in USER_QUALITY_SEGMENTS:
            query = f'''
            SELECT 
                SUM(newuser) as user_count,
                SUM(zizhu_revenue_1_aftertax) as revenue
            FROM cpz_qs_newuser_channel_i_d
            WHERE dt = '{self.target_date}' AND ({segment['condition']})
            '''
            
            df = pd.read_sql_query(query, conn)
            if not df.empty and df.iloc[0]['user_count'] is not None:
                users = df.iloc[0]['user_count']
                revenue = df.iloc[0]['revenue'] or 0
                
                quality_data.append({
                    'segment': segment['name'],
                    'user_count': users,
                    'user_percentage': users / total_users * 100 if total_users > 0 else 0,
                    'revenue': revenue,
                    'revenue_percentage': revenue / total_revenue * 100 if total_revenue > 0 else 0,
                    'description': segment['description']
                })
        
        self.report_data['sections']['user_quality'] = quality_data
    
    def _analyze_cost_roi(self, conn: sqlite3.Connection):
        """åˆ†ææˆæœ¬å’ŒROI"""
        cost_query = f'''
        SELECT 
            COUNT(*) as cost_records,
            SUM(cash_cost) as total_cost,
            AVG(cash_cost) as avg_cost,
            MIN(cash_cost) as min_cost,
            MAX(cash_cost) as max_cost
        FROM {COST_ANALYSIS['cost_table']}
        WHERE dt = '{self.target_date}'
        '''
        
        df = pd.read_sql_query(cost_query, conn)
        if not df.empty and df.iloc[0]['cost_records'] > 0:
            cost_data = df.iloc[0].to_dict()
            
            # è®¡ç®—ROI
            revenue_pre = self.report_data['sections']['core_metrics']['revenue_pre_tax']
            revenue_after = self.report_data['sections']['core_metrics']['revenue_after_tax']
            total_cost = cost_data['total_cost']
            
            cost_data['profit_pre_tax'] = revenue_pre - total_cost
            cost_data['profit_after_tax'] = revenue_after - total_cost
            cost_data['roi_pre_tax'] = (revenue_pre - total_cost) / total_cost * 100 if total_cost > 0 else 0
            cost_data['roi_after_tax'] = (revenue_after - total_cost) / total_cost * 100 if total_cost > 0 else 0
            
            self.report_data['sections']['cost_roi'] = cost_data
        else:
            self.report_data['sections']['cost_roi'] = None
    
    def _analyze_platform_tax(self, conn: sqlite3.Connection):
        """åˆ†æå¹³å°ç¨æ”¶å½±å“"""
        query = f'''
        SELECT 
            os_type,
            SUM(newuser) as user_count,
            SUM(CASE WHEN status = "good" AND verification_status = "verified" THEN newuser ELSE 0 END) as quality_users,
            SUM(zizhu_revenue_1) as revenue_pre_tax,
            SUM(zizhu_revenue_1_aftertax) as revenue_after_tax,
            ROUND((SUM(zizhu_revenue_1) - SUM(zizhu_revenue_1_aftertax)) / NULLIF(SUM(zizhu_revenue_1), 0) * 100, 1) as tax_rate,
            ROUND(SUM(zizhu_revenue_1_aftertax) / NULLIF(SUM(CASE WHEN status = "good" AND verification_status = "verified" THEN newuser ELSE 0 END), 0), 2) as arpu_after_tax
        FROM cpz_qs_newuser_channel_i_d
        WHERE dt = '{self.target_date}' AND zizhu_revenue_1 > 0
        GROUP BY os_type
        ORDER BY revenue_after_tax DESC
        '''
        
        df = pd.read_sql_query(query, conn)
        self.report_data['sections']['platform_tax'] = df.to_dict('records')
    
    def _analyze_trends(self, conn: sqlite3.Connection):
        """åˆ†ææ—¥ç¯æ¯”è¶‹åŠ¿"""
        # è·å–å¯¹æ¯”æ—¥æœŸèŒƒå›´
        target_date = datetime.strptime(self.target_date, '%Y-%m-%d')
        start_date = (target_date - timedelta(days=1)).strftime('%Y-%m-%d')
        end_date = (target_date + timedelta(days=1)).strftime('%Y-%m-%d')
        
        query = f'''
        SELECT 
            dt,
            SUM(newuser) as total_users,
            SUM(CASE WHEN status = "good" AND verification_status = "verified" THEN newuser ELSE 0 END) as quality_users,
            SUM(zizhu_revenue_1_aftertax) as revenue_after_tax,
            ROUND(SUM(zizhu_revenue_1_aftertax) / NULLIF(SUM(CASE WHEN status = "good" AND verification_status = "verified" THEN newuser ELSE 0 END), 0), 2) as arpu_after_tax
        FROM cpz_qs_newuser_channel_i_d
        WHERE dt IN ('{start_date}', '{self.target_date}', '{end_date}')
        GROUP BY dt
        ORDER BY dt
        '''
        
        df = pd.read_sql_query(query, conn)
        trend_data = df.to_dict('records')
        
        # è®¡ç®—ç¯æ¯”å˜åŒ–
        target_idx = next((i for i, item in enumerate(trend_data) if item['dt'] == self.target_date), None)
        
        if target_idx is not None:
            changes = {}
            current = trend_data[target_idx]
            
            # ä¸å‰ä¸€å¤©å¯¹æ¯”
            if target_idx > 0:
                prev = trend_data[target_idx - 1]
                changes['vs_previous'] = {
                    'date': prev['dt'],
                    'user_change': (current['total_users'] - prev['total_users']) / prev['total_users'] * 100 if prev['total_users'] > 0 else 0,
                    'revenue_change': (current['revenue_after_tax'] - prev['revenue_after_tax']) / prev['revenue_after_tax'] * 100 if prev['revenue_after_tax'] > 0 else 0,
                    'arpu_change': (current['arpu_after_tax'] - prev['arpu_after_tax']) / prev['arpu_after_tax'] * 100 if prev['arpu_after_tax'] > 0 else 0
                }
            
            # ä¸åä¸€å¤©å¯¹æ¯”
            if target_idx < len(trend_data) - 1:
                next_day = trend_data[target_idx + 1]
                changes['vs_next'] = {
                    'date': next_day['dt'],
                    'user_change': (next_day['total_users'] - current['total_users']) / current['total_users'] * 100 if current['total_users'] > 0 else 0,
                    'revenue_change': (next_day['revenue_after_tax'] - current['revenue_after_tax']) / current['revenue_after_tax'] * 100 if current['revenue_after_tax'] > 0 else 0,
                    'arpu_change': (next_day['arpu_after_tax'] - current['arpu_after_tax']) / current['arpu_after_tax'] * 100 if current['arpu_after_tax'] > 0 else 0
                }
            
            self.report_data['sections']['trends'] = {
                'data': trend_data,
                'changes': changes
            }
        else:
            self.report_data['sections']['trends'] = None
    
    def _validate_data_quality(self):
        """éªŒè¯æ•°æ®è´¨é‡"""
        core_data = self.report_data['sections']['core_metrics']
        warnings = []
        
        for rule_name, rule_config in DATA_QUALITY_RULES.items():
            # è¿™é‡Œå¯ä»¥å®ç°å…·ä½“çš„éªŒè¯é€»è¾‘
            # ç®€åŒ–ç¤ºä¾‹ï¼šæ£€æŸ¥ARPUåˆç†æ€§
            if rule_name == 'arpu_reasonable':
                arpu = core_data.get('arpu_after_tax', 0)
                if not (1 <= arpu <= 100):
                    warnings.append(f"ARPUå¼‚å¸¸: {arpu:.2f}å…ƒ")
        
        self.report_data['data_quality'] = {
            'warnings': warnings,
            'validation_time': datetime.now().isoformat()
        }
    
    def _format_console_report(self) -> str:
        """æ ¼å¼åŒ–æ§åˆ¶å°æŠ¥å‘Š"""
        lines = []
        width = EXPORT_CONFIG['console']['width']
        
        # æŠ¥å‘Šæ ‡é¢˜
        title = f"{REPORT_CONFIG['title']} - {self.target_date}"
        lines.append('â•' * width)
        lines.append(title.center(width))
        lines.append('â•' * width)
        
        # æ ¸å¿ƒæŒ‡æ ‡
        core = self.report_data['sections']['core_metrics']
        lines.append('')
        lines.append('ğŸ“Š æ ¸å¿ƒä¸šåŠ¡æŒ‡æ ‡')
        lines.append('â”' * width)
        lines.append(f"ğŸ“… ç»Ÿè®¡æ—¥æœŸ: {self.target_date}")
        lines.append(f"ğŸ“Š æ•°æ®è®°å½•: {core['record_count']:,} æ¡")
        lines.append(f"ğŸ‘¥ æ€»ç”¨æˆ·æ•°: {core['total_users']:,} äºº")
        lines.append(f"âœ… Goodç”¨æˆ·: {core['good_users']:,} äºº ({core['good_rate']:.1f}%)")
        lines.append(f"ğŸ” è®¤è¯ç”¨æˆ·: {core['verified_users']:,} äºº ({core['verified_rate']:.1f}%)")
        lines.append(f"â­ Goodä¸”è®¤è¯ç”¨æˆ·: {core['quality_users']:,} äºº ({core['quality_rate']:.1f}%)")
        lines.append(f"ğŸ’° æ€»æ”¶å…¥ï¼ˆç¨å‰ï¼‰: Â¥{core['revenue_pre_tax']:,.2f}")
        lines.append(f"ğŸ’° æ€»æ”¶å…¥ï¼ˆç¨åï¼‰: Â¥{core['revenue_after_tax']:,.2f}")
        lines.append(f"ğŸ“ˆ ARPUï¼ˆç¨åï¼‰: Â¥{core['arpu_after_tax']:.2f} ã€åŸºäºGood+è®¤è¯ç”¨æˆ·ã€‘")
        lines.append(f"ğŸ’³ ä»˜è´¹è½¬åŒ–ç‡: {core['conversion_rate']:.1f}% ({core['paying_users']:,}/{core['quality_users']:,})")
        lines.append(f"ğŸ’ ä»˜è´¹ç”¨æˆ·ARPUï¼ˆç¨åï¼‰: Â¥{core['paying_arpu']:.2f}")
        
        # æ¸ é“åˆ†æ
        if 'channels' in self.report_data['sections']:
            lines.append('')
            lines.append('ğŸ† TOP10æ¸ é“è´¨é‡åˆ†æ')
            lines.append('â”' * width)
            header = f"{'æ¸ é“':<15} {'ç”¨æˆ·æ•°':<8} {'Good+è®¤è¯':<9} {'è´¨é‡ç‡':<8} {'æ”¶å…¥(ç¨å)':<12} {'ARPU(ç¨å)':<10}"
            lines.append(header)
            lines.append('â”€' * len(header))
            
            for channel in self.report_data['sections']['channels']:
                quality = f"{channel['quality_rate']:.1f}%"
                revenue = f"Â¥{channel['revenue_after_tax']:,.0f}"
                arpu = f"Â¥{channel['arpu_after_tax']:.2f}" if channel['arpu_after_tax'] else "Â¥0.00"
                line = f"{channel['ad_channel']:<15} {channel['user_count']:<8,} {channel['quality_users']:<9,} {quality:<8} {revenue:<12} {arpu:<10}"
                lines.append(line)
        
        # æŠ¥å‘Šè¯´æ˜
        lines.append('')
        lines.append('â•' * width)
        lines.append('ğŸ“‹ æŠ¥å‘Šè¯´æ˜')
        lines.append('â•' * width)
        for key, value in REPORT_ANNOTATIONS.items():
            lines.append(f"âœ… {value}")
        lines.append('â•' * width)
        
        return '\\n'.join(lines)
    
    def _export_json_report(self) -> str:
        """å¯¼å‡ºJSONæ ¼å¼æŠ¥å‘Š"""
        filename = f"daily_report_{self.target_date.replace('-', '')}.json"
        filepath = os.path.join(project_root, 'reports', filename)
        
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.report_data, f, ensure_ascii=False, indent=2, default=str)
        
        return filepath
    
    def _export_csv_report(self) -> str:
        """å¯¼å‡ºCSVæ ¼å¼æŠ¥å‘Š"""
        filename = f"daily_report_{self.target_date.replace('-', '')}.csv"
        filepath = os.path.join(project_root, 'reports', filename)
        
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        # å°†æ ¸å¿ƒæŒ‡æ ‡è½¬æ¢ä¸ºDataFrameå¹¶ä¿å­˜
        core_df = pd.DataFrame([self.report_data['sections']['core_metrics']])
        core_df.to_csv(filepath, index=False, encoding='utf-8-sig')
        
        return filepath

def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ç”Ÿæˆæ ‡å‡†åŒ–æ¯æ—¥æ•°æ®æŠ¥å‘Š')
    parser.add_argument('date', help='æŠ¥å‘Šæ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--format', choices=['console', 'json', 'csv'], default='console', help='è¾“å‡ºæ ¼å¼')
    parser.add_argument('--db', default='./data.db', help='æ•°æ®åº“è·¯å¾„')
    
    args = parser.parse_args()
    
    generator = DailyReportGenerator(args.db)
    
    try:
        result = generator.generate_report(args.date, args.format)
        
        if args.format == 'console':
            print(result)
        else:
            print(f"æŠ¥å‘Šå·²ç”Ÿæˆ: {result}")
            
    except Exception as e:
        print(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        return 1
    
    return 0

if __name__ == '__main__':
    sys.exit(main())